<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Installation - 高新 | AI平台开发工程师</title>
<meta name=generator content="Hugo 0.145.0"><link href=https://mlcore-engine.github.io//index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlcore-engine.github.io/5kubernetes/installation/><link rel=stylesheet href=https://mlcore-engine.github.io/css/theme.min.css><link rel=stylesheet href=https://mlcore-engine.github.io/css/chroma.min.css><script defer src=https://mlcore-engine.github.io//js/fontawesome6/all.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js integrity="sha256-H3cjtrm/ztDeuhCN9I4yh4iN2Ybx/y1RM7rMmAesA0k=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js integrity="sha256-4XodgW4TwIJuDtf+v6vDJ39FVxI0veC/kSCCmnFp7ck=" crossorigin=anonymous></script><script src=https://mlcore-engine.github.io/js/bundle.js></script><style>@media screen and (min-width:480px){.sidebar{flex:0 0 20%!important;max-width:20%!important}main{flex:0 0 80%!important;max-width:80%!important}}</style><meta property="og:url" content="https://mlcore-engine.github.io/5kubernetes/installation/"><meta property="og:site_name" content="高新 | AI平台开发工程师"><meta property="og:title" content="Installation"><meta property="og:description" content='部署脚本： https://gitee.com/MLcore-Engine/ai-infra-deploy update core #配置阿里yum源命令 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo #运行以下命令生成缓存 yum clean all yum makecache yum -y update //load pubilc key install elrepo rpm -import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm //install el repo meta yum list available --disablerepo=&#39;*&#39; --enablerepo=elrepo-kernel //view available install eprepo yum --disablerepo=\* --enablerepo=elrepo-kernel list kernel #install kernel yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64 #yum -y --enablerepo=elrepo-kernel install kernel-ml.x86_64 kernel-ml-tools.x86_64 sudo yum --enablerepo=elrepo-kernel install kernel-ml kernel-ml-devel startup kernel sequence #view all available kernel on the system awk -F\&#39; &#39;$1=="menuentry " {print $2}&#39; /etc/grub2.cfg #set boot new kernel grub2-set-default 0 #运行grub2-mkconfig命令来重新创建内核配置 grub2-mkconfig -o /boot/grub2/grub.cfg reboot basic environment systemctl stop firewalld && systemctl disable firewalld sed -i &#39;/^SELINUX=/c SELINUX=disabled&#39; /etc/selinux/config setenforce 0 swapoff -a sed -i &#39;s/^.*centos-swap/#&/g&#39; /etc/fstab cat << EOF >> /etc/hosts master 192.168.31.127 kube 192.168.31.128 EOF // # 激活 br_netfilter 模块 modprobe br_netfilter cat << EOF > /etc/modules-load.d/k8s.conf br_netfilter EOF # 内核参数设置：开启IP转发，允许iptables对bridge的数据进行处理 cat << EOF > /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # 立即生效 apt-get install -y apt-transport-https ca-certificates curl gpg #time sync #master node yum install -y chrony sed -i &#39;s/^server/#&/&#39; /etc/chrony.conf cat >> /etc/chrony.conf << EOF server ntp1.aliyun.com iburst local stratum 10 allow EOF systemctl restart chronyd && systemctl enable chronyd #node yum install -y chrony sed -i &#39;s/^server/#&/&#39; /etc/chrony.conf cat >> /etc/chrony.conf << EOF server 172.16.101.11 iburst EOF systemctl restart chronyd && systemctl enable chronyd #找到最大的磁盘 挂载到/data目录 ln -s /data/kubelet /var/lib/kubelet ln -s /data/docker /var/lib/docker install docker #centos sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-engine sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo #latest sudo yum install docker-ce docker-ce-cli containerd.io -y ##ubuntu for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done # Add Docker&#39;s official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \ "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \ $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update #latest sudo apt-get install docker-ce docker-ce-cli containerd.io yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce-20.10 systemctl enable docker && systemctl start docker cat << EOF > /etc/docker/daemon.json { "registry-mirrors": ["https://h3blxdss.mirror.aliyuncs.com"], "exec-opts": ["native.cgroupdriver=systemd"], "insecure-registries": ["registry-tgq.harbor.com"] } EOF systemctl daemon-reload && systemctl restart docker install kubeadm cat << EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum install -y --nogpgcheck kubelet-1.23.6 kubeadm-1.23.6 kubectl-1.23.6 yum install --nogpgcheck --downloadonly --downloaddir=/opt/kubeadm kubelet-1.23.6 kubeadm-1.23.6 kubectl-1.23.6 #yum install -y kubelet kubeadm kubectl systemctl enable kubelet && systemctl start kubelet kubeadm config images list kubeadm config images pull --image-repository=registry.aliyuncs.com/google_containers --kubernetes-version=v1.23.6 kubeadm config images list --image-repository=registry.aliyuncs.com/google_containers init master kubeadm init --image-repository=registry.aliyuncs.com/google_containers --kubernetes-version=v1.23.6 --service-cidr=10.1.0.0/16 --pod-network-cidr=10.244.0.0/16 #--apiserver-cert-extra-sans 公网ip或者域名 # 要开始使用集群，您需要以常规用户身份运行以下命令 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 或者，如果您是root用户，则可以运行允许命令 export KUBECONFIG=/etc/kubernetes/admin.conf #部署flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml # 注意修改ip pool 与 --pod-network-cidr 一致 # 部署calico 把calico.yaml里pod所在网段改成kubeadm init时选项--pod-network-cidr所指定的网段 - name: CALICO_IPV4POOL_CIDR value: "10.244.0.0/16" # Disable file logging so `kubectl logs` works. - name: CALICO_DISABLE_FILE_LOGGING value: "true" # deploy controller oeprator kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/tigera-operator.yaml #deploy resource kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/custom-resources.yaml flannel yaml --- kind: Namespace apiVersion: v1 metadata: name: kube-flannel labels: k8s-app: flannel pod-security.kubernetes.io/enforce: privileged --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: flannel name: flannel rules: - apiGroups: - "" resources: - pods verbs: - get - apiGroups: - "" resources: - nodes verbs: - get - list - watch - apiGroups: - "" resources: - nodes/status verbs: - patch - apiGroups: - networking.k8s.io resources: - clustercidrs verbs: - list - watch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: flannel name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-flannel --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: flannel name: flannel namespace: kube-flannel --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-flannel labels: tier: node k8s-app: flannel app: flannel data: cni-conf.json: | { "name": "cbr0", "cniVersion": "0.3.1", "plugins": [ { "type": "flannel", "delegate": { "hairpinMode": true, "isDefaultGateway": true } }, { "type": "portmap", "capabilities": { "portMappings": true } } ] } net-conf.json: | { "Network": "10.244.0.0/16", "Backend": { "Type": "vxlan" } } --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds namespace: kube-flannel labels: tier: node app: flannel k8s-app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni-plugin image: docker.io/flannel/flannel-cni-plugin:v1.1.2 #image: docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.2 command: - cp args: - -f - /flannel - /opt/cni/bin/flannel volumeMounts: - name: cni-plugin mountPath: /opt/cni/bin - name: install-cni image: docker.io/flannel/flannel:v0.22.0 #image: docker.io/rancher/mirrored-flannelcni-flannel:v0.22.0 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: docker.io/flannel/flannel:v0.22.0 #image: docker.io/rancher/mirrored-flannelcni-flannel:v0.22.0 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: "100m" memory: "50Mi" securityContext: privileged: false capabilities: add: ["NET_ADMIN", "NET_RAW"] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: EVENT_QUEUE_DEPTH value: "5000" volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ - name: xtables-lock mountPath: /run/xtables.lock volumes: - name: run hostPath: path: /run/flannel - name: cni-plugin hostPath: path: /opt/cni/bin - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg - name: xtables-lock hostPath: path: /run/xtables.lock type: FileOrCreate calico yaml # This section includes base Calico installation configuration. # For more information, see: https://projectcalico.docs.tigera.io/master/reference/installation/api#operator.tigera.io/v1.Installation apiVersion: operator.tigera.io/v1 kind: Installation metadata: name: default spec: # Configures Calico networking. calicoNetwork: # Note: The ipPools section cannot be modified post-install. ipPools: - blockSize: 26 cidr: 192.168.0.0/16 encapsulation: VXLANCrossSubnet natOutgoing: Enabled nodeSelector: all() --- # This section configures the Calico API server. # For more information, see: https://projectcalico.docs.tigera.io/master/reference/installation/api#operator.tigera.io/v1.APIServer apiVersion: operator.tigera.io/v1 kind: APIServer metadata: name: default spec: {} add node #get token and ca kubeadm token list | awk -F" " &#39;{print $1}&#39; |tail -n 1 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed &#39;s/^ .* //&#39; kubeadm join 192.168.31.127:6443 --token <toekn> --discovery-token-ca-cert-hash sha256:<ca> GPU install nvidia-docker2 #Setup the repository and the GPG key: distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \ && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.repo | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo sudo yum clean expire-cache sudo yum install -y nvidia-docker2 sudo systemctl restart docker sudo docker run --rm --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi install nvidia-driver for centos7 #update version sudo yum clean all sudo yum update #验证系统内核版本和安装开发包 sudo yum install -y gcc gcc-c++ kernel-devel-$(uname -r) kernel-headers-$(uname -r) #验证gcc的版本 gcc --version #由于CUDA 11.3要求GCC的版本是6以上，下面是安装GCC7的脚本 sudo yum install centos-release-scl sudo yum install devtoolset-7 # launch a new shell instance using the Software Collection scl tool: scl enable devtoolset-7 bash gcc --version #检查当前驱动情况 sudo yum install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm sudo yum install nvidia-detect # 安装nvida-detect nvidia-detect -v # 检测能够升级到的驱动器版本 cat /proc/driver/nvidia/version # 查看当前驱动版本 #卸载之前驱动。如果第一次安装，忽略 sudo /usr/bin/nvidia-uninstall #屏蔽nouveau显卡驱动，把nvidiafb从屏蔽列表中移除 sudo rm -rf disable-nouveau.conf cat << EOF > disable-nouveau.conf blacklist nouveau options nouveau modeset=0 EOF sudo chown root:root disable-nouveau.conf sudo chmod 644 disable-nouveau.conf sudo mv disable-nouveau.conf /etc/modprobe.d/ cat /etc/modprobe.d/disable-nouveau.conf ll /etc/modprobe.d/disable-nouveau.conf #重建 initramfs 镜像 sudo systemctl set-default multi-user.target #设置运行级别为文本模式 sudo shutdown -r now #下载驱动包 NVIDIA-Linux-x86_64-525.60.13.run (https://www.nvidia.com/Download/index.aspx?lang=en-us) NVIDIA-Linux-x86_64-525.60.13.run sudo rpm -ivh http://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-ml-devel-6.3.2-1.el7.elrepo.x86_64.rpm lsmod | grep nouveau #查看nouveau是否已经禁用, 应该没有返回内容 nvidia_run=NVIDIA-Linux-x86_64-460.84.run chmod 755 $nvidia_run sudo ./$nvidia_run sudo systemctl set-default graphical.target #设置运行级别回图形模式 sudo systemctl get-default sudo shutdown -r now #查看是否安装成功 cat /proc/driver/nvidia/version nvidia-smi 磁盘分区挂载 parted $ parted /dev/sdb mklabel gpt $ parted /dev/sdb mkpart primary xfs 0% 100% $ mkfs.xfs /dev/sdb1 $ mount /dev/sdb1 /data $ df -hT /data 文件系统 类型 容量 已用 可用 已用% 挂载点 /dev/sdb1 xfs 100G 33M 100G 1% /data $ vim /etc/fstab /dev/sdb1 /data xfs defaults 0 0 # 实现开机自动挂载 fdisk fdisk /dev/sdb n 进入分区状态 p 主分区 分区大小 默认即可 w 保存 mke2fs -t ext4 /dev/sdb1 mount /dev/sdb1 ~/newpath 修改 /etc/fstab UUID=c61117ca-9176-4d0b-be4d-1b0f434359a7 /newpath ext4 defaults 0 0 UUID 的获取可以通过这个命令 blkid /dev/sdb1 最后执行 mount -a 节点集群间迁移 kubeadm reset systemctl stop kubelet systemctl stop docker rm -rf /var/lib/cni/ rm -rf /var/lib/kubelet/* rm -rf /etc/cni/ rm -rf /var/lib/etcd/* ifconfig cni0 down ifconfig flannel.1 down ifconfig docker0 down ip link set cni0 down && ip link set flannel.1 down ip link delete cni0 && ip link delete flannel.1 systemctl restart docker && systemctl restart kubelet rm -rf /root/.kube/config 然后执行命令加入到其他集群 master重新加入集群 #我们有时候会有删除master节点，再重新加入master节点的需求，比如master机器改名。这里注意重新加入时，经常会出现etcd报错 [check-etcd] Checking that the etcd cluster is healthy error executionini phase check-etcd: etcd cluster is not healthy: failed to dial endpoint https://ip:2379 with maintenance client: context deadline exceeded #这个时候，就需要去还没有停止的master节点里的etcd的pod里去，删除该老master节点对应的etcd信息 kubectl drain master01 kubectl delete node master01 #master01 执行 kubeadm reset rm -rf /etc/kubernetes/manifests/ kubectl exec -it etcd-master02 sh etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key member list #找到对应的hash etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key member remove 12637f5ec2bd02b8 kubeadm init phase upload-certs --upload-certs # 返回certificates-key #执行加入命令 You can now join any number of the control-plane node running the following command on each as root: kubeadm join 172.16.101.211:9443 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:ae579faf241a307a860d0a9e9ba1e308fe0e7a6006b90ece97eb42dfe9fc59b8 \ --control-plane --certificate-key 79d731d185a93121e73899c10445f5fcaeac8d33155f5402c48bed5543f59e3b Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use "kubeadm init phase upload-certs --upload-certs" to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.16.101.211:9443 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:ae579faf241a307a860d0a9e9ba1e308fe0e7a6006b90ece97eb42dfe9fc59b8 kubeadm join 172.16.101.211:9443 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:ae579faf241a307a860d0a9e9ba1e308fe0e7a6006b90ece97eb42dfe9fc59b8 \ --control-plane --certificate-key 28d8cd85b5b90fe7603c599915521c20dc5ab5e6be28b52d904b44efb91eed19 离线安装教程 https://cloud.tencent.com/developer/article/2165251 离线安装NFS'><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="5kubernetes"><meta property="article:published_time" content="2025-04-03T15:16:15+08:00"><meta property="article:modified_time" content="2025-04-03T15:16:15+08:00"><meta property="og:image" content="https://mlcore-engine.github.io/home/me.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://mlcore-engine.github.io/home/me.png"><meta name=twitter:title content="Installation"><meta name=twitter:description content='部署脚本： https://gitee.com/MLcore-Engine/ai-infra-deploy update core #配置阿里yum源命令 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo #运行以下命令生成缓存 yum clean all yum makecache yum -y update //load pubilc key install elrepo rpm -import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm //install el repo meta yum list available --disablerepo=&#39;*&#39; --enablerepo=elrepo-kernel //view available install eprepo yum --disablerepo=\* --enablerepo=elrepo-kernel list kernel #install kernel yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64 #yum -y --enablerepo=elrepo-kernel install kernel-ml.x86_64 kernel-ml-tools.x86_64 sudo yum --enablerepo=elrepo-kernel install kernel-ml kernel-ml-devel startup kernel sequence #view all available kernel on the system awk -F\&#39; &#39;$1=="menuentry " {print $2}&#39; /etc/grub2.cfg #set boot new kernel grub2-set-default 0 #运行grub2-mkconfig命令来重新创建内核配置 grub2-mkconfig -o /boot/grub2/grub.cfg reboot basic environment systemctl stop firewalld && systemctl disable firewalld sed -i &#39;/^SELINUX=/c SELINUX=disabled&#39; /etc/selinux/config setenforce 0 swapoff -a sed -i &#39;s/^.*centos-swap/#&/g&#39; /etc/fstab cat << EOF >> /etc/hosts master 192.168.31.127 kube 192.168.31.128 EOF // # 激活 br_netfilter 模块 modprobe br_netfilter cat << EOF > /etc/modules-load.d/k8s.conf br_netfilter EOF # 内核参数设置：开启IP转发，允许iptables对bridge的数据进行处理 cat << EOF > /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # 立即生效 apt-get install -y apt-transport-https ca-certificates curl gpg #time sync #master node yum install -y chrony sed -i &#39;s/^server/#&/&#39; /etc/chrony.conf cat >> /etc/chrony.conf << EOF server ntp1.aliyun.com iburst local stratum 10 allow EOF systemctl restart chronyd && systemctl enable chronyd #node yum install -y chrony sed -i &#39;s/^server/#&/&#39; /etc/chrony.conf cat >> /etc/chrony.conf << EOF server 172.16.101.11 iburst EOF systemctl restart chronyd && systemctl enable chronyd #找到最大的磁盘 挂载到/data目录 ln -s /data/kubelet /var/lib/kubelet ln -s /data/docker /var/lib/docker install docker #centos sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-engine sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo #latest sudo yum install docker-ce docker-ce-cli containerd.io -y ##ubuntu for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done # Add Docker&#39;s official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \ "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \ $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update #latest sudo apt-get install docker-ce docker-ce-cli containerd.io yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce-20.10 systemctl enable docker && systemctl start docker cat << EOF > /etc/docker/daemon.json { "registry-mirrors": ["https://h3blxdss.mirror.aliyuncs.com"], "exec-opts": ["native.cgroupdriver=systemd"], "insecure-registries": ["registry-tgq.harbor.com"] } EOF systemctl daemon-reload && systemctl restart docker install kubeadm cat << EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum install -y --nogpgcheck kubelet-1.23.6 kubeadm-1.23.6 kubectl-1.23.6 yum install --nogpgcheck --downloadonly --downloaddir=/opt/kubeadm kubelet-1.23.6 kubeadm-1.23.6 kubectl-1.23.6 #yum install -y kubelet kubeadm kubectl systemctl enable kubelet && systemctl start kubelet kubeadm config images list kubeadm config images pull --image-repository=registry.aliyuncs.com/google_containers --kubernetes-version=v1.23.6 kubeadm config images list --image-repository=registry.aliyuncs.com/google_containers init master kubeadm init --image-repository=registry.aliyuncs.com/google_containers --kubernetes-version=v1.23.6 --service-cidr=10.1.0.0/16 --pod-network-cidr=10.244.0.0/16 #--apiserver-cert-extra-sans 公网ip或者域名 # 要开始使用集群，您需要以常规用户身份运行以下命令 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 或者，如果您是root用户，则可以运行允许命令 export KUBECONFIG=/etc/kubernetes/admin.conf #部署flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml # 注意修改ip pool 与 --pod-network-cidr 一致 # 部署calico 把calico.yaml里pod所在网段改成kubeadm init时选项--pod-network-cidr所指定的网段 - name: CALICO_IPV4POOL_CIDR value: "10.244.0.0/16" # Disable file logging so `kubectl logs` works. - name: CALICO_DISABLE_FILE_LOGGING value: "true" # deploy controller oeprator kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/tigera-operator.yaml #deploy resource kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/custom-resources.yaml flannel yaml --- kind: Namespace apiVersion: v1 metadata: name: kube-flannel labels: k8s-app: flannel pod-security.kubernetes.io/enforce: privileged --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: flannel name: flannel rules: - apiGroups: - "" resources: - pods verbs: - get - apiGroups: - "" resources: - nodes verbs: - get - list - watch - apiGroups: - "" resources: - nodes/status verbs: - patch - apiGroups: - networking.k8s.io resources: - clustercidrs verbs: - list - watch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: flannel name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-flannel --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: flannel name: flannel namespace: kube-flannel --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-flannel labels: tier: node k8s-app: flannel app: flannel data: cni-conf.json: | { "name": "cbr0", "cniVersion": "0.3.1", "plugins": [ { "type": "flannel", "delegate": { "hairpinMode": true, "isDefaultGateway": true } }, { "type": "portmap", "capabilities": { "portMappings": true } } ] } net-conf.json: | { "Network": "10.244.0.0/16", "Backend": { "Type": "vxlan" } } --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds namespace: kube-flannel labels: tier: node app: flannel k8s-app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni-plugin image: docker.io/flannel/flannel-cni-plugin:v1.1.2 #image: docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.2 command: - cp args: - -f - /flannel - /opt/cni/bin/flannel volumeMounts: - name: cni-plugin mountPath: /opt/cni/bin - name: install-cni image: docker.io/flannel/flannel:v0.22.0 #image: docker.io/rancher/mirrored-flannelcni-flannel:v0.22.0 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: docker.io/flannel/flannel:v0.22.0 #image: docker.io/rancher/mirrored-flannelcni-flannel:v0.22.0 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: "100m" memory: "50Mi" securityContext: privileged: false capabilities: add: ["NET_ADMIN", "NET_RAW"] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: EVENT_QUEUE_DEPTH value: "5000" volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ - name: xtables-lock mountPath: /run/xtables.lock volumes: - name: run hostPath: path: /run/flannel - name: cni-plugin hostPath: path: /opt/cni/bin - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg - name: xtables-lock hostPath: path: /run/xtables.lock type: FileOrCreate calico yaml # This section includes base Calico installation configuration. # For more information, see: https://projectcalico.docs.tigera.io/master/reference/installation/api#operator.tigera.io/v1.Installation apiVersion: operator.tigera.io/v1 kind: Installation metadata: name: default spec: # Configures Calico networking. calicoNetwork: # Note: The ipPools section cannot be modified post-install. ipPools: - blockSize: 26 cidr: 192.168.0.0/16 encapsulation: VXLANCrossSubnet natOutgoing: Enabled nodeSelector: all() --- # This section configures the Calico API server. # For more information, see: https://projectcalico.docs.tigera.io/master/reference/installation/api#operator.tigera.io/v1.APIServer apiVersion: operator.tigera.io/v1 kind: APIServer metadata: name: default spec: {} add node #get token and ca kubeadm token list | awk -F" " &#39;{print $1}&#39; |tail -n 1 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed &#39;s/^ .* //&#39; kubeadm join 192.168.31.127:6443 --token <toekn> --discovery-token-ca-cert-hash sha256:<ca> GPU install nvidia-docker2 #Setup the repository and the GPG key: distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \ && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.repo | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo sudo yum clean expire-cache sudo yum install -y nvidia-docker2 sudo systemctl restart docker sudo docker run --rm --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi install nvidia-driver for centos7 #update version sudo yum clean all sudo yum update #验证系统内核版本和安装开发包 sudo yum install -y gcc gcc-c++ kernel-devel-$(uname -r) kernel-headers-$(uname -r) #验证gcc的版本 gcc --version #由于CUDA 11.3要求GCC的版本是6以上，下面是安装GCC7的脚本 sudo yum install centos-release-scl sudo yum install devtoolset-7 # launch a new shell instance using the Software Collection scl tool: scl enable devtoolset-7 bash gcc --version #检查当前驱动情况 sudo yum install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm sudo yum install nvidia-detect # 安装nvida-detect nvidia-detect -v # 检测能够升级到的驱动器版本 cat /proc/driver/nvidia/version # 查看当前驱动版本 #卸载之前驱动。如果第一次安装，忽略 sudo /usr/bin/nvidia-uninstall #屏蔽nouveau显卡驱动，把nvidiafb从屏蔽列表中移除 sudo rm -rf disable-nouveau.conf cat << EOF > disable-nouveau.conf blacklist nouveau options nouveau modeset=0 EOF sudo chown root:root disable-nouveau.conf sudo chmod 644 disable-nouveau.conf sudo mv disable-nouveau.conf /etc/modprobe.d/ cat /etc/modprobe.d/disable-nouveau.conf ll /etc/modprobe.d/disable-nouveau.conf #重建 initramfs 镜像 sudo systemctl set-default multi-user.target #设置运行级别为文本模式 sudo shutdown -r now #下载驱动包 NVIDIA-Linux-x86_64-525.60.13.run (https://www.nvidia.com/Download/index.aspx?lang=en-us) NVIDIA-Linux-x86_64-525.60.13.run sudo rpm -ivh http://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-ml-devel-6.3.2-1.el7.elrepo.x86_64.rpm lsmod | grep nouveau #查看nouveau是否已经禁用, 应该没有返回内容 nvidia_run=NVIDIA-Linux-x86_64-460.84.run chmod 755 $nvidia_run sudo ./$nvidia_run sudo systemctl set-default graphical.target #设置运行级别回图形模式 sudo systemctl get-default sudo shutdown -r now #查看是否安装成功 cat /proc/driver/nvidia/version nvidia-smi 磁盘分区挂载 parted $ parted /dev/sdb mklabel gpt $ parted /dev/sdb mkpart primary xfs 0% 100% $ mkfs.xfs /dev/sdb1 $ mount /dev/sdb1 /data $ df -hT /data 文件系统 类型 容量 已用 可用 已用% 挂载点 /dev/sdb1 xfs 100G 33M 100G 1% /data $ vim /etc/fstab /dev/sdb1 /data xfs defaults 0 0 # 实现开机自动挂载 fdisk fdisk /dev/sdb n 进入分区状态 p 主分区 分区大小 默认即可 w 保存 mke2fs -t ext4 /dev/sdb1 mount /dev/sdb1 ~/newpath 修改 /etc/fstab UUID=c61117ca-9176-4d0b-be4d-1b0f434359a7 /newpath ext4 defaults 0 0 UUID 的获取可以通过这个命令 blkid /dev/sdb1 最后执行 mount -a 节点集群间迁移 kubeadm reset systemctl stop kubelet systemctl stop docker rm -rf /var/lib/cni/ rm -rf /var/lib/kubelet/* rm -rf /etc/cni/ rm -rf /var/lib/etcd/* ifconfig cni0 down ifconfig flannel.1 down ifconfig docker0 down ip link set cni0 down && ip link set flannel.1 down ip link delete cni0 && ip link delete flannel.1 systemctl restart docker && systemctl restart kubelet rm -rf /root/.kube/config 然后执行命令加入到其他集群 master重新加入集群 #我们有时候会有删除master节点，再重新加入master节点的需求，比如master机器改名。这里注意重新加入时，经常会出现etcd报错 [check-etcd] Checking that the etcd cluster is healthy error executionini phase check-etcd: etcd cluster is not healthy: failed to dial endpoint https://ip:2379 with maintenance client: context deadline exceeded #这个时候，就需要去还没有停止的master节点里的etcd的pod里去，删除该老master节点对应的etcd信息 kubectl drain master01 kubectl delete node master01 #master01 执行 kubeadm reset rm -rf /etc/kubernetes/manifests/ kubectl exec -it etcd-master02 sh etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key member list #找到对应的hash etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key member remove 12637f5ec2bd02b8 kubeadm init phase upload-certs --upload-certs # 返回certificates-key #执行加入命令 You can now join any number of the control-plane node running the following command on each as root: kubeadm join 172.16.101.211:9443 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:ae579faf241a307a860d0a9e9ba1e308fe0e7a6006b90ece97eb42dfe9fc59b8 \ --control-plane --certificate-key 79d731d185a93121e73899c10445f5fcaeac8d33155f5402c48bed5543f59e3b Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use "kubeadm init phase upload-certs --upload-certs" to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.16.101.211:9443 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:ae579faf241a307a860d0a9e9ba1e308fe0e7a6006b90ece97eb42dfe9fc59b8 kubeadm join 172.16.101.211:9443 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:ae579faf241a307a860d0a9e9ba1e308fe0e7a6006b90ece97eb42dfe9fc59b8 \ --control-plane --certificate-key 28d8cd85b5b90fe7603c599915521c20dc5ab5e6be28b52d904b44efb91eed19 离线安装教程 https://cloud.tencent.com/developer/article/2165251 离线安装NFS'><meta itemprop=name content="Installation"><meta itemprop=description content='部署脚本： https://gitee.com/MLcore-Engine/ai-infra-deploy update core #配置阿里yum源命令 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo #运行以下命令生成缓存 yum clean all yum makecache yum -y update //load pubilc key install elrepo rpm -import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm //install el repo meta yum list available --disablerepo=&#39;*&#39; --enablerepo=elrepo-kernel //view available install eprepo yum --disablerepo=\* --enablerepo=elrepo-kernel list kernel #install kernel yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64 #yum -y --enablerepo=elrepo-kernel install kernel-ml.x86_64 kernel-ml-tools.x86_64 sudo yum --enablerepo=elrepo-kernel install kernel-ml kernel-ml-devel startup kernel sequence #view all available kernel on the system awk -F\&#39; &#39;$1=="menuentry " {print $2}&#39; /etc/grub2.cfg #set boot new kernel grub2-set-default 0 #运行grub2-mkconfig命令来重新创建内核配置 grub2-mkconfig -o /boot/grub2/grub.cfg reboot basic environment systemctl stop firewalld && systemctl disable firewalld sed -i &#39;/^SELINUX=/c SELINUX=disabled&#39; /etc/selinux/config setenforce 0 swapoff -a sed -i &#39;s/^.*centos-swap/#&/g&#39; /etc/fstab cat << EOF >> /etc/hosts master 192.168.31.127 kube 192.168.31.128 EOF // # 激活 br_netfilter 模块 modprobe br_netfilter cat << EOF > /etc/modules-load.d/k8s.conf br_netfilter EOF # 内核参数设置：开启IP转发，允许iptables对bridge的数据进行处理 cat << EOF > /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # 立即生效 apt-get install -y apt-transport-https ca-certificates curl gpg #time sync #master node yum install -y chrony sed -i &#39;s/^server/#&/&#39; /etc/chrony.conf cat >> /etc/chrony.conf << EOF server ntp1.aliyun.com iburst local stratum 10 allow EOF systemctl restart chronyd && systemctl enable chronyd #node yum install -y chrony sed -i &#39;s/^server/#&/&#39; /etc/chrony.conf cat >> /etc/chrony.conf << EOF server 172.16.101.11 iburst EOF systemctl restart chronyd && systemctl enable chronyd #找到最大的磁盘 挂载到/data目录 ln -s /data/kubelet /var/lib/kubelet ln -s /data/docker /var/lib/docker install docker #centos sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-engine sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo #latest sudo yum install docker-ce docker-ce-cli containerd.io -y ##ubuntu for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done # Add Docker&#39;s official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \ "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \ $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update #latest sudo apt-get install docker-ce docker-ce-cli containerd.io yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce-20.10 systemctl enable docker && systemctl start docker cat << EOF > /etc/docker/daemon.json { "registry-mirrors": ["https://h3blxdss.mirror.aliyuncs.com"], "exec-opts": ["native.cgroupdriver=systemd"], "insecure-registries": ["registry-tgq.harbor.com"] } EOF systemctl daemon-reload && systemctl restart docker install kubeadm cat << EOF > /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum install -y --nogpgcheck kubelet-1.23.6 kubeadm-1.23.6 kubectl-1.23.6 yum install --nogpgcheck --downloadonly --downloaddir=/opt/kubeadm kubelet-1.23.6 kubeadm-1.23.6 kubectl-1.23.6 #yum install -y kubelet kubeadm kubectl systemctl enable kubelet && systemctl start kubelet kubeadm config images list kubeadm config images pull --image-repository=registry.aliyuncs.com/google_containers --kubernetes-version=v1.23.6 kubeadm config images list --image-repository=registry.aliyuncs.com/google_containers init master kubeadm init --image-repository=registry.aliyuncs.com/google_containers --kubernetes-version=v1.23.6 --service-cidr=10.1.0.0/16 --pod-network-cidr=10.244.0.0/16 #--apiserver-cert-extra-sans 公网ip或者域名 # 要开始使用集群，您需要以常规用户身份运行以下命令 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 或者，如果您是root用户，则可以运行允许命令 export KUBECONFIG=/etc/kubernetes/admin.conf #部署flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml # 注意修改ip pool 与 --pod-network-cidr 一致 # 部署calico 把calico.yaml里pod所在网段改成kubeadm init时选项--pod-network-cidr所指定的网段 - name: CALICO_IPV4POOL_CIDR value: "10.244.0.0/16" # Disable file logging so `kubectl logs` works. - name: CALICO_DISABLE_FILE_LOGGING value: "true" # deploy controller oeprator kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/tigera-operator.yaml #deploy resource kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/custom-resources.yaml flannel yaml --- kind: Namespace apiVersion: v1 metadata: name: kube-flannel labels: k8s-app: flannel pod-security.kubernetes.io/enforce: privileged --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: flannel name: flannel rules: - apiGroups: - "" resources: - pods verbs: - get - apiGroups: - "" resources: - nodes verbs: - get - list - watch - apiGroups: - "" resources: - nodes/status verbs: - patch - apiGroups: - networking.k8s.io resources: - clustercidrs verbs: - list - watch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: flannel name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-flannel --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: flannel name: flannel namespace: kube-flannel --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-flannel labels: tier: node k8s-app: flannel app: flannel data: cni-conf.json: | { "name": "cbr0", "cniVersion": "0.3.1", "plugins": [ { "type": "flannel", "delegate": { "hairpinMode": true, "isDefaultGateway": true } }, { "type": "portmap", "capabilities": { "portMappings": true } } ] } net-conf.json: | { "Network": "10.244.0.0/16", "Backend": { "Type": "vxlan" } } --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds namespace: kube-flannel labels: tier: node app: flannel k8s-app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni-plugin image: docker.io/flannel/flannel-cni-plugin:v1.1.2 #image: docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.2 command: - cp args: - -f - /flannel - /opt/cni/bin/flannel volumeMounts: - name: cni-plugin mountPath: /opt/cni/bin - name: install-cni image: docker.io/flannel/flannel:v0.22.0 #image: docker.io/rancher/mirrored-flannelcni-flannel:v0.22.0 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: docker.io/flannel/flannel:v0.22.0 #image: docker.io/rancher/mirrored-flannelcni-flannel:v0.22.0 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: "100m" memory: "50Mi" securityContext: privileged: false capabilities: add: ["NET_ADMIN", "NET_RAW"] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: EVENT_QUEUE_DEPTH value: "5000" volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ - name: xtables-lock mountPath: /run/xtables.lock volumes: - name: run hostPath: path: /run/flannel - name: cni-plugin hostPath: path: /opt/cni/bin - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg - name: xtables-lock hostPath: path: /run/xtables.lock type: FileOrCreate calico yaml # This section includes base Calico installation configuration. # For more information, see: https://projectcalico.docs.tigera.io/master/reference/installation/api#operator.tigera.io/v1.Installation apiVersion: operator.tigera.io/v1 kind: Installation metadata: name: default spec: # Configures Calico networking. calicoNetwork: # Note: The ipPools section cannot be modified post-install. ipPools: - blockSize: 26 cidr: 192.168.0.0/16 encapsulation: VXLANCrossSubnet natOutgoing: Enabled nodeSelector: all() --- # This section configures the Calico API server. # For more information, see: https://projectcalico.docs.tigera.io/master/reference/installation/api#operator.tigera.io/v1.APIServer apiVersion: operator.tigera.io/v1 kind: APIServer metadata: name: default spec: {} add node #get token and ca kubeadm token list | awk -F" " &#39;{print $1}&#39; |tail -n 1 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed &#39;s/^ .* //&#39; kubeadm join 192.168.31.127:6443 --token <toekn> --discovery-token-ca-cert-hash sha256:<ca> GPU install nvidia-docker2 #Setup the repository and the GPG key: distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \ && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.repo | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo sudo yum clean expire-cache sudo yum install -y nvidia-docker2 sudo systemctl restart docker sudo docker run --rm --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi install nvidia-driver for centos7 #update version sudo yum clean all sudo yum update #验证系统内核版本和安装开发包 sudo yum install -y gcc gcc-c++ kernel-devel-$(uname -r) kernel-headers-$(uname -r) #验证gcc的版本 gcc --version #由于CUDA 11.3要求GCC的版本是6以上，下面是安装GCC7的脚本 sudo yum install centos-release-scl sudo yum install devtoolset-7 # launch a new shell instance using the Software Collection scl tool: scl enable devtoolset-7 bash gcc --version #检查当前驱动情况 sudo yum install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm sudo yum install nvidia-detect # 安装nvida-detect nvidia-detect -v # 检测能够升级到的驱动器版本 cat /proc/driver/nvidia/version # 查看当前驱动版本 #卸载之前驱动。如果第一次安装，忽略 sudo /usr/bin/nvidia-uninstall #屏蔽nouveau显卡驱动，把nvidiafb从屏蔽列表中移除 sudo rm -rf disable-nouveau.conf cat << EOF > disable-nouveau.conf blacklist nouveau options nouveau modeset=0 EOF sudo chown root:root disable-nouveau.conf sudo chmod 644 disable-nouveau.conf sudo mv disable-nouveau.conf /etc/modprobe.d/ cat /etc/modprobe.d/disable-nouveau.conf ll /etc/modprobe.d/disable-nouveau.conf #重建 initramfs 镜像 sudo systemctl set-default multi-user.target #设置运行级别为文本模式 sudo shutdown -r now #下载驱动包 NVIDIA-Linux-x86_64-525.60.13.run (https://www.nvidia.com/Download/index.aspx?lang=en-us) NVIDIA-Linux-x86_64-525.60.13.run sudo rpm -ivh http://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-ml-devel-6.3.2-1.el7.elrepo.x86_64.rpm lsmod | grep nouveau #查看nouveau是否已经禁用, 应该没有返回内容 nvidia_run=NVIDIA-Linux-x86_64-460.84.run chmod 755 $nvidia_run sudo ./$nvidia_run sudo systemctl set-default graphical.target #设置运行级别回图形模式 sudo systemctl get-default sudo shutdown -r now #查看是否安装成功 cat /proc/driver/nvidia/version nvidia-smi 磁盘分区挂载 parted $ parted /dev/sdb mklabel gpt $ parted /dev/sdb mkpart primary xfs 0% 100% $ mkfs.xfs /dev/sdb1 $ mount /dev/sdb1 /data $ df -hT /data 文件系统 类型 容量 已用 可用 已用% 挂载点 /dev/sdb1 xfs 100G 33M 100G 1% /data $ vim /etc/fstab /dev/sdb1 /data xfs defaults 0 0 # 实现开机自动挂载 fdisk fdisk /dev/sdb n 进入分区状态 p 主分区 分区大小 默认即可 w 保存 mke2fs -t ext4 /dev/sdb1 mount /dev/sdb1 ~/newpath 修改 /etc/fstab UUID=c61117ca-9176-4d0b-be4d-1b0f434359a7 /newpath ext4 defaults 0 0 UUID 的获取可以通过这个命令 blkid /dev/sdb1 最后执行 mount -a 节点集群间迁移 kubeadm reset systemctl stop kubelet systemctl stop docker rm -rf /var/lib/cni/ rm -rf /var/lib/kubelet/* rm -rf /etc/cni/ rm -rf /var/lib/etcd/* ifconfig cni0 down ifconfig flannel.1 down ifconfig docker0 down ip link set cni0 down && ip link set flannel.1 down ip link delete cni0 && ip link delete flannel.1 systemctl restart docker && systemctl restart kubelet rm -rf /root/.kube/config 然后执行命令加入到其他集群 master重新加入集群 #我们有时候会有删除master节点，再重新加入master节点的需求，比如master机器改名。这里注意重新加入时，经常会出现etcd报错 [check-etcd] Checking that the etcd cluster is healthy error executionini phase check-etcd: etcd cluster is not healthy: failed to dial endpoint https://ip:2379 with maintenance client: context deadline exceeded #这个时候，就需要去还没有停止的master节点里的etcd的pod里去，删除该老master节点对应的etcd信息 kubectl drain master01 kubectl delete node master01 #master01 执行 kubeadm reset rm -rf /etc/kubernetes/manifests/ kubectl exec -it etcd-master02 sh etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key member list #找到对应的hash etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key member remove 12637f5ec2bd02b8 kubeadm init phase upload-certs --upload-certs # 返回certificates-key #执行加入命令 You can now join any number of the control-plane node running the following command on each as root: kubeadm join 172.16.101.211:9443 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:ae579faf241a307a860d0a9e9ba1e308fe0e7a6006b90ece97eb42dfe9fc59b8 \ --control-plane --certificate-key 79d731d185a93121e73899c10445f5fcaeac8d33155f5402c48bed5543f59e3b Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use "kubeadm init phase upload-certs --upload-certs" to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.16.101.211:9443 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:ae579faf241a307a860d0a9e9ba1e308fe0e7a6006b90ece97eb42dfe9fc59b8 kubeadm join 172.16.101.211:9443 --token abcdef.0123456789abcdef \ --discovery-token-ca-cert-hash sha256:ae579faf241a307a860d0a9e9ba1e308fe0e7a6006b90ece97eb42dfe9fc59b8 \ --control-plane --certificate-key 28d8cd85b5b90fe7603c599915521c20dc5ab5e6be28b52d904b44efb91eed19 离线安装教程 https://cloud.tencent.com/developer/article/2165251 离线安装NFS'><meta itemprop=datePublished content="2025-04-03T15:16:15+08:00"><meta itemprop=dateModified content="2025-04-03T15:16:15+08:00"><meta itemprop=wordCount content="2807"><meta itemprop=image content="https://mlcore-engine.github.io/home/me.png"></head><body><div class=container><header><h1>高新 | AI平台开发工程师</h1><a href=https://github.com/mlcore-engine/mlcore-engine class=github><i class="fab fa-github"></i></a></header><div class=content-container><main><h1>Installation</h1><h2 id=部署脚本>部署脚本： <a href=https://gitee.com/MLcore-Engine/ai-infra-deploy>https://gitee.com/MLcore-Engine/ai-infra-deploy</a></h2><h2 id=update-core>update core</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#配置阿里yum源命令</span>
</span></span><span style=display:flex><span>curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
</span></span><span style=display:flex><span>curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#运行以下命令生成缓存</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>yum clean all
</span></span><span style=display:flex><span>yum makecache
</span></span><span style=display:flex><span>yum -y update
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>//load pubilc key install elrepo
</span></span><span style=display:flex><span>rpm -import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
</span></span><span style=display:flex><span>rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>//install el repo meta
</span></span><span style=display:flex><span>yum list available --disablerepo<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;*&#39;</span> --enablerepo<span style=color:#f92672>=</span>elrepo-kernel
</span></span><span style=display:flex><span>//view available install eprepo
</span></span><span style=display:flex><span>yum --disablerepo<span style=color:#f92672>=</span><span style=color:#ae81ff>\*</span> --enablerepo<span style=color:#f92672>=</span>elrepo-kernel list kernel
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#install kernel</span>
</span></span><span style=display:flex><span>yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64
</span></span><span style=display:flex><span><span style=color:#75715e>#yum -y --enablerepo=elrepo-kernel install kernel-ml.x86_64 kernel-ml-tools.x86_64</span>
</span></span><span style=display:flex><span>sudo yum --enablerepo<span style=color:#f92672>=</span>elrepo-kernel install kernel-ml kernel-ml-devel
</span></span></code></pre></div><h2 id=startup-kernel-sequence>startup kernel sequence</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#view all available kernel on the system</span>
</span></span><span style=display:flex><span>awk -F<span style=color:#ae81ff>\&#39;</span> <span style=color:#e6db74>&#39;$1==&#34;menuentry &#34; {print $2}&#39;</span> /etc/grub2.cfg
</span></span><span style=display:flex><span>          
</span></span><span style=display:flex><span><span style=color:#75715e>#set boot new kernel</span>
</span></span><span style=display:flex><span>grub2-set-default <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#运行grub2-mkconfig命令来重新创建内核配置</span>
</span></span><span style=display:flex><span>grub2-mkconfig -o /boot/grub2/grub.cfg
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>reboot
</span></span></code></pre></div><h2 id=basic-environment>basic environment</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl stop firewalld <span style=color:#f92672>&amp;&amp;</span> systemctl disable firewalld
</span></span><span style=display:flex><span>sed -i <span style=color:#e6db74>&#39;/^SELINUX=/c SELINUX=disabled&#39;</span> /etc/selinux/config
</span></span><span style=display:flex><span>setenforce <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>swapoff -a
</span></span><span style=display:flex><span>sed -i <span style=color:#e6db74>&#39;s/^.*centos-swap/#&amp;/g&#39;</span> /etc/fstab
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cat <span style=color:#e6db74>&lt;&lt; EOF &gt;&gt; /etc/hosts
</span></span></span><span style=display:flex><span><span style=color:#e6db74>master 192.168.31.127
</span></span></span><span style=display:flex><span><span style=color:#e6db74>kube 192.168.31.128
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>//
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 激活 br_netfilter 模块</span>
</span></span><span style=display:flex><span>modprobe br_netfilter
</span></span><span style=display:flex><span>cat <span style=color:#e6db74>&lt;&lt; EOF &gt; /etc/modules-load.d/k8s.conf
</span></span></span><span style=display:flex><span><span style=color:#e6db74>br_netfilter
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 内核参数设置：开启IP转发，允许iptables对bridge的数据进行处理</span>
</span></span><span style=display:flex><span>cat <span style=color:#e6db74>&lt;&lt; EOF &gt; /etc/sysctl.d/k8s.conf 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>net.ipv4.ip_forward = 1
</span></span></span><span style=display:flex><span><span style=color:#e6db74>net.bridge.bridge-nf-call-iptables = 1
</span></span></span><span style=display:flex><span><span style=color:#e6db74>net.bridge.bridge-nf-call-ip6tables = 1
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 立即生效</span>
</span></span><span style=display:flex><span> apt-get install -y apt-transport-https ca-certificates curl gpg
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#time sync</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#master node</span>
</span></span><span style=display:flex><span>yum install -y chrony
</span></span><span style=display:flex><span>sed -i <span style=color:#e6db74>&#39;s/^server/#&amp;/&#39;</span> /etc/chrony.conf
</span></span><span style=display:flex><span>cat &gt;&gt; /etc/chrony.conf <span style=color:#e6db74>&lt;&lt; EOF
</span></span></span><span style=display:flex><span><span style=color:#e6db74>server ntp1.aliyun.com iburst
</span></span></span><span style=display:flex><span><span style=color:#e6db74>local stratum 10
</span></span></span><span style=display:flex><span><span style=color:#e6db74>allow
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>systemctl restart chronyd <span style=color:#f92672>&amp;&amp;</span> systemctl enable chronyd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#node</span>
</span></span><span style=display:flex><span>yum install -y chrony
</span></span><span style=display:flex><span>sed -i <span style=color:#e6db74>&#39;s/^server/#&amp;/&#39;</span> /etc/chrony.conf
</span></span><span style=display:flex><span>cat &gt;&gt; /etc/chrony.conf  <span style=color:#e6db74>&lt;&lt; EOF
</span></span></span><span style=display:flex><span><span style=color:#e6db74>server 172.16.101.11 iburst
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>systemctl restart chronyd <span style=color:#f92672>&amp;&amp;</span> systemctl enable chronyd
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-zsh data-lang=zsh><span style=display:flex><span><span style=color:#75715e>#找到最大的磁盘  挂载到/data目录</span>
</span></span><span style=display:flex><span>ln -s /data/kubelet /var/lib/kubelet
</span></span><span style=display:flex><span>ln -s /data/docker /var/lib/docker
</span></span></code></pre></div><h2 id=install-docker>install docker</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e>#centos</span>
</span></span><span style=display:flex><span>sudo yum remove docker <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                  docker-client <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                  docker-client-latest <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                  docker-common <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                  docker-latest <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                  docker-latest-logrotate <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                  docker-logrotate <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>                  docker-engine
</span></span><span style=display:flex><span>                  
</span></span><span style=display:flex><span>sudo yum install -y yum-utils
</span></span><span style=display:flex><span>sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
</span></span><span style=display:flex><span><span style=color:#75715e>#latest</span>
</span></span><span style=display:flex><span>sudo yum install docker-ce docker-ce-cli containerd.io -y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>##ubuntu</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; <span style=color:#66d9ef>do</span> sudo apt-get remove $pkg; <span style=color:#66d9ef>done</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Add Docker&#39;s official GPG key:</span>
</span></span><span style=display:flex><span>sudo apt-get update
</span></span><span style=display:flex><span>sudo apt-get install ca-certificates curl
</span></span><span style=display:flex><span>sudo install -m <span style=color:#ae81ff>0755</span> -d /etc/apt/keyrings
</span></span><span style=display:flex><span>sudo curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
</span></span><span style=display:flex><span>sudo chmod a+r /etc/apt/keyrings/docker.asc
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Add the repository to Apt sources:</span>
</span></span><span style=display:flex><span>echo <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  <span style=color:#e6db74>&#34;deb [arch=</span><span style=color:#66d9ef>$(</span>dpkg --print-architecture<span style=color:#66d9ef>)</span><span style=color:#e6db74> signed-by=/etc/apt/keyrings/docker.asc] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  </span><span style=color:#66d9ef>$(</span>. /etc/os-release <span style=color:#f92672>&amp;&amp;</span> echo <span style=color:#e6db74>&#34;</span>$VERSION_CODENAME<span style=color:#e6db74>&#34;</span><span style=color:#66d9ef>)</span><span style=color:#e6db74> stable&#34;</span> | <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null
</span></span><span style=display:flex><span>sudo apt-get update
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#latest</span>
</span></span><span style=display:flex><span>sudo apt-get install docker-ce docker-ce-cli containerd.io 
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>yum install -y yum-utils device-mapper-persistent-data lvm2
</span></span><span style=display:flex><span>yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
</span></span><span style=display:flex><span>yum install -y docker-ce-20.10
</span></span><span style=display:flex><span>systemctl enable docker <span style=color:#f92672>&amp;&amp;</span> systemctl start docker 
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#e6db74>&lt;&lt; EOF &gt; /etc/docker/daemon.json
</span></span></span><span style=display:flex><span><span style=color:#e6db74>{
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;registry-mirrors&#34;: [&#34;https://h3blxdss.mirror.aliyuncs.com&#34;],
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;insecure-registries&#34;: [&#34;registry-tgq.harbor.com&#34;]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>}
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span> systemctl daemon-reload <span style=color:#f92672>&amp;&amp;</span> systemctl restart docker
</span></span></code></pre></div><h2 id=install-kubeadm>install kubeadm</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#e6db74>&lt;&lt; EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span></span></span><span style=display:flex><span><span style=color:#e6db74>[kubernetes]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>name=Kubernetes
</span></span></span><span style=display:flex><span><span style=color:#e6db74>baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
</span></span></span><span style=display:flex><span><span style=color:#e6db74>enabled=1
</span></span></span><span style=display:flex><span><span style=color:#e6db74>gpgcheck=1
</span></span></span><span style=display:flex><span><span style=color:#e6db74>repo_gpgcheck=1
</span></span></span><span style=display:flex><span><span style=color:#e6db74>gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>yum install -y --nogpgcheck kubelet-1.23.6 kubeadm-1.23.6 kubectl-1.23.6
</span></span><span style=display:flex><span>yum install --nogpgcheck  --downloadonly --downloaddir<span style=color:#f92672>=</span>/opt/kubeadm kubelet-1.23.6 kubeadm-1.23.6 kubectl-1.23.6
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#yum install -y kubelet kubeadm kubectl</span>
</span></span><span style=display:flex><span>systemctl enable kubelet <span style=color:#f92672>&amp;&amp;</span> systemctl start kubelet
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span> kubeadm config images list 
</span></span><span style=display:flex><span> kubeadm config images pull  --image-repository<span style=color:#f92672>=</span>registry.aliyuncs.com/google_containers  --kubernetes-version<span style=color:#f92672>=</span>v1.23.6
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm config images list --image-repository<span style=color:#f92672>=</span>registry.aliyuncs.com/google_containers
</span></span></code></pre></div><h2 id=init-master>init master</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubeadm init --image-repository<span style=color:#f92672>=</span>registry.aliyuncs.com/google_containers  --kubernetes-version<span style=color:#f92672>=</span>v1.23.6 --service-cidr<span style=color:#f92672>=</span>10.1.0.0/16 --pod-network-cidr<span style=color:#f92672>=</span>10.244.0.0/16
</span></span><span style=display:flex><span><span style=color:#75715e>#--apiserver-cert-extra-sans 公网ip或者域名</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 要开始使用集群，您需要以常规用户身份运行以下命令</span>
</span></span><span style=display:flex><span>mkdir -p $HOME/.kube
</span></span><span style=display:flex><span>sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
</span></span><span style=display:flex><span>sudo chown <span style=color:#66d9ef>$(</span>id -u<span style=color:#66d9ef>)</span>:<span style=color:#66d9ef>$(</span>id -g<span style=color:#66d9ef>)</span> $HOME/.kube/config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 或者，如果您是root用户，则可以运行允许命令</span>
</span></span><span style=display:flex><span>export KUBECONFIG<span style=color:#f92672>=</span>/etc/kubernetes/admin.conf
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#部署flannel</span>
</span></span><span style=display:flex><span>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</span></span><span style=display:flex><span><span style=color:#75715e># 注意修改ip pool 与 --pod-network-cidr 一致</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 部署calico</span>
</span></span><span style=display:flex><span>把calico.yaml里pod所在网段改成kubeadm init时选项--pod-network-cidr所指定的网段 
</span></span><span style=display:flex><span>- name: CALICO_IPV4POOL_CIDR
</span></span><span style=display:flex><span>value: <span style=color:#e6db74>&#34;10.244.0.0/16&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Disable file logging so `kubectl logs` works.</span>
</span></span><span style=display:flex><span>- name: CALICO_DISABLE_FILE_LOGGING
</span></span><span style=display:flex><span>value: <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># deploy controller oeprator</span>
</span></span><span style=display:flex><span>kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/tigera-operator.yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#deploy resource</span>
</span></span><span style=display:flex><span>kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/custom-resources.yaml
</span></span></code></pre></div><h4 id=flannel-yaml>flannel yaml</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Namespace</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kube-flannel</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>k8s-app</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>pod-security.kubernetes.io/enforce</span>: <span style=color:#ae81ff>privileged</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ClusterRole</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>k8s-app</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span><span style=color:#f92672>rules</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>apiGroups</span>:
</span></span><span style=display:flex><span>  - <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>pods</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>verbs</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>get</span>
</span></span><span style=display:flex><span>- <span style=color:#f92672>apiGroups</span>:
</span></span><span style=display:flex><span>  - <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>nodes</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>verbs</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>get</span>
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>list</span>
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>watch</span>
</span></span><span style=display:flex><span>- <span style=color:#f92672>apiGroups</span>:
</span></span><span style=display:flex><span>  - <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>nodes/status</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>verbs</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>patch</span>
</span></span><span style=display:flex><span>- <span style=color:#f92672>apiGroups</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>networking.k8s.io</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>clustercidrs</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>verbs</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>list</span>
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>watch</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ClusterRoleBinding</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>k8s-app</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span><span style=color:#f92672>roleRef</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>apiGroup</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ClusterRole</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span><span style=color:#f92672>subjects</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ServiceAccount</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-flannel</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ServiceAccount</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>k8s-app</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-flannel</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ConfigMap</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kube-flannel-cfg</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-flannel</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>tier</span>: <span style=color:#ae81ff>node</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>k8s-app</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span><span style=color:#f92672>data</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>cni-conf.json</span>: |<span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      &#34;name&#34;: &#34;cbr0&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      &#34;cniVersion&#34;: &#34;0.3.1&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      &#34;plugins&#34;: [
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          &#34;type&#34;: &#34;flannel&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          &#34;delegate&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            &#34;hairpinMode&#34;: true,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            &#34;isDefaultGateway&#34;: true
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          }
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        },
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          &#34;type&#34;: &#34;portmap&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          &#34;capabilities&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            &#34;portMappings&#34;: true
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          }
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        }
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      ]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    }</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>net-conf.json</span>: |<span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      &#34;Network&#34;: &#34;10.244.0.0/16&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      &#34;Backend&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;Type&#34;: &#34;vxlan&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      }
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    }</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>DaemonSet</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kube-flannel-ds</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-flannel</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>tier</span>: <span style=color:#ae81ff>node</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>k8s-app</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>tier</span>: <span style=color:#ae81ff>node</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>affinity</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>nodeAffinity</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>requiredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>nodeSelectorTerms</span>:
</span></span><span style=display:flex><span>            - <span style=color:#f92672>matchExpressions</span>:
</span></span><span style=display:flex><span>              - <span style=color:#f92672>key</span>: <span style=color:#ae81ff>kubernetes.io/os</span>
</span></span><span style=display:flex><span>                <span style=color:#f92672>operator</span>: <span style=color:#ae81ff>In</span>
</span></span><span style=display:flex><span>                <span style=color:#f92672>values</span>:
</span></span><span style=display:flex><span>                - <span style=color:#ae81ff>linux</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>hostNetwork</span>: <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>priorityClassName</span>: <span style=color:#ae81ff>system-node-critical</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>tolerations</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>operator</span>: <span style=color:#ae81ff>Exists</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>effect</span>: <span style=color:#ae81ff>NoSchedule</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>serviceAccountName</span>: <span style=color:#ae81ff>flannel</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>initContainers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>install-cni-plugin</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>image</span>: <span style=color:#ae81ff>docker.io/flannel/flannel-cni-plugin:v1.1.2</span>
</span></span><span style=display:flex><span>       <span style=color:#75715e>#image: docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.2</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>command</span>:
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>cp</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>args</span>:
</span></span><span style=display:flex><span>        - -<span style=color:#ae81ff>f</span>
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>/flannel</span>
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>/opt/cni/bin/flannel</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cni-plugin</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/opt/cni/bin</span>
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>install-cni</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>image</span>: <span style=color:#ae81ff>docker.io/flannel/flannel:v0.22.0</span>
</span></span><span style=display:flex><span>       <span style=color:#75715e>#image: docker.io/rancher/mirrored-flannelcni-flannel:v0.22.0</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>command</span>:
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>cp</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>args</span>:
</span></span><span style=display:flex><span>        - -<span style=color:#ae81ff>f</span>
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>/etc/kube-flannel/cni-conf.json</span>
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>/etc/cni/net.d/10-flannel.conflist</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cni</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/etc/cni/net.d</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel-cfg</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/etc/kube-flannel/</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kube-flannel</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>image</span>: <span style=color:#ae81ff>docker.io/flannel/flannel:v0.22.0</span>
</span></span><span style=display:flex><span>       <span style=color:#75715e>#image: docker.io/rancher/mirrored-flannelcni-flannel:v0.22.0</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>command</span>:
</span></span><span style=display:flex><span>        - <span style=color:#ae81ff>/opt/bin/flanneld</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>args</span>:
</span></span><span style=display:flex><span>        - --<span style=color:#ae81ff>ip-masq</span>
</span></span><span style=display:flex><span>        - --<span style=color:#ae81ff>kube-subnet-mgr</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;100m&#34;</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;50Mi&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>securityContext</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>privileged</span>: <span style=color:#66d9ef>false</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>capabilities</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>add</span>: [<span style=color:#e6db74>&#34;NET_ADMIN&#34;</span>, <span style=color:#e6db74>&#34;NET_RAW&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:#f92672>env</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>POD_NAME</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>valueFrom</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>fieldRef</span>:
</span></span><span style=display:flex><span>              <span style=color:#f92672>fieldPath</span>: <span style=color:#ae81ff>metadata.name</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>POD_NAMESPACE</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>valueFrom</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>fieldRef</span>:
</span></span><span style=display:flex><span>              <span style=color:#f92672>fieldPath</span>: <span style=color:#ae81ff>metadata.namespace</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>EVENT_QUEUE_DEPTH</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>value</span>: <span style=color:#e6db74>&#34;5000&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>run</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/run/flannel</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel-cfg</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/etc/kube-flannel/</span>
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>xtables-lock</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/run/xtables.lock</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>run</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>hostPath</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/run/flannel</span>
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cni-plugin</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>hostPath</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/opt/cni/bin</span>
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cni</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>hostPath</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/etc/cni/net.d</span>
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel-cfg</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>configMap</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kube-flannel-cfg</span>
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>xtables-lock</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>hostPath</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/run/xtables.lock</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>type</span>: <span style=color:#ae81ff>FileOrCreate</span>
</span></span></code></pre></div><h4 id=calico-yaml>calico yaml</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#75715e># This section includes base Calico installation configuration.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># For more information, see: https://projectcalico.docs.tigera.io/master/reference/installation/api#operator.tigera.io/v1.Installation</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>operator.tigera.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Installation</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>default</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#75715e># Configures Calico networking.</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>calicoNetwork</span>:
</span></span><span style=display:flex><span>    <span style=color:#75715e># Note: The ipPools section cannot be modified post-install.</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>ipPools</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>blockSize</span>: <span style=color:#ae81ff>26</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>cidr</span>: <span style=color:#ae81ff>192.168.0.0</span><span style=color:#ae81ff>/16</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>encapsulation</span>: <span style=color:#ae81ff>VXLANCrossSubnet</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>natOutgoing</span>: <span style=color:#ae81ff>Enabled</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>nodeSelector</span>: <span style=color:#ae81ff>all()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># This section configures the Calico API server.</span>
</span></span><span style=display:flex><span><span style=color:#75715e># For more information, see: https://projectcalico.docs.tigera.io/master/reference/installation/api#operator.tigera.io/v1.APIServer</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>operator.tigera.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>APIServer</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>default</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>: {}
</span></span></code></pre></div><h2 id=add-node>add node</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#get token and ca</span>
</span></span><span style=display:flex><span>kubeadm token list | awk -F<span style=color:#e6db74>&#34; &#34;</span> <span style=color:#e6db74>&#39;{print $1}&#39;</span> |tail -n <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed <span style=color:#e6db74>&#39;s/^ .* //&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm join 192.168.31.127:6443 --token &lt;toekn&gt; --discovery-token-ca-cert-hash sha256:&lt;ca&gt;
</span></span></code></pre></div><h2 id=gpu>GPU</h2><h3 id=install-nvidia-docker2>install nvidia-docker2</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e>#Setup the repository and the GPG key:</span>
</span></span><span style=display:flex><span>distribution<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>. /etc/os-release;echo $ID$VERSION_ID<span style=color:#66d9ef>)</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>   <span style=color:#f92672>&amp;&amp;</span> curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.repo | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo yum clean expire-cache
</span></span><span style=display:flex><span>sudo yum install -y nvidia-docker2
</span></span><span style=display:flex><span>sudo systemctl restart docker
</span></span><span style=display:flex><span>sudo docker run --rm --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi
</span></span></code></pre></div><h3 id=install-nvidia-driver-for-centos7>install nvidia-driver for centos7</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e>#update version</span>
</span></span><span style=display:flex><span>sudo yum clean all
</span></span><span style=display:flex><span>sudo yum update
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#验证系统内核版本和安装开发包</span>
</span></span><span style=display:flex><span>sudo yum install -y gcc gcc-c++ kernel-devel-<span style=color:#66d9ef>$(</span>uname -r<span style=color:#66d9ef>)</span> kernel-headers-<span style=color:#66d9ef>$(</span>uname -r<span style=color:#66d9ef>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#验证gcc的版本 </span>
</span></span><span style=display:flex><span>gcc --version
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#由于CUDA 11.3要求GCC的版本是6以上，下面是安装GCC7的脚本</span>
</span></span><span style=display:flex><span>sudo yum install centos-release-scl
</span></span><span style=display:flex><span>sudo yum install devtoolset-7
</span></span><span style=display:flex><span><span style=color:#75715e># launch a new shell instance using the Software Collection scl tool:</span>
</span></span><span style=display:flex><span>scl enable devtoolset-7 bash
</span></span><span style=display:flex><span>gcc --version
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#检查当前驱动情况</span>
</span></span><span style=display:flex><span>sudo yum install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm
</span></span><span style=display:flex><span>sudo yum install nvidia-detect      <span style=color:#75715e># 安装nvida-detect</span>
</span></span><span style=display:flex><span>nvidia-detect -v                    <span style=color:#75715e># 检测能够升级到的驱动器版本</span>
</span></span><span style=display:flex><span>cat /proc/driver/nvidia/version     <span style=color:#75715e># 查看当前驱动版本</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#卸载之前驱动。如果第一次安装，忽略</span>
</span></span><span style=display:flex><span>sudo /usr/bin/nvidia-uninstall
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#屏蔽nouveau显卡驱动，把nvidiafb从屏蔽列表中移除</span>
</span></span><span style=display:flex><span>sudo rm -rf  disable-nouveau.conf
</span></span><span style=display:flex><span>cat <span style=color:#e6db74>&lt;&lt; EOF &gt; disable-nouveau.conf
</span></span></span><span style=display:flex><span><span style=color:#e6db74>blacklist nouveau
</span></span></span><span style=display:flex><span><span style=color:#e6db74>options nouveau modeset=0
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>   
</span></span><span style=display:flex><span>sudo chown root:root disable-nouveau.conf
</span></span><span style=display:flex><span>sudo chmod <span style=color:#ae81ff>644</span> disable-nouveau.conf
</span></span><span style=display:flex><span>sudo mv disable-nouveau.conf /etc/modprobe.d/
</span></span><span style=display:flex><span>   
</span></span><span style=display:flex><span>cat /etc/modprobe.d/disable-nouveau.conf
</span></span><span style=display:flex><span>ll /etc/modprobe.d/disable-nouveau.conf
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#重建 initramfs 镜像</span>
</span></span><span style=display:flex><span>sudo systemctl set-default multi-user.target    <span style=color:#75715e>#设置运行级别为文本模式</span>
</span></span><span style=display:flex><span>sudo shutdown -r now
</span></span><span style=display:flex><span>             
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#下载驱动包 NVIDIA-Linux-x86_64-525.60.13.run (https://www.nvidia.com/Download/index.aspx?lang=en-us)</span>
</span></span><span style=display:flex><span>NVIDIA-Linux-x86_64-525.60.13.run
</span></span><span style=display:flex><span>sudo rpm -ivh http://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-ml-devel-6.3.2-1.el7.elrepo.x86_64.rpm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>lsmod | grep nouveau                            <span style=color:#75715e>#查看nouveau是否已经禁用, 应该没有返回内容</span>
</span></span><span style=display:flex><span>nvidia_run<span style=color:#f92672>=</span>NVIDIA-Linux-x86_64-460.84.run
</span></span><span style=display:flex><span>chmod <span style=color:#ae81ff>755</span> $nvidia_run
</span></span><span style=display:flex><span>sudo  ./$nvidia_run
</span></span><span style=display:flex><span>sudo systemctl set-default graphical.target     <span style=color:#75715e>#设置运行级别回图形模式</span>
</span></span><span style=display:flex><span>sudo systemctl get-default
</span></span><span style=display:flex><span>sudo shutdown -r now
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#查看是否安装成功</span>
</span></span><span style=display:flex><span>cat /proc/driver/nvidia/version    
</span></span><span style=display:flex><span>nvidia-smi
</span></span></code></pre></div><h2 id=磁盘分区挂载>磁盘分区挂载</h2><h3 id=parted>parted</h3><pre tabindex=0><code>$ parted /dev/sdb mklabel gpt

$ parted /dev/sdb mkpart primary xfs 0% 100%

$ mkfs.xfs /dev/sdb1

$ mount /dev/sdb1 /data

$ df -hT /data

文件系统       类型  容量  已用  可用 已用% 挂载点

/dev/sdb1      xfs   100G   33M  100G    1% /data

$ vim /etc/fstab

/dev/sdb1   /data   xfs   defaults   0 0

# 实现开机自动挂载
</code></pre><h3 id=fdisk>fdisk</h3><pre tabindex=0><code>fdisk /dev/sdb
n 进入分区状态
p 主分区
分区大小 默认即可
w 保存
mke2fs -t ext4 /dev/sdb1
mount /dev/sdb1 ~/newpath
修改  /etc/fstab 
UUID=c61117ca-9176-4d0b-be4d-1b0f434359a7  /newpath  ext4  defaults  0  0 
UUID 的获取可以通过这个命令 blkid /dev/sdb1
最后执行 mount -a 
</code></pre><h3 id=节点集群间迁移>节点集群间迁移</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubeadm reset
</span></span><span style=display:flex><span>systemctl stop kubelet
</span></span><span style=display:flex><span>systemctl stop docker
</span></span><span style=display:flex><span>rm -rf /var/lib/cni/
</span></span><span style=display:flex><span>rm -rf /var/lib/kubelet/*
</span></span><span style=display:flex><span>rm -rf /etc/cni/
</span></span><span style=display:flex><span>rm -rf /var/lib/etcd/*
</span></span><span style=display:flex><span>ifconfig cni0 down
</span></span><span style=display:flex><span>ifconfig flannel.1 down
</span></span><span style=display:flex><span>ifconfig docker0 down
</span></span><span style=display:flex><span>ip link set cni0 down <span style=color:#f92672>&amp;&amp;</span> ip link set flannel.1 down 
</span></span><span style=display:flex><span>ip link delete cni0 <span style=color:#f92672>&amp;&amp;</span> ip link delete flannel.1
</span></span><span style=display:flex><span>systemctl restart docker <span style=color:#f92672>&amp;&amp;</span> systemctl restart kubelet
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rm -rf /root/.kube/config
</span></span><span style=display:flex><span>然后执行命令加入到其他集群
</span></span></code></pre></div><h3 id=master重新加入集群>master重新加入集群</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e>#我们有时候会有删除master节点，再重新加入master节点的需求，比如master机器改名。这里注意重新加入时，经常会出现etcd报错</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>check-etcd<span style=color:#f92672>]</span> Checking that the etcd cluster is healthy error executionini phase check-etcd: etcd cluster is not healthy: failed to dial endpoint https://ip:2379 with maintenance client: context deadline exceeded
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#这个时候，就需要去还没有停止的master节点里的etcd的pod里去，删除该老master节点对应的etcd信息</span>
</span></span><span style=display:flex><span>kubectl drain master01
</span></span><span style=display:flex><span>kubectl delete node master01
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#master01 执行</span>
</span></span><span style=display:flex><span>kubeadm reset
</span></span><span style=display:flex><span>rm -rf /etc/kubernetes/manifests/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubectl exec -it etcd-master02 sh
</span></span><span style=display:flex><span>etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key member list
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#找到对应的hash</span>
</span></span><span style=display:flex><span>etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key member remove 12637f5ec2bd02b8
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm init phase upload-certs --upload-certs  <span style=color:#75715e># 返回certificates-key</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#执行加入命令</span>
</span></span><span style=display:flex><span>You can now join any number of the control-plane node running the following command on each as root:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  kubeadm join 172.16.101.211:9443 --token abcdef.0123456789abcdef <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>        --discovery-token-ca-cert-hash sha256:ae579faf241a307a860d0a9e9ba1e308fe0e7a6006b90ece97eb42dfe9fc59b8 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>        --control-plane --certificate-key 79d731d185a93121e73899c10445f5fcaeac8d33155f5402c48bed5543f59e3b
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
</span></span><span style=display:flex><span>As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;kubeadm init phase upload-certs --upload-certs&#34;</span> to reload certs afterward.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Then you can join any number of worker nodes by running the following on each as root:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm join 172.16.101.211:9443 --token abcdef.0123456789abcdef <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>        --discovery-token-ca-cert-hash sha256:ae579faf241a307a860d0a9e9ba1e308fe0e7a6006b90ece97eb42dfe9fc59b8
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  kubeadm join 172.16.101.211:9443 --token abcdef.0123456789abcdef <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>        --discovery-token-ca-cert-hash sha256:ae579faf241a307a860d0a9e9ba1e308fe0e7a6006b90ece97eb42dfe9fc59b8 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>        --control-plane --certificate-key 28d8cd85b5b90fe7603c599915521c20dc5ab5e6be28b52d904b44efb91eed19
</span></span></code></pre></div><h3 id=离线安装教程>离线安装教程</h3><pre tabindex=0><code>https://cloud.tencent.com/developer/article/2165251
</code></pre><p>离线安装NFS</p><pre tabindex=0><code>https://blog.csdn.net/u013014761/article/details/100054241
https://qizhanming.com/blog/2018/08/08/how-to-install-nfs-on-centos-7
https://blog.csdn.net/u013014761/article/details/100054241
https://cloud.tencent.com/developer/article/2254970
</code></pre><h3 id=keepalived>keepalived</h3><pre tabindex=0><code>https://www.cnblogs.com/rexcheny/p/10778567.html
</code></pre><h3 id=ha>HA</h3><pre tabindex=0><code>https://www.cnblogs.com/wubolive/p/17140058.html#_label0_0
https://ost.51cto.com/posts/13131
https://www.linuxtechi.com/setup-highly-available-kubernetes-cluster-kubeadm/
https://developer.aliyun.com/article/1136864
https://hevodata.com/learn/kubernetes-high-availability/
</code></pre><h3 id=tecent-article>tecent article</h3><pre tabindex=0><code>https://tencentcloudcontainerteam.github.io/2019/08/12/troubleshooting-with-kubernetes-network/
https://tencentcloudcontainerteam.github.io/2019/12/15/no-route-to-host/
</code></pre><h3 id=kubeadm-离线安装包下载>kubeadm 离线安装包下载</h3><pre tabindex=0><code>yum install --downloadonly --downloaddir=/home/centos/k8s kubelet-1.23.6 kubeadm-1.23.6 kubectl-1.23.6
</code></pre><pre tabindex=0><code>external IP 不兼容 ipvs 问题
https://blog.csdn.net/qq_41586875/article/details/124330823
</code></pre><h3 id=regenerate-adminconf>regenerate admin.conf</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm init phase kubeconfig admin 
</span></span></code></pre></div><h3 id=更新证书>更新证书</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e>#查看证书过期时间</span>
</span></span><span style=display:flex><span>kubeadm certs check-expiration
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#对于低版本的k8s</span>
</span></span><span style=display:flex><span>kubeadm alpha certs check-expiration
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#备份相关文件</span>
</span></span><span style=display:flex><span>cp -r /etc/kubernetes /etc/kubernetes.old
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#在三个主节点上执行命令更新证书</span>
</span></span><span style=display:flex><span>kubeadm certs renew all
</span></span><span style=display:flex><span><span style=color:#75715e>#低版本执行</span>
</span></span><span style=display:flex><span>kubeadm alpha certs renew all
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#在每个master上重启相关服务</span>
</span></span><span style=display:flex><span>docker ps | egrep <span style=color:#e6db74>&#34;k8s_kube-apiserver|k8s_kube-scheduler|k8s_kube-controller&#34;</span> | awk <span style=color:#e6db74>&#39;{print $1}&#39;</span> | xargs docker restart 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#更新 .kube/config</span>
</span></span><span style=display:flex><span>cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
</span></span><span style=display:flex><span>chown <span style=color:#66d9ef>$(</span>id -u<span style=color:#66d9ef>)</span>:<span style=color:#66d9ef>$(</span>id -g<span style=color:#66d9ef>)</span> $HOME/.kube/config
</span></span><span style=display:flex><span><span style=color:#75715e>#参考文献： https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/</span>
</span></span></code></pre></div><h3 id=升级centos-g>升级centos g++</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#SCL(Software Collections)是一个CentOS/RHEL Linux平台的软件多版本共存解决方案，为用户提供一种方便、安全地安装和使用应用程序和运行时环境的多个版本的方式。</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#Developer Toolset是为CentOS和REHL Linux平台开发者设计的开发工具集，提供GCC工具集、GNU Debugger以及其它开发、调试、性能测试工具的不同版本。</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#因此，可以通过安装scl源并下载对应版本的devtoolset来达到管理高版本gcc、g++、gfortran等开发工具的目的。</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#安装scl源</span>
</span></span><span style=display:flex><span>yum -y install centos-release-scl
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#安装devtoolset-x</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#可以根据需要安装指定版本的devtoolset，这里以 devtooset-9 为例。</span>
</span></span><span style=display:flex><span>yum -y install devtoolset-9
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#默认安装位置为 /opt 目录</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#激活devtoolset</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#编辑 ~/.bash_profile，在文件尾部添加如下命令。</span>
</span></span><span style=display:flex><span>source /opt/rh/devtoolset-9/enable
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>source ~/.bash_profile
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#检查是否安装成功</span>
</span></span><span style=display:flex><span>gcc -v
</span></span></code></pre></div><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=https://mlcore-engine.github.io/5kubernetes/ title=Kubernetes><i class="fas fa-arrow-left" aria-hidden=true></i>&nbsp;Prev - Kubernetes</a>
<a class="nav nav-next" href=https://mlcore-engine.github.io/7golang/ title=golang>Next - golang <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlcore-engine.github.io/>Home</a></li><li><a href=https://mlcore-engine.github.io/1learn-cs/>自学CS学习路线图</a></li><li><a href=https://mlcore-engine.github.io/2learn-english/>英语学习</a></li><li><a href=https://mlcore-engine.github.io/3math-foundation/>矩阵的5种变换</a></li><li><a href=https://mlcore-engine.github.io/4transformer/>Transformer</a></li><li class="parent has-sub-menu"><a href=https://mlcore-engine.github.io/5kubernetes/>Kubernetes<span class="mark opened">-</span></a><ul class=sub-menu><li class=active><a href=https://mlcore-engine.github.io/5kubernetes/installation/>Installation</a></li></ul></li><li><a href=https://mlcore-engine.github.io/7golang/>golang</a></li><li class=has-sub-menu><a href=https://mlcore-engine.github.io/6linux-foundation/>Linux基础知识<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlcore-engine.github.io/6linux-foundation/linux-commands/>50个常用Linux命令</a></li><li><a href=https://mlcore-engine.github.io/6linux-foundation/linux-common/>Linux Common</a></li></ul></li><li class=has-sub-menu><a href=https://mlcore-engine.github.io/exercise/>Exercises<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlcore-engine.github.io/exercise/workout/>Workout</a></li></ul></li><li class=has-sub-menu><a href=https://mlcore-engine.github.io/others/>Others<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlcore-engine.github.io/others/create-hugo-gitpage/>hugo和gitPage制作个人网页</a></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>