<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Do_sample_para - 高新 | AI平台开发工程师</title>
<meta name=description content="AI平台开发工程师，专注于AI平台工程和Kubernetes云原生技术。拥有AI平台开发、GPU资源优化和AI服务部署经验"><meta name=generator content="Hugo 0.145.0"><link href=https://mlcore-engine.github.io//index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlcore-engine.github.io/transformer/do_sample_para/><link rel=stylesheet href=https://mlcore-engine.github.io/css/theme.min.css><link rel=stylesheet href=https://mlcore-engine.github.io/css/chroma.min.css><script defer src=https://mlcore-engine.github.io//js/fontawesome6/all.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js integrity="sha256-H3cjtrm/ztDeuhCN9I4yh4iN2Ybx/y1RM7rMmAesA0k=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js integrity="sha256-4XodgW4TwIJuDtf+v6vDJ39FVxI0veC/kSCCmnFp7ck=" crossorigin=anonymous></script><script src=https://mlcore-engine.github.io/js/bundle.js></script><style>@media screen and (min-width:480px){.sidebar{flex:0 0 20%!important;max-width:20%!important}main{flex:0 0 80%!important;max-width:80%!important}}body{background-color:#f8f5e6!important;font-family:kaiti,stkaiti,楷体,楷体_gb2312,simkai,华文楷体,Kai,-apple-system,BlinkMacSystemFont,segoe ui,Roboto,sans-serif!important;font-size:20px!important;line-height:1.8!important}.container,.content-container,main{background-color:#f8f5e6!important}.sidebar{background-color:inherit;font-size:16px!important}h1,h2,h3,h4,h5,h6{font-family:kaiti,stkaiti,楷体,楷体_gb2312,simkai,华文楷体,Kai,noto serif,Georgia,serif!important;font-weight:600!important;line-height:1.5!important}h1{font-size:2.4em!important}h2{font-size:2em!important}h3{font-size:1.7em!important}h4{font-size:1.5em!important}h5{font-size:1.3em!important}h6{font-size:1.2em!important}p{font-size:20px!important;margin-bottom:1.2em!important}li{font-size:20px!important;margin-bottom:.5em!important}article,.content,.post-content,main p,main li,main td,main th,blockquote,.markdown{font-size:20px!important}pre,code{font-family:jetbrains mono,Consolas,Monaco,andale mono,ubuntu mono,monospace!important;font-size:1.1em!important}a{color:#06c!important;text-decoration:none!important}a:hover{text-decoration:underline!important}table{font-size:20px!important}</style><meta property="og:url" content="https://mlcore-engine.github.io/transformer/do_sample_para/"><meta property="og:site_name" content="高新 | AI平台开发工程师"><meta property="og:title" content="Do_sample_para"><meta property="og:description" content="在大语言模型微调和采样过程中有几个重要的采样参数需要懂得，下面我将讲解三个特别重要的采样参数，这3个参数的设置会直接影响模型的表现。顺便再解释一下经常看到的一个重要概念logits。 top_k top_p temperature 背景：语言模型的概率分布 假设语言模型在预测下一个词时，面对一个包含 5 个词的词汇表：[“猫”, “狗”, “鸟”, “鱼”, “马”]。模型会为每个词分配一个概率，例如："><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="transformer"><meta property="article:published_time" content="2025-04-16T17:11:57+08:00"><meta property="article:modified_time" content="2025-04-17T13:08:49+08:00"><meta property="og:image" content="https://mlcore-engine.github.io/home/me.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://mlcore-engine.github.io/home/me.png"><meta name=twitter:title content="Do_sample_para"><meta name=twitter:description content="在大语言模型微调和采样过程中有几个重要的采样参数需要懂得，下面我将讲解三个特别重要的采样参数，这3个参数的设置会直接影响模型的表现。顺便再解释一下经常看到的一个重要概念logits。 top_k top_p temperature 背景：语言模型的概率分布 假设语言模型在预测下一个词时，面对一个包含 5 个词的词汇表：[“猫”, “狗”, “鸟”, “鱼”, “马”]。模型会为每个词分配一个概率，例如："><meta itemprop=name content="Do_sample_para"><meta itemprop=description content="在大语言模型微调和采样过程中有几个重要的采样参数需要懂得，下面我将讲解三个特别重要的采样参数，这3个参数的设置会直接影响模型的表现。顺便再解释一下经常看到的一个重要概念logits。 top_k top_p temperature 背景：语言模型的概率分布 假设语言模型在预测下一个词时，面对一个包含 5 个词的词汇表：[“猫”, “狗”, “鸟”, “鱼”, “马”]。模型会为每个词分配一个概率，例如："><meta itemprop=datePublished content="2025-04-16T17:11:57+08:00"><meta itemprop=dateModified content="2025-04-17T13:08:49+08:00"><meta itemprop=wordCount content="5913"><meta itemprop=image content="https://mlcore-engine.github.io/home/me.png"><link rel=apple-touch-icon sizes=180x180 href=/favicon/favicon.png><link rel=icon type=image/png sizes=32x32 href=/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon/favicon-16x16.png><link rel=manifest href=/favicon/site.webmanifest><link rel=mask-icon href=/favicon/safari-pinned-tab.svg color=#5bbad5><link rel="shortcut icon" href=/favicon.ico><meta name=msapplication-TileColor content="#da532c"><meta name=msapplication-config content="/favicon/browserconfig.xml"><meta name=theme-color content="#ffffff"><meta name=description content="在大语言模型微调和采样过程中有几个重要的采样参数需要懂得，下面我将讲解三个特别重要的采样参数，这3个参数的设置会直接影响模型的表现。顺便再解释一下经常看到的一个重要概念logits。 top_k top_p temperature 背景：语言模型的概率分布 假设语言模型在预测下一个词时，面对一个包含 5 个词的词汇表：[&ldquo;猫&rdquo;, &ldquo;狗&rdquo;, &ldquo;鸟&rdquo;, &ldquo;鱼&rdquo;, &ldquo;马&rdquo;]。模型会为每个词分配一个概率，例如：
"><meta name=keywords content="AI,机器学习,golang,kubernetes,技术博客"><meta name=author content="高新"><meta property="og:type" content="article"><meta property="og:url" content="https://mlcore-engine.github.io/transformer/do_sample_para/"><meta property="og:title" content="Do_sample_para | 高新 | AI平台开发工程师"><meta property="og:description" content="在大语言模型微调和采样过程中有几个重要的采样参数需要懂得，下面我将讲解三个特别重要的采样参数，这3个参数的设置会直接影响模型的表现。顺便再解释一下经常看到的一个重要概念logits。 top_k top_p temperature 背景：语言模型的概率分布 假设语言模型在预测下一个词时，面对一个包含 5 个词的词汇表：[&ldquo;猫&rdquo;, &ldquo;狗&rdquo;, &ldquo;鸟&rdquo;, &ldquo;鱼&rdquo;, &ldquo;马&rdquo;]。模型会为每个词分配一个概率，例如：
"><meta property="og:image" content="https://mlcore-engine.github.io/home/me.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:url content="https://mlcore-engine.github.io/transformer/do_sample_para/"><meta name=twitter:title content="Do_sample_para | 高新 | AI平台开发工程师"><meta name=twitter:description content="在大语言模型微调和采样过程中有几个重要的采样参数需要懂得，下面我将讲解三个特别重要的采样参数，这3个参数的设置会直接影响模型的表现。顺便再解释一下经常看到的一个重要概念logits。 top_k top_p temperature 背景：语言模型的概率分布 假设语言模型在预测下一个词时，面对一个包含 5 个词的词汇表：[&ldquo;猫&rdquo;, &ldquo;狗&rdquo;, &ldquo;鸟&rdquo;, &ldquo;鱼&rdquo;, &ldquo;马&rdquo;]。模型会为每个词分配一个概率，例如：
"><meta name=twitter:image content="https://mlcore-engine.github.io/home/me.png"><link rel=canonical href=https://mlcore-engine.github.io/transformer/do_sample_para/><link rel=stylesheet href=/css/math.css><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},svg:{fontCache:"global"}}</script><script id=MathJax-script async src=/js/mathjax/tex-svg.js></script></head><body><div class=container><header><h1>高新 | AI平台开发工程师</h1><a href=https://github.com/mlcore-engine/mlcore-engine class=github><i class="fab fa-github"></i></a><p class=description>AI平台开发工程师，专注于AI平台工程和Kubernetes云原生技术。拥有AI平台开发、GPU资源优化和AI服务部署经验</p></header><div class=content-container><main><h1>Do_sample_para</h1><h5 id=在大语言模型微调和采样过程中有几个重要的采样参数需要懂得下面我将讲解三个特别重要的采样参数这3个参数的设置会直接影响模型的表现顺便再解释一下经常看到的一个重要概念logits>在大语言模型微调和采样过程中有几个重要的采样参数需要懂得，下面我将讲解三个特别重要的采样参数，这3个参数的设置会直接影响模型的表现。顺便再解释一下经常看到的一个重要概念logits。</h5><ul><li><strong>top_k</strong></li><li><strong>top_p</strong></li><li><strong>temperature</strong></li></ul><hr><h4 id=背景语言模型的概率分布><strong>背景：语言模型的概率分布</strong></h4><p>假设语言模型在预测下一个词时，面对一个包含 5 个词的词汇表：[&ldquo;猫&rdquo;, &ldquo;狗&rdquo;, &ldquo;鸟&rdquo;, &ldquo;鱼&rdquo;, &ldquo;马&rdquo;]。模型会为每个词分配一个概率，例如：</p><ul><li>猫: 0.4</li><li>狗: 0.3</li><li>鸟: 0.15</li><li>鱼: 0.1</li><li>马: 0.05</li></ul><p>如果使用 <strong>贪婪解码</strong>（greedy decoding），模型会直接选择概率最高的词 &ldquo;猫&rdquo;。但这种方式生成的文本可能过于单一，因此我们引入 Top-K 和 Top-P 以及temperature 采样来增加多样性。</p><hr><h4 id=top-k-采样><strong>Top-K 采样</strong></h4><p><strong>Top-K 采样</strong> 是从概率最高的前 K 个词中随机选择一个词。K 的大小决定了选择的范围和输出的多样性。</p><h5 id=例子><strong>例子：</strong></h5><p>继续用上面的概率分布：</p><ul><li>猫: 0.4</li><li>狗: 0.3</li><li>鸟: 0.15</li><li>鱼: 0.1</li><li>马: 0.05</li></ul><ol><li><p><strong>K = 1</strong><br>只选择概率最高的一个词，即 &ldquo;猫&rdquo;。这等同于贪婪解码，输出完全基于最高概率，没有随机性。</p></li><li><p><strong>K = 2</strong><br>选择概率最高的前 2 个词：</p><ul><li>猫 (0.4)</li><li>狗 (0.3)<br>模型会在这两个词中随机选择一个。比如，随机选择 &ldquo;狗&rdquo;，那么输出就是 &ldquo;狗&rdquo;。相比 K=1，多了一些随机性。</li></ul></li><li><p><strong>K = 5</strong><br>选择所有 5 个词，相当于从整个词汇表中随机采样。这时输出的创造性最强，但可能会偏离高概率的词。</p></li></ol><h5 id=特点><strong>特点：</strong></h5><ul><li>K 值越高（如 K=5），输出越具创造性和多样性，因为可选的词更多。</li><li>K 值越低（如 K=1），输出越受限制，更接近事实或高概率的词。</li></ul><hr><h4 id=top-p-采样><strong>Top-P 采样</strong></h4><p><strong>Top-P 采样</strong>（也叫 nucleus sampling）是根据累积概率选择词集。它会从概率最高的词开始累加，直到累积概率达到或超过某个阈值 P，然后从这个词集中随机选择一个词。</p><h5 id=例子-1><strong>例子：</strong></h5><p>还是用相同的概率分布：</p><ul><li>猫: 0.4</li><li>狗: 0.3</li><li>鸟: 0.15</li><li>鱼: 0.1</li><li>马: 0.05</li></ul><ol><li><p><strong>P = 0.0</strong><br>理论上累积概率为 0 时不选择任何词，但实际上这等同于贪婪解码，只选 &ldquo;猫&rdquo;。</p></li><li><p><strong>P = 0.7</strong><br>从概率最高的词开始累加：</p><ul><li>猫: 0.4</li><li>猫 + 狗: 0.4 + 0.3 = 0.7<br>累积概率达到 0.7，词集为 [&ldquo;猫&rdquo;, &ldquo;狗&rdquo;]。模型在这两个词中随机选择一个，比如 &ldquo;狗&rdquo;。</li></ul></li><li><p><strong>P = 0.85</strong><br>继续累加：</p><ul><li>猫: 0.4</li><li>猫 + 狗: 0.4 + 0.3 = 0.7</li><li>猫 + 狗 + 鸟: 0.4 + 0.3 + 0.15 = 0.85<br>累积概率达到 0.85，词集为 [&ldquo;猫&rdquo;, &ldquo;狗&rdquo;, &ldquo;鸟&rdquo;]。模型从中随机选择一个，比如 &ldquo;鸟&rdquo;。</li></ul></li><li><p><strong>P = 1.0</strong><br>所有词的累积概率为 1.0，词集包含 [&ldquo;猫&rdquo;, &ldquo;狗&rdquo;, &ldquo;鸟&rdquo;, &ldquo;鱼&rdquo;, &ldquo;马&rdquo;]。模型从整个词汇表中随机选择，输出的多样性最高。</p></li></ol><h5 id=特点-1><strong>特点：</strong></h5><ul><li>P 值越高（如 P=1.0），选择的词集越大，输出越多样化。</li><li>P 值越低（如 P=0.0），选择的词集越小，输出越接近贪婪解码，更基于高概率词。</li></ul><hr><h4 id=top-k-和-top-p-的比较><strong>Top-K 和 Top-P 的比较</strong></h4><ul><li><strong>Top-K</strong>：固定选择前 K 个词，范围大小不随概率分布变化。例如，K=2 时总是选 2 个词，即使概率分布很集中或很分散。</li><li><strong>Top-P</strong>：动态调整词集大小，根据概率分布选择累积概率达到 P 的词。如果概率分布集中，词集可能很小；如果分布平坦，词集会更大。</li></ul><h4 id=举个极端例子><strong>举个极端例子：</strong></h4><p>假设概率分布极度集中：</p><ul><li><p>猫: 0.9</p></li><li><p>狗: 0.05</p></li><li><p>鸟: 0.03</p></li><li><p>鱼: 0.01</p></li><li><p>马: 0.01</p></li><li><p><strong>Top-K (K=2)</strong>：选 &ldquo;猫&rdquo; 和 &ldquo;狗&rdquo;，忽略概率差异。</p></li><li><p><strong>Top-P (P=0.9)</strong>：只选 &ldquo;猫&rdquo;，因为 0.9 已达到阈值，更贴近分布特性。</p></li></ul><hr><h4 id=实际应用><strong>实际应用</strong></h4><ul><li><strong>创意写作</strong>：用较大的 K（如 5）或 P（如 0.9），生成更具想象力的文本。</li><li><strong>问答系统</strong>：用较小的 K（如 1 或 2）或 P（如 0.5），生成更准确、基于事实的答案。</li></ul><hr><h4 id=总结><strong>总结</strong></h4><ul><li><strong>Top-K 采样</strong>：从概率最高的前 K 个词中随机选一个，K 越大越多样，K=1 等同贪婪解码。</li><li><strong>Top-P 采样</strong>：从累积概率达到 P 的词集中随机选一个，P 从 0（贪婪解码）到 1（全词汇表）。</li></ul><h4 id=temperature><strong>Temperature</strong></h4><p><strong>Temperature</strong> 是一个控制生成文本随机性和多样性的超参数，它直接调整模型输出的概率分布。</p><hr><h4 id=背景语言模型的概率分布-1><strong>背景：语言模型的概率分布</strong></h4><p>假设语言模型预测下一个词，词汇表包含 5 个词：[&ldquo;猫&rdquo;, &ldquo;狗&rdquo;, &ldquo;鸟&rdquo;, &ldquo;鱼&rdquo;, &ldquo;马&rdquo;]，原始概率分布如下：</p><ul><li>猫: 0.4</li><li>狗: 0.3</li><li>鸟: 0.15</li><li>鱼: 0.1</li><li>马: 0.05</li></ul><p>这些概率是模型的原始输出（logits 经过 softmax 转换）。Temperature 参数通过缩放 logits 来改变概率分布的"形状"，从而影响生成的随机性和多样性。</p><hr><h4 id=temperature-的作用><strong>Temperature 的作用</strong></h4><p>Temperature（记为 $ T $）是一个正数，作用于模型的 logits。公式如下：</p><ol><li><p>模型的 logits（未归一化的分数）为 $z_i$（对应每个词）。</p></li><li><p>Temperature 调整后的 logits 为 $z_i / T$。</p></li><li><p>调整后的 logits 再通过 softmax 转换为新的概率分布：</p><p>$
P(i) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
$</p></li></ol><ul><li><strong>T > 1</strong>：使概率分布更平滑，增加低概率词的选中机会，生成更随机、更有创造性的输出。</li><li><strong>T &lt; 1</strong>：使概率分布更尖锐，放大高概率词的优势，生成更确定、基于事实的输出。</li><li><strong>T = 1</strong>：保持原始概率分布不变。</li><li><strong>T → 0</strong>：趋向于贪婪解码，只选择概率最高的词。</li></ul><hr><h4 id=不同-temperature-的效果><strong>不同 Temperature 的效果</strong></h4><p>为了简化，直接用原始概率分布（假设它们来自 T=1 的 softmax 输出），并通过 temperature 调整后的概率分布来说明效果。以下是不同 temperature 值对概率分布的影响：</p><h5 id=1-temperature--1默认><strong>1. Temperature = 1（默认）</strong></h5><ul><li>概率分布保持不变：<ul><li>猫: 0.4</li><li>狗: 0.3</li><li>鸟: 0.15</li><li>鱼: 0.1</li><li>马: 0.05</li></ul></li><li>如果采样（而不是贪婪解码），模型会根据这些概率随机选择一个词。例如，40% 概率选 &ldquo;猫&rdquo;，30% 概率选 &ldquo;狗&rdquo;。输出有一定随机性，但仍偏向高概率词。</li></ul><h5 id=2-temperature--05低温度><strong>2. Temperature = 0.5（低温度）</strong></h5><p>降低 temperature 使概率分布更尖锐，高概率词的概率进一步增加，低概率词的概率被压缩。假设调整后概率分布变为：</p><ul><li><p>猫: 0.65（概率显著增加）</p></li><li><p>狗: 0.25</p></li><li><p>鸟: 0.07</p></li><li><p>鱼: 0.02</p></li><li><p>马: 0.01</p></li><li><p><strong>效果</strong>：模型更倾向于选择 &ldquo;猫&rdquo; 或 &ldquo;狗&rdquo;，尤其是 &ldquo;猫&rdquo;（概率从 0.4 升到 0.65）。输出更确定、更可预测，接近贪婪解码（如果 T 趋向 0，则几乎只选 &ldquo;猫&rdquo;）。</p></li><li><p><strong>应用场景</strong>：适合需要高准确性、基于事实的任务，如问答系统。</p></li></ul><h5 id=3-temperature--2高温度><strong>3. Temperature = 2（高温度）</strong></h5><p>提高 temperature 使概率分布更平滑，低概率词的概率增加，高概率词的概率降低。假设调整后概率分布变为：</p><ul><li><p>猫: 0.28</p></li><li><p>狗: 0.24</p></li><li><p>鸟: 0.20</p></li><li><p>鱼: 0.16</p></li><li><p>马: 0.12</p></li><li><p><strong>效果</strong>：所有词的概率更接近，&ldquo;马&rdquo;（原本 0.05）现在有 0.12 的概率被选中。模型生成更随机、更具多样性的输出，可能生成意想不到的词如 &ldquo;马&rdquo;。</p></li><li><p><strong>应用场景</strong>：适合创意写作或头脑风暴，需要多样性和新奇的输出。</p></li></ul><h5 id=4-temperature--01极低温度><strong>4. Temperature = 0.1（极低温度）</strong></h5><p>当 temperature 非常低时，概率分布极度尖锐，几乎只选择最高概率的词：</p><ul><li><p>猫: 0.99</p></li><li><p>狗: 0.01</p></li><li><p>鸟: ~0</p></li><li><p>鱼: ~0</p></li><li><p>马: ~0</p></li><li><p><strong>效果</strong>：模型几乎总是选择 &ldquo;猫&rdquo;，等同于贪婪解码。输出完全基于最高概率，没有随机性。</p></li><li><p><strong>应用场景</strong>：需要绝对确定性的场景，但可能过于单调。</p></li></ul><h5 id=5-temperature--10极高温度><strong>5. Temperature = 10（极高温度）</strong></h5><p>当 temperature 非常高时（毫无意义），概率分布接近均匀分布：</p><ul><li><p>猫: 0.2</p></li><li><p>狗: 0.2</p></li><li><p>鸟: 0.2</p></li><li><p>鱼: 0.2</p></li><li><p>马: 0.2</p></li><li><p><strong>效果</strong>：每个词被选中的概率几乎相等，模型随机性极高，可能生成非常离谱或不相关的词，如 &ldquo;马&rdquo;。输出几乎完全随机，失去语义连贯性。</p></li><li><p><strong>应用场景</strong>：极少使用，除非需要完全随机的输出。</p></li></ul><hr><ul><li><strong>Temperature 调整概率分布的形状</strong>：决定高概率词和低概率词的相对权重。</li><li><strong>Top-K 或 Top-P 限制采样范围</strong>：决定从哪些词中采样。</li></ul><h5 id=结合例子><strong>结合例子：</strong></h5><p>假设使用 Top-K=3 采样，原始分布为：</p><ul><li>猫: 0.4, 狗: 0.3, 鸟: 0.15, 鱼: 0.1, 马: 0.05</li></ul><ol><li><p><strong>T = 0.5, Top-K=3</strong>：</p><ul><li>调整后分布：猫: 0.65, 狗: 0.25, 鸟: 0.07（鱼和马概率极低）。</li><li>Top-K=3 选择 [&ldquo;猫&rdquo;, &ldquo;狗&rdquo;, &ldquo;鸟&rdquo;]，但由于猫的概率极高，输出几乎总是 &ldquo;猫&rdquo; 或 &ldquo;狗&rdquo;。</li><li>结果：输出非常可预测，接近事实。</li></ul></li><li><p><strong>T = 2, Top-K=3</strong>：</p><ul><li>调整后分布：猫: 0.28, 狗: 0.24, 鸟: 0.20（鱼和马概率较高，但未进入 Top-K）。</li><li>Top-K=3 选择 [&ldquo;猫&rdquo;, &ldquo;狗&rdquo;, &ldquo;鸟&rdquo;]，概率更均匀，&ldquo;鸟&rdquo; 被选中的机会显著增加。</li><li>结果：输出更具多样性和创造性。</li></ul></li><li><p><strong>T = 1, Top-P=0.85</strong>：</p><ul><li>原始分布：猫 (0.4) + 狗 (0.3) + 鸟 (0.15) = 0.85。</li><li>Top-P=0.85 选择 [&ldquo;猫&rdquo;, &ldquo;狗&rdquo;, &ldquo;鸟&rdquo;]，按原始概率采样。</li><li>结果：适度的随机性和多样性。</li></ul></li><li><p><strong>T = 2, Top-P=0.85</strong>：</p><ul><li>调整后分布更平滑：猫: 0.28, 狗: 0.24, 鸟: 0.20, 鱼: 0.16（累积到 0.88，接近 0.85）。</li><li>Top-P=0.85 可能只选 [&ldquo;猫&rdquo;, &ldquo;狗&rdquo;, &ldquo;鸟&rdquo;]，但 &ldquo;鸟&rdquo; 的机会更高。</li><li>结果：比 T=1 时更随机，生成更具创意。</li></ul></li></ol><hr><h4 id=实际应用-1><strong>实际应用</strong></h4><ul><li><strong>低 Temperature（如 0.5）</strong>：适合需要高准确性和可预测性的任务，如技术文档生成、问答系统。</li><li><strong>高 Temperature（如 1.2）</strong>：适合需要创意和多样性的任务，如故事生成、诗歌创作。</li><li><strong>结合 Top-K/Top-P</strong>：Temperature 控制概率分布的平滑度，Top-K/Top-P 限制采样范围，二者配合可以精细调整生成效果。</li></ul><hr><h4 id=总结-1><strong>总结</strong></h4><ul><li><strong>Temperature</strong> 控制概率分布的尖锐或平滑程度：<ul><li><strong>T &lt; 1</strong>：分布更尖锐，偏向高概率词，输出更确定（接近贪婪解码）。</li><li><strong>T > 1</strong>：分布更平滑，增加低概率词的机会，输出更随机和多样。</li><li><strong>T = 1</strong>：保持原始分布。</li></ul></li><li>在之前的例子中，T=0.5 使 &ldquo;猫&rdquo; 主导，T=2 让 &ldquo;马&rdquo; 也有机会被选中。</li><li>与 Top-K 和 Top-P 结合使用时，Temperature 决定概率分布的特性，Top-K/Top-P 决定采样范围，共同塑造生成文本的风格。</li></ul><hr><h4 id=解释一个上文提到的重要概念><strong>解释一个上文提到的重要概念</strong></h4><p><strong>Logits</strong> 是语言模型在预测下一个词时，输出的原始、未归一化的分数（或“得分”）。它们通常是由模型的最后一层（例如全连接层）计算得来的，表示每个可能词的“倾向性”或“可能性”。Logits 本身没有特定的范围（可以是任意实数），需要通过归一化（如 softmax）转换为概率。</p><h4 id=背景><strong>背景</strong></h4><p>假设语言模型预测下一个词，词汇表包含 5 个词：[&ldquo;猫&rdquo;, &ldquo;狗&rdquo;, &ldquo;鸟&rdquo;, &ldquo;鱼&rdquo;, &ldquo;马&rdquo;]。模型的最后一层为每个词输出一个 logit 分数：</p><ul><li>猫: 2.0</li><li>狗: 1.5</li><li>鸟: 0.5</li><li>鱼: 0.0</li><li>马: -1.0</li></ul><p>这些 logits 是模型对每个词的“偏好”评分，但它们不是概率（例如，2.0 和 -1.0 无法直接解释为概率）。为了将 logits 转换为概率，我们需要使用 <strong>softmax</strong> 函数。</p><hr><h4 id=什么是-softmax><strong>什么是 Softmax？</strong></h4><p><strong>Softmax</strong> 是一种归一化函数，将一组任意实数（logits）转换为概率分布，使得：</p><ol><li>每个概率值在 [0, 1] 范围内。</li><li>所有概率之和等于 1。</li></ol><p>Softmax 的公式为：</p><p>$
P(x_i) = \frac{\exp(z_i)}{\sum_{j=1}^N \exp(z_j)}
$</p><p>其中：</p><ul><li>$z_i$ 是第 $i$ 个词的 logit。</li><li>$ \exp(z_i) $ 是 $ z_i $ 的指数函数（$ e^{z_i} $）。</li><li>分母是所有词的指数和，用于归一化。</li><li>$ P(x_i) $ 是第 $ i $ 个词的概率。</li></ul><p>Softmax 的作用是将 logits 转换为可解释的概率，同时保留 logits 之间的相对大小关系。</p><hr><h4 id=从-logits-到-softmax><strong>从 Logits 到 Softmax</strong></h4><h5 id=输入-logits><strong>输入 Logits</strong></h5><ul><li>猫: 2.0</li><li>狗: 1.5</li><li>鸟: 0.5</li><li>鱼: 0.0</li><li>马: -1.0</li></ul><h5 id=步骤-1计算指数expz_><strong>步骤 1：计算指数$exp(z_i)$</strong></h5><p>Softmax 使用指数函数 $ \exp(x) $（即 $ e^x $），其中 $ e \approx 2.71828 $ 来处理 logits，确保输出为正数：</p><ul><li>猫: $ \exp(2.0) \approx 7.389 $</li><li>狗: $ \exp(1.5) \approx 4.482 $</li><li>鸟: $ \exp(0.5) \approx 1.649 $</li><li>鱼: $ \exp(0.0) = 1.000 $</li><li>马: $ \exp(-1.0) \approx 0.368 $</li></ul><p>这些值反映了 logits 的相对大小，指数函数放大了高 logits 的值，压缩了低 logits 的值。</p><h5 id=步骤-2计算指数和><strong>步骤 2：计算指数和</strong></h5><p>将所有指数值相加，得到分母：</p><p>$
7.389 + 4.482 + 1.649 + 1.000 + 0.368 \approx 14.888
$</p><h5 id=步骤-3计算概率softmax><strong>步骤 3：计算概率（softmax）</strong></h5><p>对每个词的指数值除以总和，得到概率：</p><ul><li>猫: $ \frac{7.389}{14.888} \approx 0.496 $ （49.6%）</li><li>狗: $ \frac{4.482}{14.888} \approx 0.301 $ （30.1%）</li><li>鸟: $ \frac{1.649}{14.888} \approx 0.111 $ （11.1%）</li><li>鱼: $ \frac{1.000}{14.888} \approx 0.067 $ （6.7%）</li><li>马: $ \frac{0.368}{14.888} \approx 0.025 $ （2.5%）</li></ul><h4 id=输出概率分布><strong>输出概率分布</strong></h4><p>最终的概率分布为：</p><ul><li>猫: 0.496</li><li>狗: 0.301</li><li>鸟: 0.111</li><li>鱼: 0.067</li><li>马: 0.025</li></ul><p>检查：所有概率之和为 $ 0.496 + 0.301 + 0.111 + 0.067 + 0.025 = 1.000 $，满足概率分布的要求。</p><hr><h4 id=softmax-的特点><strong>Softmax 的特点</strong></h4><ol><li><strong>放大差异</strong>：由于指数函数的非线性，较高的 logit（如 2.0）会得到远高于低 logit（如 -1.0）的概率（0.496 vs. 0.025）。</li><li><strong>归一化</strong>：确保概率和为 1，适合用于分类任务（如预测下一个词）。</li><li><strong>保留相对顺序</strong>：logit 的大小顺序在 softmax 后仍然保持（猫 > 狗 > 鸟 > 鱼 > 马）。</li></ol><hr><h4 id=验证-temperature-->**验证 Temperature **</h4><p>temperature 参数会调整 logits。假设我们用 temperature: $ T = 0.5 $：</p><ol><li><p>调整 logits：$ z_i / T = z_i / 0.5 = 2 \cdot z_i $。</p></li><li><p>新 logits：</p><ul><li>猫: $ 2.0 \cdot 2 = 4.0 $</li><li>狗: $ 1.5 \cdot 2 = 3.0 $</li><li>鸟: $ 0.5 \cdot 2 = 1.0 $</li><li>鱼: $ 0.0 \cdot 2 = 0.0 $</li><li>马: $ -1.0 \cdot 2 = -2.0 $</li></ul></li><li><p>计算指数：</p><ul><li>猫: $ \exp(4.0) \approx 54.598 $</li><li>狗: $ \exp(3.0) \approx 20.085 $</li><li>鸟: $ \exp(1.0) \approx 2.718 $</li><li>鱼: $ \exp(0.0) = 1.000 $</li><li>马: $ \exp(-2.0) \approx 0.135 $</li></ul></li><li><p>指数和：$ 54.598 + 20.085 + 2.718 + 1.000 + 0.135 \approx 78.536 $</p></li><li><p>概率：</p><ul><li>猫: $ \frac{54.598}{78.536} \approx 0.695 $ （69.5%）</li><li>狗: $ \frac{20.085}{78.536} \approx 0.256 $ （25.6%）</li><li>鸟: $ \frac{2.718}{78.536} \approx 0.035 $ （3.5%）</li><li>鱼: $ \frac{1.000}{78.536} \approx 0.013 $ （1.3%）</li><li>马: $ \frac{0.135}{78.536} \approx 0.002 $ （0.2%）</li></ul></li></ol><p><strong>效果</strong>：T=0.5 使分布更尖锐，高概率词（如 &ldquo;猫&rdquo;）的概率进一步增加，低概率词（如 &ldquo;马&rdquo;）的概率接近 0，输出更确定。</p><hr><h4 id=结合-top-k-和-top-p><strong>结合 Top-K 和 Top-P</strong></h4><ul><li><strong>Top-K=3</strong>（T=1）：选择 [&ldquo;猫&rdquo;, &ldquo;狗&rdquo;, &ldquo;鸟&rdquo;]，概率为 [0.496, 0.301, 0.111]，从中随机采样。</li><li><strong>Top-P=0.85</strong>（T=1）：累积概率 0.496 + 0.301 + 0.111 = 0.908 > 0.85，选择 [&ldquo;猫&rdquo;, &ldquo;狗&rdquo;, &ldquo;鸟&rdquo;]。</li><li>如果用 T=0.5，Top-P=0.85 可能只选 [&ldquo;猫&rdquo;, &ldquo;狗&rdquo;]（0.695 + 0.256 = 0.951 > 0.85），因为分布更集中。</li></ul><hr><h4 id=实际应用-2><strong>实际应用</strong></h4><ul><li><strong>Logits</strong>：模型的原始输出，表示对每个词的“倾向”。高 logit（如 2.0）表示更可能，低 logit（如 -1.0）表示不太可能。</li><li><strong>Softmax</strong>：将 logits 转换为概率，用于采样或选择下一个词。结合 temperature、Top-K 或 Top-P，可以控制输出的随机性和多样性。</li><li><strong>场景</strong>：<ul><li><strong>问答</strong>：低 temperature（如 0.5）+ 小 Top-K（如 2），基于高 logit 词生成准确答案。</li><li><strong>创意写作</strong>：高 temperature（如 2）+ 大 Top-P（如 0.9），利用更平滑的分布生成多样化文本。</li></ul></li></ul><hr><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=https://mlcore-engine.github.io/transformer/ title=ML基础><i class="fas fa-arrow-left" aria-hidden=true></i>&nbsp;Prev - ML基础</a>
<a class="nav nav-next" href=https://mlcore-engine.github.io/transformer/entropy/ title=Entropy>Next - Entropy <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlcore-engine.github.io/>about me</a></li><li><a href=https://mlcore-engine.github.io/learn_cs/>cs基础</a></li><li class=has-sub-menu><a href=https://mlcore-engine.github.io/algorithm/>算法题<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlcore-engine.github.io/algorithm/strings/>Strings</a></li><li><a href=https://mlcore-engine.github.io/algorithm/dynamic-programmnig/>动态规划问题</a></li></ul></li><li class=has-sub-menu><a href=https://mlcore-engine.github.io/math_foundation/>ML中的数学<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlcore-engine.github.io/math_foundation/information/>信息量</a></li><li><a href=https://mlcore-engine.github.io/math_foundation/likelihood_entropy/>似然函数_交叉熵</a></li><li><a href=https://mlcore-engine.github.io/math_foundation/kl_dpo/>Kl散度与dpo算法</a></li></ul></li><li class="parent has-sub-menu"><a href=https://mlcore-engine.github.io/transformer/>ML基础<span class="mark opened">-</span></a><ul class=sub-menu><li class=active><a href=https://mlcore-engine.github.io/transformer/do_sample_para/>Do_sample_para</a></li><li><a href=https://mlcore-engine.github.io/transformer/entropy/>Entropy</a></li></ul></li><li class=has-sub-menu><a href=https://mlcore-engine.github.io/kubernetes/>kubernetes<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlcore-engine.github.io/kubernetes/installation/>Installation</a></li></ul></li><li><a href=https://mlcore-engine.github.io/learn_english/>英语学习</a></li><li class=has-sub-menu><a href=https://mlcore-engine.github.io/golang/>golang<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlcore-engine.github.io/golang/foundation/>Foundation</a></li></ul></li><li class=has-sub-menu><a href=https://mlcore-engine.github.io/linux_foundation/>Linux基础<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlcore-engine.github.io/linux_foundation/linux-commands/>50个常用Linux命令</a></li><li><a href=https://mlcore-engine.github.io/linux_foundation/linux-common/>Linux Common</a></li></ul></li><li class=has-sub-menu><a href=https://mlcore-engine.github.io/exercise/>workout<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlcore-engine.github.io/exercise/workout/>Workout</a></li></ul></li><li class=has-sub-menu><a href=https://mlcore-engine.github.io/others/>others<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlcore-engine.github.io/others/create-hugo-gitpage/>使用 Hugo 和 GitHub Pages 创建个人网站</a></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>