<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ML基础 on 高新 | AI平台开发工程师</title>
    <link>http://localhost:1313/transformer/</link>
    <description>Recent content in ML基础 on 高新 | AI平台开发工程师</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 31 May 2025 22:50:26 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/transformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>模型量化基础</title>
      <link>http://localhost:1313/transformer/quantization2/</link>
      <pubDate>Sat, 31 May 2025 22:48:15 +0800</pubDate>
      <guid>http://localhost:1313/transformer/quantization2/</guid>
      <description>&lt;h2 id=&#34;模型量化概述&#34;&gt;模型量化概述&lt;/h2&gt;&#xA;&lt;p&gt;在深度学习中，模型量化（model quantization）是指将原本使用高精度浮点数（如 &lt;code&gt;float32&lt;/code&gt;）存储和计算的模型参数与激活值，通过某种映射转换成低精度整数（如 &lt;code&gt;int8&lt;/code&gt;、&lt;code&gt;uint8&lt;/code&gt; 或 &lt;code&gt;int16&lt;/code&gt;）的过程。量化的主要动机包括：&lt;/p&gt;</description>
    </item>
    <item>
      <title>模型量化基础代码版</title>
      <link>http://localhost:1313/transformer/quantization/</link>
      <pubDate>Tue, 27 May 2025 15:58:39 +0800</pubDate>
      <guid>http://localhost:1313/transformer/quantization/</guid>
      <description>&lt;h1 id=&#34;深度学习模型量化完整教程&#34;&gt;深度学习模型量化完整教程&lt;/h1&gt;&#xA;&lt;h2 id=&#34;目录&#34;&gt;目录&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#1-%E9%87%8F%E5%8C%96%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA&#34;&gt;量化基础理论&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#2-%E9%87%8F%E5%8C%96%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86&#34;&gt;量化的数学原理&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#3-%E9%87%8F%E5%8C%96%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3&#34;&gt;量化方法详解&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#4-pytorch%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98&#34;&gt;PyTorch量化实战&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#6-%E9%87%8F%E5%8C%96%E6%84%9F%E7%9F%A5%E8%AE%AD%E7%BB%83qat&#34;&gt;量化感知训练(QAT)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#7-%E9%AB%98%E7%BA%A7%E9%87%8F%E5%8C%96%E6%8A%80%E6%9C%AF&#34;&gt;高级量化技术&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#8-%E9%87%8F%E5%8C%96%E8%B0%83%E8%AF%95%E4%B8%8E%E4%BC%98%E5%8C%96&#34;&gt;量化调试与优化&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#9-%E5%AE%9E%E9%99%85%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90&#34;&gt;实际案例分析&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#10-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88&#34;&gt;常见问题与解决方案&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;1-量化基础理论&#34;&gt;1. 量化基础理论&lt;/h2&gt;&#xA;&lt;h3 id=&#34;11-什么是模型量化&#34;&gt;1.1 什么是模型量化？&lt;/h3&gt;&#xA;&lt;p&gt;模型量化是将神经网络中的浮点数参数和计算转换为低精度表示的技术。这个过程类似于音频的数字化采样：将连续的模拟信号转换为离散的数字信号。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Do_sample_para</title>
      <link>http://localhost:1313/transformer/do_sample_para/</link>
      <pubDate>Wed, 16 Apr 2025 17:11:57 +0800</pubDate>
      <guid>http://localhost:1313/transformer/do_sample_para/</guid>
      <description>&lt;h5 id=&#34;在大语言模型微调和采样过程中有几个重要的采样参数需要懂得下面我将讲解三个特别重要的采样参数这3个参数的设置会直接影响模型的表现顺便再解释一下经常看到的一个重要概念logits&#34;&gt;在大语言模型微调和采样过程中有几个重要的采样参数需要懂得，下面我将讲解三个特别重要的采样参数，这3个参数的设置会直接影响模型的表现。顺便再解释一下经常看到的一个重要概念logits。&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;top_k&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;top_p&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;temperature&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h4 id=&#34;背景语言模型的概率分布&#34;&gt;&lt;strong&gt;背景：语言模型的概率分布&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;假设语言模型在预测下一个词时，面对一个包含 5 个词的词汇表：[&amp;ldquo;猫&amp;rdquo;, &amp;ldquo;狗&amp;rdquo;, &amp;ldquo;鸟&amp;rdquo;, &amp;ldquo;鱼&amp;rdquo;, &amp;ldquo;马&amp;rdquo;]。模型会为每个词分配一个概率，例如：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Nn Begin</title>
      <link>http://localhost:1313/transformer/nn_begin/</link>
      <pubDate>Tue, 15 Apr 2025 14:31:46 +0800</pubDate>
      <guid>http://localhost:1313/transformer/nn_begin/</guid>
      <description>&lt;p&gt;记不住NN的一些基本概念， 下面用一个例子让你能够快速的理解这些概念并且记住。主要解释输入层、隐藏层（传输层/全连接层）、输出层和池化层等概念，并展示计算过程，当你记不起一些概念的时候回想这个例子，你都能轻松回忆起神经网络的工作原理。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Entropy</title>
      <link>http://localhost:1313/transformer/entropy/</link>
      <pubDate>Mon, 07 Apr 2025 10:57:56 +0800</pubDate>
      <guid>http://localhost:1313/transformer/entropy/</guid>
      <description>&lt;h3 id=&#34;对于entropy的理解&#34;&gt;对于Entropy的理解&lt;/h3&gt;&#xA;&lt;p&gt;$H(X) = -\sum_{i=1}^{n} p(x_i) \log_2 p(x_i)$&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h4 id=&#34;1-公式的直观解释&#34;&gt;1. &lt;strong&gt;公式的直观解释&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;先来看看公式里每个部分的含义：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;$H(X)$&lt;/strong&gt;：表示随机变量 $X$ 的熵，也就是不确定性的度量。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;$p(x_i)$&lt;/strong&gt;：第 $i$ 个可能结果的概率，介于 0 到 1 之间。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;$\log_2 p(x_i)$&lt;/strong&gt;：对概率取以 2 为底的对数。因为 $p(x_i)$ 是小于 1 的数，所以 $\log_2 p(x_i)$ 是负值。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;负号和求和&lt;/strong&gt;：对所有可能结果的 $p(x_i) \log_2 p(x_i)$ 求和，然后取负号，使熵变成正值。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;为什么用负号？&lt;/strong&gt;&lt;br&gt;&#xA;因为 $p(x_i)$ 小于 1 时，$\log_2 p(x_i)$ 是负数，$p(x_i) \log_2 p(x_i)$ 也是负数。加一个负号后，熵 $H(X)$ 变成正数，直观地反映不确定性的大小。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
