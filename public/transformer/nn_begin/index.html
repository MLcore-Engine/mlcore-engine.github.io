<!DOCTYPE html>
<html lang="zh-cn">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<title>Nn Begin - 高新 | AI平台开发工程师</title>
<meta name="description" content="AI平台开发工程师，专注于AI平台工程和Kubernetes云原生技术。拥有AI平台开发、GPU资源优化和AI服务部署经验">
<meta name="generator" content="Hugo 0.145.0">
<link href="http://localhost:1313//index.xml" rel="alternate" type="application/rss+xml">
<link rel="canonical" href="http://localhost:1313/transformer/nn_begin/">
<link rel="stylesheet" href="http://localhost:1313/css/theme.min.css">
<link rel="stylesheet" href="http://localhost:1313/css/chroma.min.css">
<script defer src="http://localhost:1313//js/fontawesome6/all.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js" integrity="sha256-H3cjtrm/ztDeuhCN9I4yh4iN2Ybx/y1RM7rMmAesA0k=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js" integrity="sha256-4XodgW4TwIJuDtf+v6vDJ39FVxI0veC/kSCCmnFp7ck=" crossorigin="anonymous"></script>
<script src="http://localhost:1313/js/bundle.js"></script><style>
 
@media screen and (min-width: 480px) {
  .sidebar {
    flex: 0 0 20% !important;
    max-width: 20% !important;
  }
  
  main {
    flex: 0 0 80% !important;
    max-width: 80% !important;
  }
}

 
body {
  background-color: #f8f5e6 !important;  
  font-family: 'KaiTi', 'STKaiti', '楷体', '楷体_GB2312', 'SimKai', '华文楷体', Kai, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif !important;  
  font-size: 20px !important;  
  line-height: 1.8 !important;  
}

 
.container, .content-container, main {
  background-color: #f8f5e6 !important;
}

 
.sidebar {
  background-color: inherit;
  font-size: 16px !important;  
}

 
h1, h2, h3, h4, h5, h6 {
  font-family: 'KaiTi', 'STKaiti', '楷体', '楷体_GB2312', 'SimKai', '华文楷体', Kai, 'Noto Serif', Georgia, serif !important;
  font-weight: 600 !important;
  line-height: 1.5 !important;
}

 
h1 {
  font-size: 2.4em !important;
}

h2 {
  font-size: 2em !important;
}

h3 {
  font-size: 1.7em !important;
}

h4 {
  font-size: 1.5em !important;
}

h5 {
  font-size: 1.3em !important;
}

h6 {
  font-size: 1.2em !important;
}

 
p {
  font-size: 20px !important;
  margin-bottom: 1.2em !important;
}

 
li {
  font-size: 20px !important;
  margin-bottom: 0.5em !important;
}

 
article, .content, .post-content, main p, main li, main td, main th, blockquote, .markdown {
  font-size: 20px !important;
}

 
pre, code {
  font-family: 'JetBrains Mono', Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace !important;
  font-size: 1.1em !important;  
}

 
a {
  color: #0066cc !important;
  text-decoration: none !important;
}

a:hover {
  text-decoration: underline !important;
}

 
table {
  font-size: 20px !important;
}
</style> <meta property="og:url" content="http://localhost:1313/transformer/nn_begin/">
  <meta property="og:site_name" content="高新 | AI平台开发工程师">
  <meta property="og:title" content="Nn Begin">
  <meta property="og:description" content="记不住NN的一些基本概念， 下面用一个例子让你能够快速的理解这些概念并且记住。主要解释输入层、隐藏层（传输层/全连接层）、输出层和池化层等概念，并展示计算过程，当你记不起一些概念的时候回想这个例子，你都能轻松回忆起神经网络的工作原理。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="transformer">
    <meta property="article:published_time" content="2025-04-15T14:31:46+08:00">
    <meta property="article:modified_time" content="2025-04-17T13:08:49+08:00">
    <meta property="og:image" content="http://localhost:1313/home/me.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/home/me.png">
  <meta name="twitter:title" content="Nn Begin">
  <meta name="twitter:description" content="记不住NN的一些基本概念， 下面用一个例子让你能够快速的理解这些概念并且记住。主要解释输入层、隐藏层（传输层/全连接层）、输出层和池化层等概念，并展示计算过程，当你记不起一些概念的时候回想这个例子，你都能轻松回忆起神经网络的工作原理。">

  <meta itemprop="name" content="Nn Begin">
  <meta itemprop="description" content="记不住NN的一些基本概念， 下面用一个例子让你能够快速的理解这些概念并且记住。主要解释输入层、隐藏层（传输层/全连接层）、输出层和池化层等概念，并展示计算过程，当你记不起一些概念的时候回想这个例子，你都能轻松回忆起神经网络的工作原理。">
  <meta itemprop="datePublished" content="2025-04-15T14:31:46+08:00">
  <meta itemprop="dateModified" content="2025-04-17T13:08:49+08:00">
  <meta itemprop="wordCount" content="1770">
  <meta itemprop="image" content="http://localhost:1313/home/me.png">
<link rel="apple-touch-icon" sizes="180x180" href="/favicon/favicon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
<link rel="manifest" href="/favicon/site.webmanifest">
<link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="/favicon.ico">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/favicon/browserconfig.xml">
<meta name="theme-color" content="#ffffff"> 


<meta name="description" content="记不住NN的一些基本概念， 下面用一个例子让你能够快速的理解这些概念并且记住。主要解释输入层、隐藏层（传输层/全连接层）、输出层和池化层等概念，并展示计算过程，当你记不起一些概念的时候回想这个例子，你都能轻松回忆起神经网络的工作原理。
">
<meta name="keywords" content="AI, 机器学习, golang, kubernetes, 技术博客">
<meta name="author" content="高新">


<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/transformer/nn_begin/">
<meta property="og:title" content="Nn Begin | 高新 | AI平台开发工程师">
<meta property="og:description" content="记不住NN的一些基本概念， 下面用一个例子让你能够快速的理解这些概念并且记住。主要解释输入层、隐藏层（传输层/全连接层）、输出层和池化层等概念，并展示计算过程，当你记不起一些概念的时候回想这个例子，你都能轻松回忆起神经网络的工作原理。
">
<meta property="og:image" content="http://localhost:1313/home/me.png">


<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:url" content="http://localhost:1313/transformer/nn_begin/">
<meta name="twitter:title" content="Nn Begin | 高新 | AI平台开发工程师">
<meta name="twitter:description" content="记不住NN的一些基本概念， 下面用一个例子让你能够快速的理解这些概念并且记住。主要解释输入层、隐藏层（传输层/全连接层）、输出层和池化层等概念，并展示计算过程，当你记不起一些概念的时候回想这个例子，你都能轻松回忆起神经网络的工作原理。
">
<meta name="twitter:image" content="http://localhost:1313/home/me.png">


<link rel="canonical" href="http://localhost:1313/transformer/nn_begin/">


<link rel="stylesheet" href="/css/math.css">


<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script id="MathJax-script" async src="/js/mathjax/tex-svg.js"></script> </head>
<body>

<div class="container"><header>
<h1>高新 | AI平台开发工程师</h1><a href="https://github.com/mlcore-engine/mlcore-engine" class="github"><i class="fab fa-github"></i></a>
<p class="description">AI平台开发工程师，专注于AI平台工程和Kubernetes云原生技术。拥有AI平台开发、GPU资源优化和AI服务部署经验</p>

</header>


<div class="content-container">
<main><h1>Nn Begin</h1>
<p>记不住NN的一些基本概念， 下面用一个例子让你能够快速的理解这些概念并且记住。主要解释输入层、隐藏层（传输层/全连接层）、输出层和池化层等概念，并展示计算过程，当你记不起一些概念的时候回想这个例子，你都能轻松回忆起神经网络的工作原理。</p>
<hr>
<h4 id="1-输入层">1. 输入层</h4>
<ul>
<li><strong>作用</strong>：接收外部输入数据。</li>
<li><strong>特点</strong>：输入层的神经元数量通常等于输入数据的特征数。每个神经元代表一个输入特征。</li>
</ul>
<h4 id="2-隐藏层传输层全连接层">2. 隐藏层（传输层/全连接层）</h4>
<ul>
<li><strong>作用</strong>：对输入数据进行处理和特征提取。</li>
<li><strong>特点</strong>：隐藏层可以有多层，每层包含多个神经元。每个神经元与前一层的所有神经元相连（全连接），通过权重计算和激活函数引入非线性。</li>
</ul>
<h4 id="3-输出层">3. 输出层</h4>
<ul>
<li><strong>作用</strong>：生成最终的预测结果。</li>
<li><strong>特点</strong>：输出层的神经元数量取决于任务类型。例如，回归任务通常有1个输出神经元，分类任务的输出神经元数量等于类别数。</li>
</ul>
<h4 id="4-池化层">4. 池化层</h4>
<ul>
<li><strong>作用</strong>：通常用于卷积神经网络（CNN）中，通过降维和特征提取减少计算量并防止过拟合。</li>
<li><strong>特点</strong>：池化层对输入特征图进行局部操作，例如取最大值（最大池化）或平均值（平均池化），从而减小尺寸。</li>
</ul>
<hr>
<h3 id="简单的例子判断点是否在单位圆内">简单的例子：判断点是否在单位圆内</h3>
<p>一个简单的二分类问题来演示神经网络的计算过程：判断一个二维坐标点 (x, y) 是否在单位圆内（即 x^2 + y^2 &lt;= 1）。</p>
<h4 id="问题描述">问题描述</h4>
<ul>
<li><strong>输入</strong>：二维坐标 (x, y)。</li>
<li><strong>输出</strong>：
<ul>
<li>如果 x^2 + y^2 &lt;= 1（点在单位圆内），输出 1。</li>
<li>否则，输出 0。</li>
</ul>
</li>
</ul>
<h4 id="神经网络结构">神经网络结构</h4>
<ul>
<li><strong>输入层</strong>：2 个神经元，分别接收 x 和 y。</li>
<li><strong>隐藏层</strong>：1 个隐藏层，包含 3 个神经元，使用 ReLU 激活函数（ReLU 将负值变为 0，正值保持不变）。</li>
<li><strong>输出层</strong>：1 个神经元，使用 sigmoid 激活函数（将输出压缩到 0 到 1 之间）。</li>
<li><strong>注</strong>：这个例子不包含池化层，但稍后单独演示池化层的计算。</li>
</ul>
<hr>
<h3 id="计算过程前向传播">计算过程：前向传播</h3>
<p>假设输入点是 (0.5, 0.5)，我们将一步步计算神经网络的输出。</p>
<h4 id="步骤-1输入层">步骤 1：输入层</h4>
<ul>
<li>输入数据：x = [0.5, 0.5]</li>
<li>输入层的 2 个神经元分别接收 x = 0.5 和 y = 0.5。</li>
</ul>
<h4 id="步骤-2隐藏层">步骤 2：隐藏层</h4>
<p>隐藏层有 3 个神经元，每个神经元与输入层的 2 个神经元全连接。我们需要用权重和偏置计算输出。</p>
<h4 id="权重和偏置">权重和偏置</h4>
<p>假设隐藏层的权重矩阵 W1 和偏置向量 b1 如下：</p>
<pre tabindex="0"><code>W1 =
|  1 |  1 |
|  1 | -1 |
| -1 |  1 |

b1 =
| 0 |
| 0 |
| 0 |
</code></pre><ul>
<li>W1 是一个 3x2 矩阵，表示 3 个隐藏神经元与 2 个输入神经元的连接权重。</li>
<li>b1 是偏置，这里设为 0 以简化计算。</li>
</ul>
<h4 id="线性组合">线性组合</h4>
<p>计算隐藏层的输入：</p>
<pre tabindex="0"><code>z1 = W1 * x + b1
</code></pre><p>代入数值：</p>
<pre tabindex="0"><code>w1 =
|  1 |  1 |
|  1 | -1 |
| -1 |  1 |

x =
| 0.5 |
| 0.5 |

b1 =
| 0 |
| 0 |
| 0 |
</code></pre><p>计算矩阵乘法：</p>
<ul>
<li>第 1 个神经元：1 * 0.5 + 1 * 0.5 = 0.5 + 0.5 = 1</li>
<li>第 2 个神经元：1 * 0.5 + (-1) * 0.5 = 0.5 - 0.5 = 0</li>
<li>第 3 个神经元：(-1) * 0.5 + 1 * 0.5 = -0.5 + 0.5 = 0</li>
</ul>
<p>结果：</p>
<pre tabindex="0"><code>z1 =
| 1 |
| 0 |
| 0 |
</code></pre><h4 id="激活函数">激活函数</h4>
<p>对 z1 应用 ReLU 激活函数（ReLU(x) = max(x, 0)）：</p>
<pre tabindex="0"><code>a1 = ReLU(z1) =
| max(1, 0) |
| max(0, 0) |
| max(0, 0) |
         =
| 1 |
| 0 |
| 0 |
</code></pre><p>隐藏层的输出是 a1 = [1, 0, 0]。</p>
<h3 id="步骤-3输出层">步骤 3：输出层</h3>
<p>输出层有 1 个神经元，与隐藏层的 3 个神经元全连接。</p>
<h4 id="权重和偏置-1">权重和偏置</h4>
<p>假设输出层的权重矩阵 W2 和偏置 b2 如下：</p>
<pre tabindex="0"><code>W2 = [1, 1, 1]
b2 = 0
</code></pre><ul>
<li>W2 是一个 1x3 矩阵，表示输出神经元与 3 个隐藏神经元的连接权重。</li>
</ul>
<h4 id="线性组合-1">线性组合</h4>
<p>计算输出层的输入：</p>
<pre tabindex="0"><code>z2 = W2 * a1 + b2
</code></pre><p>代入数值：</p>
<pre tabindex="0"><code>z2 = [1, 1, 1] * [1, 0, 0] + 0 = 1 * 1 + 1 * 0 + 1 * 0 = 1
</code></pre><h4 id="激活函数-1">激活函数</h4>
<p>对 z2 应用 sigmoid 激活函数：</p>
<pre tabindex="0"><code>a2 = 1 / (1 + exp(-z2))
</code></pre><p>代入 z2 = 1：</p>
<pre tabindex="0"><code>a2 = 1 / (1 + exp(-1)) ≈ 1 / (1 + 0.368) ≈ 0.731
</code></pre><h4 id="结果解释">结果解释</h4>
<ul>
<li>输出 a2 ≈ 0.731。通常，二分类任务中，如果输出大于 0.5，则预测为 1，否则为 0。</li>
<li>这里 0.731 &gt; 0.5，所以预测该点在单位圆内。</li>
<li>验证：0.5^2 + 0.5^2 = 0.25 + 0.25 = 0.5 &lt;= 1，确实在单位圆内，预测正确。</li>
</ul>
<hr>
<h3 id="池化层的演示">池化层的演示</h3>
<p>虽然上面的例子没有使用池化层，但池化层在卷积神经网络中非常重要。这里通过一个独立的小例子说明它的作用。</p>
<h4 id="示例特征图">示例特征图</h4>
<p>假设有一个 4x4 的特征图：</p>
<pre tabindex="0"><code>[
  1,  3,  2,  4
  5,  6,  7,  8
  9, 10, 11, 12
 13, 14, 15, 16
]
</code></pre><h4 id="最大池化">最大池化</h4>
<p>应用 2x2 的最大池化，步长为 2：</p>
<ul>
<li>
<p>将特征图分成 4 个 2x2 的区域，取每个区域的最大值：</p>
<ul>
<li>左上：max(1, 3, 5, 6) = 6</li>
<li>右上：max(2, 4, 7, 8) = 8</li>
<li>左下：max(9, 10, 13, 14) = 14</li>
<li>右下：max(11, 12, 15, 16) = 16</li>
</ul>
</li>
<li>
<p>池化结果：</p>
</li>
</ul>
<pre tabindex="0"><code>[
  6,  8
 14, 16
]
</code></pre><h4 id="池化层的作用">池化层的作用</h4>
<ul>
<li>将 4x4 的特征图缩小为 2x2，减少了计算量。</li>
<li>保留了主要特征（最大值），有助于提取重要信息并防止过拟合。</li>
</ul>
<hr>
<h3 id="总结">总结</h3>
<p>通过这个例子，我们展示了神经网络的核心概念：</p>
<ul>
<li><strong>输入层</strong>：接收数据（例如 (0.5, 0.5)）。</li>
<li><strong>隐藏层（全连接层）</strong>：通过权重和激活函数处理数据（例如从 [0.5, 0.5] 到 [1, 0, 0]）。</li>
<li><strong>输出层</strong>：生成预测结果（例如 0.731 表示在单位圆内）。</li>
<li><strong>池化层</strong>：降维并提取特征（例如 4x4 特征图变为 2x2）。</li>
</ul>
<div class="edit-meta"> <br></div><nav class="pagination"><a class="nav nav-prev" href="http://localhost:1313/transformer/do_sample_para/" title="Do_sample_para"><i class="fas fa-arrow-left" aria-hidden="true"></i>&nbsp;Prev - Do_sample_para</a>
<a class="nav nav-next" href="http://localhost:1313/transformer/entropy/" title="Entropy">Next - Entropy <i class="fas fa-arrow-right" aria-hidden="true"></i></a>
</nav><footer> </footer>
</main>
<div class="sidebar">

<nav class="slide-menu">
<ul>
<li class=""><a href="http://localhost:1313/">about me</a></li>

<li class=" has-sub-menu"><a href="http://localhost:1313/learn_cs/">cs基础<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/learn_cs/git-crash-course/">GitCrashCourse</a></li>
<li class=""><a href="http://localhost:1313/learn_cs/react-electron/ts-foundation/">ts-foundation</a></li>
<li class=""><a href="http://localhost:1313/learn_cs/react-electron/ipc/">ipc基础</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="http://localhost:1313/algorithm/">算法题<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/algorithm/double_point/">双指针</a></li>
<li class=""><a href="http://localhost:1313/algorithm/strings/">string类题目</a></li>
<li class=""><a href="http://localhost:1313/algorithm/dynamic-programmnig/">动态规划问题</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="http://localhost:1313/math_foundation/">ML中的数学<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/math_foundation/information/">信息量</a></li>
<li class=""><a href="http://localhost:1313/math_foundation/likelihood_entropy/">似然函数_交叉熵</a></li>
<li class=""><a href="http://localhost:1313/math_foundation/kl_dpo/">Kl散度与dpo算法</a></li>
</ul>
  
</li>

<li class="parent has-sub-menu"><a href="http://localhost:1313/transformer/">ML基础<span class="mark opened">-</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/transformer/do_sample_para/">Do_sample_para</a></li>
<li class="active"><a href="http://localhost:1313/transformer/nn_begin/">Nn Begin</a></li>
<li class=""><a href="http://localhost:1313/transformer/entropy/">Entropy</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="http://localhost:1313/kubernetes/">kubernetes<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/kubernetes/installation/">Installation</a></li>
</ul>
  
</li>

<li class=""><a href="http://localhost:1313/learn_english/">英语学习</a>
  
</li>

<li class=" has-sub-menu"><a href="http://localhost:1313/golang/">golang<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/golang/matrix-golang/">matrix-golang</a></li>
<li class=""><a href="http://localhost:1313/golang/sort/">sort包用法</a></li>
<li class=""><a href="http://localhost:1313/golang/foundation/">Go 语言基础知识</a></li>
<li class=""><a href="http://localhost:1313/golang/base/">Base</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="http://localhost:1313/linux_foundation/">Linux基础<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/linux_foundation/linux-commands/">50个常用Linux命令</a></li>
<li class=""><a href="http://localhost:1313/linux_foundation/cs_foundation/">计算机基础知识</a></li>
<li class=""><a href="http://localhost:1313/linux_foundation/linux-common/">Linux Common</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="http://localhost:1313/exercise/">workout<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/exercise/workout/">Workout</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="http://localhost:1313/others/">others<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/others/create-hugo-gitpage/">使用 Hugo 和 GitHub Pages 创建个人网站</a></li>
</ul>
  
</li>
</ul>
</nav>

 
<div class="sidebar-footer"></div>
</div>

</div><a href="#" id="backtothetop-fixed" class="backtothetop"
 data-backtothetop-duration="600"
 data-backtothetop-easing="easeOutQuart"
 data-backtothetop-fixed-fadeIn="1000"
 data-backtothetop-fixed-fadeOut="1000"
 data-backtothetop-fixed-bottom="10"
 data-backtothetop-fixed-right="20">
<span class="fa-layers fa-fw">
<i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i>
</span></a>
</div>
</body>
</html>
