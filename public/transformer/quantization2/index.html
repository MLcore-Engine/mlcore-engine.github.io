<!DOCTYPE html>
<html lang="zh-cn">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<title>模型量化基础 - 高新 | AI平台开发工程师</title>
<meta name="description" content="AI平台开发工程师，专注于AI平台工程和Kubernetes云原生技术。拥有AI平台开发、GPU资源优化和AI服务部署经验">
<meta name="generator" content="Hugo 0.145.0">
<link href="http://localhost:1313//index.xml" rel="alternate" type="application/rss+xml">
<link rel="canonical" href="http://localhost:1313/transformer/quantization2/">
<link rel="stylesheet" href="http://localhost:1313/css/theme.min.css">
<link rel="stylesheet" href="http://localhost:1313/css/chroma.min.css">
<script defer src="http://localhost:1313//js/fontawesome6/all.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js" integrity="sha256-H3cjtrm/ztDeuhCN9I4yh4iN2Ybx/y1RM7rMmAesA0k=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js" integrity="sha256-4XodgW4TwIJuDtf+v6vDJ39FVxI0veC/kSCCmnFp7ck=" crossorigin="anonymous"></script>
<script src="http://localhost:1313/js/bundle.js"></script><style>
 
@media screen and (min-width: 480px) {
  .sidebar {
    flex: 0 0 20% !important;
    max-width: 20% !important;
  }
  
  main {
    flex: 0 0 80% !important;
    max-width: 80% !important;
  }
}

 
body {
  background-color: #f8f5e6 !important;  
  font-family: 'KaiTi', 'STKaiti', '楷体', '楷体_GB2312', 'SimKai', '华文楷体', Kai, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif !important;  
  font-size: 20px !important;  
  line-height: 1.8 !important;  
}

 
.container, .content-container, main {
  background-color: #f8f5e6 !important;
}

 
.sidebar {
  background-color: inherit;
  font-size: 16px !important;  
}

 
h1, h2, h3, h4, h5, h6 {
  font-family: 'KaiTi', 'STKaiti', '楷体', '楷体_GB2312', 'SimKai', '华文楷体', Kai, 'Noto Serif', Georgia, serif !important;
  font-weight: 600 !important;
  line-height: 1.5 !important;
}

 
h1 {
  font-size: 2.4em !important;
}

h2 {
  font-size: 2em !important;
}

h3 {
  font-size: 1.7em !important;
}

h4 {
  font-size: 1.5em !important;
}

h5 {
  font-size: 1.3em !important;
}

h6 {
  font-size: 1.2em !important;
}

 
p {
  font-size: 20px !important;
  margin-bottom: 1.2em !important;
}

 
li {
  font-size: 20px !important;
  margin-bottom: 0.5em !important;
}

 
article, .content, .post-content, main p, main li, main td, main th, blockquote, .markdown {
  font-size: 20px !important;
}

 
pre, code {
  font-family: 'JetBrains Mono', Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace !important;
  font-size: 1.1em !important;  
}

 
a {
  color: #0066cc !important;
  text-decoration: none !important;
}

a:hover {
  text-decoration: underline !important;
}

 
table {
  font-size: 20px !important;
}
</style> <meta property="og:url" content="http://localhost:1313/transformer/quantization2/">
  <meta property="og:site_name" content="高新 | AI平台开发工程师">
  <meta property="og:title" content="模型量化基础">
  <meta property="og:description" content="模型量化概述 在深度学习中，模型量化（model quantization）是指将原本使用高精度浮点数（如 float32）存储和计算的模型参数与激活值，通过某种映射转换成低精度整数（如 int8、uint8 或 int16）的过程。量化的主要动机包括：">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="transformer">
    <meta property="article:published_time" content="2025-05-31T22:48:15+08:00">
    <meta property="article:modified_time" content="2025-05-31T22:48:15+08:00">
    <meta property="og:image" content="http://localhost:1313/home/me.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/home/me.png">
  <meta name="twitter:title" content="模型量化基础">
  <meta name="twitter:description" content="模型量化概述 在深度学习中，模型量化（model quantization）是指将原本使用高精度浮点数（如 float32）存储和计算的模型参数与激活值，通过某种映射转换成低精度整数（如 int8、uint8 或 int16）的过程。量化的主要动机包括：">

  <meta itemprop="name" content="模型量化基础">
  <meta itemprop="description" content="模型量化概述 在深度学习中，模型量化（model quantization）是指将原本使用高精度浮点数（如 float32）存储和计算的模型参数与激活值，通过某种映射转换成低精度整数（如 int8、uint8 或 int16）的过程。量化的主要动机包括：">
  <meta itemprop="datePublished" content="2025-05-31T22:48:15+08:00">
  <meta itemprop="dateModified" content="2025-05-31T22:48:15+08:00">
  <meta itemprop="wordCount" content="14588">
  <meta itemprop="image" content="http://localhost:1313/home/me.png">
<link rel="apple-touch-icon" sizes="180x180" href="/favicon/favicon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
<link rel="manifest" href="/favicon/site.webmanifest">
<link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="/favicon.ico">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/favicon/browserconfig.xml">
<meta name="theme-color" content="#ffffff"> 


<meta name="description" content="模型量化概述 在深度学习中，模型量化（model quantization）是指将原本使用高精度浮点数（如 float32）存储和计算的模型参数与激活值，通过某种映射转换成低精度整数（如 int8、uint8 或 int16）的过程。量化的主要动机包括：
">
<meta name="keywords" content="AI, 机器学习, golang, kubernetes, 技术博客">
<meta name="author" content="高新">


<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/transformer/quantization2/">
<meta property="og:title" content="模型量化基础 | 高新 | AI平台开发工程师">
<meta property="og:description" content="模型量化概述 在深度学习中，模型量化（model quantization）是指将原本使用高精度浮点数（如 float32）存储和计算的模型参数与激活值，通过某种映射转换成低精度整数（如 int8、uint8 或 int16）的过程。量化的主要动机包括：
">
<meta property="og:image" content="http://localhost:1313/home/me.png">


<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:url" content="http://localhost:1313/transformer/quantization2/">
<meta name="twitter:title" content="模型量化基础 | 高新 | AI平台开发工程师">
<meta name="twitter:description" content="模型量化概述 在深度学习中，模型量化（model quantization）是指将原本使用高精度浮点数（如 float32）存储和计算的模型参数与激活值，通过某种映射转换成低精度整数（如 int8、uint8 或 int16）的过程。量化的主要动机包括：
">
<meta name="twitter:image" content="http://localhost:1313/home/me.png">


<link rel="canonical" href="http://localhost:1313/transformer/quantization2/">


<link rel="stylesheet" href="/css/math.css">


<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script id="MathJax-script" async src="/js/mathjax/tex-svg.js"></script> </head>
<body>

<div class="container"><header>
<h1>高新 | AI平台开发工程师</h1><a href="https://github.com/mlcore-engine/mlcore-engine" class="github"><i class="fab fa-github"></i></a>
<p class="description">AI平台开发工程师，专注于AI平台工程和Kubernetes云原生技术。拥有AI平台开发、GPU资源优化和AI服务部署经验</p>

</header>


<div class="content-container">
<main><h1>模型量化基础</h1>
<h2 id="模型量化概述">模型量化概述</h2>
<p>在深度学习中，模型量化（model quantization）是指将原本使用高精度浮点数（如 <code>float32</code>）存储和计算的模型参数与激活值，通过某种映射转换成低精度整数（如 <code>int8</code>、<code>uint8</code> 或 <code>int16</code>）的过程。量化的主要动机包括：</p>
<ul>
<li><strong>减少模型存储空间</strong>：<code>float32</code> 占用 4 字节，而 <code>int8</code> 只占用 1 字节，模型体积最多可以缩小 4 倍。</li>
<li><strong>加速推理速度</strong>：整数运算相较于浮点运算更节省硬件资源，特别是在专用加速器（如 DSP、NPU）上，<code>int8</code> 乘加速度通常比 <code>float32</code> 快 2–4 倍。</li>
<li><strong>降低功耗</strong>：整数算力消耗一般低于浮点算力，适合在移动端、嵌入式设备等对功耗敏感的场景。</li>
</ul>
<p>然而，将模型从浮点映射到整数会引入<strong>量化误差</strong>，如果误差较大，会导致模型精度下降。为了在“节省资源”与“保持精度”之间取得平衡，就产生了多种量化方法与实践技巧。下面我们将从基础概念、量化流程、常见策略，到具体的数值示例，一步步详细讲解。</p>
<hr>
<h2 id="一量化基础概念">一、量化基础概念</h2>
<h3 id="11-浮点与整数表示区别">1.1 浮点与整数表示区别</h3>
<ul>
<li>
<p><strong>浮点类型 (<code>float32</code>)</strong></p>
<ul>
<li>格式：1 位符号、8 位指数、23 位尾数</li>
<li>范围：≈ $–3.4e38, +3.4e38$；能表示非常小或非常大的数，且可表示小数。</li>
<li>精度：约 7 位十进制有效数字。</li>
</ul>
</li>
<li>
<p><strong>整数类型 (<code>int8</code> / <code>uint8</code> / <code>int16</code> 等）</strong></p>
<ul>
<li><code>int8</code>：8 位二进制补码，范围 $-128, +127$；只能表示整数，没有小数。</li>
<li><code>uint8</code>：8 位无符号整数，范围 $0, 255$。</li>
<li><code>int16</code>：16 位补码，范围 $-32768, +32767$。</li>
</ul>
</li>
</ul>
<p>量化的核心在于：<strong>如何将连续、精细的浮点数值范围，映射到离散的、较小范围的整数区间，并尽量减少映射（舍入）误差</strong>。</p>
<hr>
<h3 id="12-量化的核心scale-与-zero-point">1.2 量化的核心：Scale 与 Zero-Point</h3>
<p>最常用的<strong>线性（affine）量化</strong>定义如下：</p>
<ul>
<li>
<p>假设浮点数值空间为 $[x_{\min},,x_{\max}]$。</p>
</li>
<li>
<p>整数目标空间为 $[q_{\min},,q_{\max}]$，如 <code>int8</code> 为 $[-128,,127]<code>，</code>uint8` 为 ([0,,255]$。</p>
</li>
<li>
<p>量化时用到的两个参数：</p>
<ol>
<li>
<p><strong>Scale（缩放因子）</strong>：表示“对应每个整数步进，相当于浮点数空间的多少距离”。</p>
<p>$$
\text{scale} ;=; \frac{x_{\max} - x_{\min}}{q_{\max} - q_{\min}}.
$$</p>
</li>
<li>
<p><strong>Zero-Point（零点偏移）</strong>：为了让“浮点 0.0”在整数空间有一个对应的整数值。</p>
<p>$$
\text{zero_point} ;=; \mathrm{round}\Bigl(q_{\min} ;-; \frac{x_{\min}}{\text{scale}}\Bigr),
$$</p>
<p>然后再裁剪使其在 $[q_{\min},,q_{\max}]$ 内。</p>
</li>
</ol>
</li>
</ul>
<p>有了这两个参数后，<strong>量化（Quantize）</strong> 与 <strong>反量化（Dequantize）</strong> 的公式为：</p>
<blockquote>
<ul>
<li>
<p>量化：</p>
<p>$$
q = \mathrm{clamp}\Bigl(,\mathrm{round}\bigl(\tfrac{x}{\text{scale}}\bigr) ;+; \text{zero_point},; q_{\min},,q_{\max}\Bigr).
$$</p>
<p>其中 <code>clamp(a, low, high)</code> 表示将 <code>a</code> 限制在 <code>[low, high]</code> 范围内。</p>
</li>
<li>
<p>反量化：</p>
<p>$$
\hat{x} ;=; \text{scale} ;\times; \bigl(q ;-; \text{zero_point}\bigr).
$$</p>
<p>其中 $\hat{x}$ 是量化后再还原回浮点的近似值。</p>
</li>
</ul></blockquote>
<p><strong>注意：</strong></p>
<ol>
<li>如果量化对象只包含非负数（如 ReLU 激活后的输出），可以选用 <code>uint8</code> $[0,,255]$ 作为整数区间，这时 <code>zero_point</code> 通常是非负的；</li>
<li>如果要对称量化（Symmetric Quantization），也可强制设 <code>zero_point = 0</code>，这时只需关心浮点范围 $[-x_{\max}, +x_{\max}]$ 对应到整数 $[-127, +127]\ 或\ [-128, +128]$（取决于具体策略）；但对称量化会丢弃掉浮点域的下界偏移信息，量化误差可能更大。</li>
</ol>
<hr>
<h2 id="二量化类型与流程">二、量化类型与流程</h2>
<p>根据“<strong>何时量化</strong>”与“<strong>如何量化</strong>”的不同，常见的量化方法可分为：</p>
<ol>
<li><strong>后训练量化 (Post-Training Quantization, PTQ)</strong></li>
<li><strong>量化感知训练 (Quantization-Aware Training, QAT)</strong></li>
</ol>
<h3 id="21-后训练量化-ptq">2.1 后训练量化 (PTQ)</h3>
<h4 id="211-定义与流程">2.1.1 定义与流程</h4>
<p>PTQ 指的是：在模型训练完成（浮点精度）后，对已有的浮点模型进行一次或多次推断（使用代表性数据集），统计各层权重与激活的最小/最大值，然后直接计算 <code>scale</code> 与 <code>zero_point</code>，将浮点参数与激活值转换为整数形式，最后导出量化模型进行推理。
PTQ 优势在于：无需重新训练，流程简单且耗时低，适用于模型更新不频繁的场景；缺点是量化误差较大，尤其对于对宽分布激活层（如 ReLU 非对称分布）会出现精度下降。</p>
<h4 id="212-ptq-详细步骤">2.1.2 PTQ 详细步骤</h4>
<p>以一个简单的前馈网络为例，包含两层线性（Linear）+ReLU。</p>
<ol>
<li>
<p><strong>收集浮点模型 &amp; 准备代表性数据集</strong></p>
<ul>
<li>
<p>用训练集或验证集的一部分数据做推断采样，统计各层激活值的最小值与最大值。</p>
</li>
<li>
<p>统计结果示例（假设一批样本推断后的统计）：</p>
<ul>
<li><strong>第一层 Linear 权重</strong>：$-0.8, +0.6$</li>
<li><strong>第一层 激活输出 (ReLU 后)</strong>：$0, 5.2$</li>
<li><strong>第二层 Linear 权重</strong>：$-0.3, +0.4$</li>
<li><strong>第二层 激活输出 (ReLU 后)</strong>：$0, 2.8$</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>计算权重量化参数</strong></p>
<ul>
<li>
<p>对于第一层权重范围 $[-0.8, 0.6]$，假设采用对称量化（symmetric）并映射到 <code>int8</code> $[-127, +127]$：</p>
<p>$$
\text{scale}_w^{(1)}
= \frac{\max(|-0.8|, |0.6|)}{127}
= \frac{0.8}{127} \approx 0.0062992,
\quad \text{zero_point}_w^{(1)} = 0.
$$</p>
</li>
<li>
<p>对称量化时，<code>zero_point</code> 固定为 0。</p>
</li>
</ul>
</li>
<li>
<p><strong>计算激活量化参数</strong></p>
<ul>
<li>
<p>第一层激活范围 $[0,,5.2]$，因为是非负且包含 0，可选 <code>uint8</code> $[0,255]$。</p>
<p>$$
\text{scale}_a^{(1)}
= \frac{5.2 - 0}{255 - 0}
= \frac{5.2}{255} \approx 0.0203922,
\quad \text{zero_point}_a^{(1)} = 0.
$$</p>
</li>
<li>
<p>第二层激活范围 $[0,,2.8]$，同理：</p>
<p>$$
\text{scale}_a^{(2)}
= \frac{2.8}{255} \approx 0.0109804,
\quad \text{zero_point}_a^{(2)} = 0.
$$</p>
</li>
</ul>
</li>
<li>
<p><strong>将浮点权重量化为整数</strong></p>
<ul>
<li>
<p>对第一层某个浮点权重 <code>w = 0.5</code>：</p>
<p>$$
q_w^{(1)} = \mathrm{round}\Bigl(\tfrac{0.5}{0.0062992}\Bigr) = \mathrm{round}(79.365) = 79.
$$</p>
</li>
<li>
<p>对第二层某个浮点权重 <code>w = -0.3</code>：</p>
<p>$$
q_w^{(2)} = \mathrm{round}\Bigl(\tfrac{-0.3}{0.0031496}\Bigr) = \mathrm{round}(-95.228) = -95,
\quad (\text{scale}_w^{(2)} = 0.3 / 127 = 0.0023622)
$$</p>
</li>
<li>
<p>量化后，所有权重均替换为 <code>int8</code>。</p>
</li>
</ul>
</li>
<li>
<p><strong>推理时，激活在线量化</strong></p>
<ul>
<li>
<p>输入图片或文本先做前向推断：</p>
<ol>
<li>
<p>第一层输入先量化成整数：</p>
<p>$$
q_x^{(1)} = \mathrm{round}\Bigl(\tfrac{x_{\mathrm{浮点}}}{\text{scale}_a^{(1)}}\Bigr).
$$</p>
</li>
<li>
<p>用 <code>int8</code> 权重与 <code>int8</code> 激活做 8 位乘加，得到一个中间累加值 <code>int32</code>：</p>
<p>$$
s = \sum_k \bigl(q_w^{(1)}[k] \times q_x^{(1)}[k]\bigr)
\quad (\text{int32_accum}).
$$</p>
</li>
<li>
<p>将累加结果 <code>s</code> 乘以权重 scale 和激活 scale，再加上两个 zero_point 的修正（若有）得到浮点近似：</p>
<p>$$
\hat{y}
= \text{scale}_w^{(1)} \times \text{scale}_a^{(1)}
\times \bigl(s - (N \times \text{zero_point}_w^{(1)} \times \text{zero_point}_a^{(1)})\bigr).
$$</p>
</li>
<li>
<p>最后做 ReLU / 非线性，并对激活再次量化，进入下一层。</p>
</li>
</ol>
</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>PTQ 优缺点</strong></p>
<ul>
<li>优点：无需重新训练，工程实施简单；</li>
<li>缺点：量化误差较大，尤其对低比特量化（如 4 位）或对分布敏感层（激活分布宽/偏斜）效果差异明显，可能导致精度大幅下降。</li>
</ul></blockquote>
<hr>
<h3 id="22-量化感知训练-qat">2.2 量化感知训练 (QAT)</h3>
<h4 id="221-定义与思想">2.2.1 定义与思想</h4>
<p>QAT 指在模型训练阶段就“考虑”量化误差，让网络在训练过程中适应低精度表示，从而获得更高的量化后精度。核心思路是：</p>
<ol>
<li>在训练前向传播时，把权重与激活“模拟量化”成整数形式（通常使用伪量化、“fake quantize”），计算量化误差；</li>
<li>反向传播时依然使用浮点梯度更新权重，并通过 Straight-Through Estimator (STE) 来近似传播量化过程的导数；</li>
<li>训练结束后，将最终浮点权重量化为整数，导出精度接近原浮点模型的量化模型。</li>
</ol>
<h4 id="222-qat-详细步骤">2.2.2 QAT 详细步骤</h4>
<ol>
<li>
<p><strong>定义模拟量化算子（FakeQuant）</strong></p>
<ul>
<li>
<p>对参数 <code>w</code>：</p>
<p>$$
w_q = \mathrm{clamp}\Bigl(\mathrm{round}(\tfrac{w}{s_w}) + z_w,; q_{\min},,q_{\max}\Bigr),
\quad w_{\text{fake}} = s_w \times (w_q - z_w).
$$</p>
</li>
<li>
<p>对激活 <code>x</code>：</p>
<p>$$
x_q = \mathrm{clamp}\Bigl(\mathrm{round}(\tfrac{x}{s_a}) + z_a,; q_{\min},,q_{\max}\Bigr),
\quad x_{\text{fake}} = s_a \times (x_q - z_a).
$$</p>
</li>
<li>
<p>在前向传播中，用 <code>w_fake</code>、<code>x_fake</code> 代替原始浮点权重与激活进行运算。这样网络在训练时“看到”的就是带量化误差的权重。</p>
</li>
</ul>
</li>
<li>
<p><strong>反向传播（STE）</strong></p>
<ul>
<li>
<p>模拟量化算子中含有 <code>round(…)</code> 与 <code>clamp(…)</code> 等不可导操作。为了让梯度能够“流过”量化步骤，一般使用 <strong>Straight-Through Estimator (STE)</strong>。例如：</p>
<ul>
<li>在反向传播中，将 <code>∂ L / ∂ w_fake</code> 直接赋给 <code>∂ L / ∂ w</code>，近似认为 <code>d(round(x))/dx ≈ 1</code> 在可行区间内。</li>
</ul>
</li>
<li>
<p>这样网络中的所有参数都在“量化 + 反量化”操作中被训练，模型自适应量化误差。</p>
</li>
</ul>
</li>
<li>
<p><strong>训练流程</strong></p>
<ol>
<li><strong>预训练</strong>：先训练出一个基本精度良好的浮点模型。</li>
<li><strong>插入 FakeQuant 篇节</strong>：将网络中所有要量化的层（如卷积、线性、激活）后面插入 FakeQuant 操作。自动更新 scale 与 zero_point（可训练或使用统计值）。</li>
<li><strong>微调</strong>：在原有学习率基础上，通常使用较小学习率进行数轮微调。让权重与激活“适应”量化误差。</li>
<li><strong>导出量化模型</strong>：将 FakeQuant 中浮点权重真正量化为整数（导出版本），并去掉模拟量化节点，得到可部署的整数模型。</li>
</ol>
</li>
</ol>
<blockquote>
<p><strong>QAT 优缺点</strong></p>
<ul>
<li>优点：通常量化后精度最优，尤其在低比特（如 4 位、甚至 2 位）时能显著提升模型性能。</li>
<li>缺点：需要重新训练或微调，训练开销较大；实现复杂度高，对训练流程需要较大改动。</li>
</ul></blockquote>
<hr>
<h2 id="三量化参数与策略">三、量化参数与策略</h2>
<h3 id="31-对称量化-symmetric-quantization">3.1 对称量化 (Symmetric Quantization)</h3>
<ul>
<li>
<p>设定 <code>zero_point = 0</code>，仅使用一个 <code>scale</code>。</p>
</li>
<li>
<p>通常对浮点范围 $[-x_{\max}, +x_{\max}]$ 对称倒映到整数范围 $[-127, +127]$。</p>
</li>
<li>
<p>计算：</p>
<p>$$
\text{scale} = \frac{x_{\max}}{127},
\quad q = \mathrm{clamp}\bigl(\mathrm{round}(x / \text{scale}), -127, +127\bigr).
$$</p>
</li>
<li>
<p><strong>优点</strong>：实现简单，多数硬件能直接支持无偏移的整数乘加。</p>
</li>
<li>
<p><strong>缺点</strong>：如果浮点范围 $[x_{\min},,x_{\max}]$ 并不对称（多数实际分布偏斜），会浪费部分整数码位或导致更大量化误差。</p>
</li>
</ul>
<h3 id="32-非对称量化-asymmetric-quantization">3.2 非对称量化 (Asymmetric Quantization)</h3>
<ul>
<li>
<p>允许 <code>zero_point ≠ 0</code>，可映射任意浮点区间 $[x_{\min},,x_{\max}]$ 到整数区间 $[q_{\min},,q_{\max}]$。</p>
</li>
<li>
<p>计算：</p>
<p>$$
\begin{aligned}
&amp;\text{scale} = \frac{x_{\max} - x_{\min}}{q_{\max} - q_{\min}}, \
&amp;\text{zero_point}
= \mathrm{round}\Bigl(q_{\min} - \frac{x_{\min}}{\text{scale}}\Bigr),
\quad \text{zero_point} \leftarrow \mathrm{clamp}(\text{zero_point},,q_{\min},,q_{\max}).
\end{aligned}
$$</p>
</li>
<li>
<p><strong>优点</strong>：对浮点范围偏斜（如有大量正值、没有负值）的层量化更灵活，能更好利用整数区间。</p>
</li>
<li>
<p><strong>缺点</strong>：硬件实现可能需要在乘加中加上 <code>zero_point</code> 修正，稍微复杂。</p>
</li>
</ul>
<h3 id="33-逐通道量化-per-channel-quantization">3.3 逐通道量化 (Per-Channel Quantization)</h3>
<ul>
<li>针对卷积或全连接层的 <strong>各个输出通道</strong>（即各行/各列的权重）分别计算 <code>scale</code> 与 <code>zero_point</code>。</li>
<li>举例：卷积核维度为 $(C_{\text{out}},,C_{\text{in}},,k_h,,k_w)$，对每个 <code>C_out</code> 通道分别统计权重最小/最大值，计算单独的 <code>scale_i</code>、<code>zero_point_i</code>。</li>
<li><strong>优点</strong>：能更精细地压缩权重动态范围，减少不同通道间的量化误差。</li>
<li><strong>缺点</strong>：增加存储 <code>scale</code> 与 <code>zero_point</code> 的开销；硬件需要支持分通道乘加时不同缩放因子。</li>
</ul>
<hr>
<h2 id="四一步步示例从前向量化到整数推理">四、一步步示例：从前向量化到整数推理</h2>
<p>下面我们用一个最简单的两层全连接（Fully Connected）网络示例，<strong>手工演示一下量化</strong>。网络结构：</p>
<pre tabindex="0"><code>输入 x（浮点，shape = [1 × 2])
→ 线性层 W1（shape = [2 × 2]）+ b1（shape = [1 × 2）→ ReLU → 量化（激活）
→ 线性层 W2（shape = [2 × 1]）+ b2（shape = [1 × 1）→ 输出 y
</code></pre><p>为了直观，我们只考虑权重量化，并将激活作为浮点保留（为简单起见）。在真实部署时，常常也对激活进行量化；这里仅展示量化权重如何影响计算。</p>
<h3 id="41-设定浮点模型">4.1 设定浮点模型</h3>
<ul>
<li>
<p><strong>第一层权重（W1）</strong>：</p>
<p>$$
W1 =
\begin{bmatrix}
0.50 &amp; -0.80 \
0.20 &amp;  0.10
\end{bmatrix},
\quad b1 = [,0.1,; -0.2,].
$$</p>
</li>
<li>
<p><strong>第二层权重（W2）</strong>：</p>
<p>$$
W2 =
\begin{bmatrix}
0.30 \
-0.75
\end{bmatrix},
\quad b2 = [,0.05,].
$$</p>
</li>
<li>
<p><strong>输入</strong>：</p>
<p>$$
x = [,1.2,; -0.7,].
$$</p>
</li>
</ul>
<h4 id="411-浮点前向计算">4.1.1 浮点前向计算</h4>
<ol>
<li>
<p><strong>第一层线性</strong>：</p>
<p>$$
z_1 = x \cdot W1 + b1
= [,1.2,; -0.7,]
\times
\begin{bmatrix}
0.50 &amp; -0.80 \
0.20 &amp;  0.10
\end{bmatrix}
+ [,0.1,;-0.2,].
$$</p>
<p>计算：</p>
<ul>
<li>第一个输出通道：
$  1.2 \times 0.50 ;+; (-0.7)\times 0.20 ;+; 0.10    = 0.60 ;-;0.14 ;+;0.10 = 0.56.$</li>
<li>第二个输出通道：
$  1.2 \times (-0.80) ;+; (-0.7)\times 0.10 ;-;0.20    = -0.96 ;-;0.07 ;-;0.20 = -1.23.$</li>
</ul>
<p>$$
z_1 = [,0.56,; -1.23,].
$$</p>
</li>
<li>
<p><strong>ReLU 激活</strong>：</p>
<p>$$
a_1 = \max(0,,z_1) = [,0.56,; 0.00,].
$$</p>
</li>
<li>
<p><strong>第二层线性</strong>：</p>
<p>$$
z_2 = a_1 \cdot W2 + b2
= [,0.56,; 0.00,]
\times
\begin{bmatrix} 0.30 \ -0.75 \end{bmatrix}
+ [,0.05,].
$$</p>
<p>计算：</p>
<p>$$
z_2 = 0.56 \times 0.30 + 0.00 \times (-0.75) + 0.05
= 0.168 + 0.00 + 0.05 = 0.218.
$$</p>
</li>
<li>
<p><strong>输出</strong>：</p>
<p>$$
y_{\text{float}} = [,0.218,].
$$</p>
</li>
</ol>
<p>这段浮点推理结果约为 0.218。下面我们尝试对<strong>两层权重同时进行 int8 量化</strong>，然后再用量化权重与浮点激活混合方式近似计算，观察量化误差。</p>
<hr>
<h3 id="42-统计权重范围">4.2 统计权重范围</h3>
<ul>
<li><strong>W1 通道 1 (输出维度 0) 权重</strong>：$0.50, 0.20$，最小 0.20、最大 0.50；</li>
<li><strong>W1 通道 2 (输出维度 1) 权重</strong>：$-0.80, 0.10$，最小 -0.80、最大 0.10；</li>
<li><strong>W2 通道 1 (输出维度 0) 权重</strong>：$0.30, -0.75$，最小 -0.75、最大 0.30。</li>
</ul>
<p>为简化，此处采用<strong>逐通道对称量化</strong>（Symmetric Per-Channel）策略：</p>
<ul>
<li>每个输出通道分别计算 <code>scale</code>，<code>zero_point</code> 固定为 0；</li>
<li>映射到 <code>int8</code> $[-127, +127]$。</li>
</ul>
<h4 id="421-第一层-w1-逐通道量化">4.2.1 第一层 W1 逐通道量化</h4>
<ul>
<li>
<p><strong>W1 第一个输出通道（第一行）权重</strong>：$0.50, 0.20$</p>
<ul>
<li>
<p>最大绝对值：$\max(|0.50|,,|0.20|) = 0.50$。</p>
</li>
<li>
<p>对称量化 scale：</p>
<p>$$
\text{scale}_{W1,0}
= \frac{0.50}{127} \approx 0.0039370.
$$</p>
</li>
<li>
<p><code>zero_point_{W1,0} = 0</code>。</p>
</li>
<li>
<p>对应的量化公式：</p>
<p>$$
q_{W1,0}[i]
= \mathrm{round}\bigl(\tfrac{W1[0,i]}{\text{scale}<em>{W1,0}}\bigr),
\quad i=0,1,
\quad q</em>{W1,0}[i] \in [-127,,127].
$$</p>
</li>
<li>
<p>具体数值：</p>
<ul>
<li>
<p>对 $W1[0,0] = 0.50$：</p>
<p>$$
q = \mathrm{round}\Bigl(\frac{0.50}{0.0039370}\Bigr)
= \mathrm{round}(127.00) = 127.
$$</p>
</li>
<li>
<p>对 $W1[0,1] = 0.20$：</p>
<p>$$
q = \mathrm{round}\Bigl(\frac{0.20}{0.0039370}\Bigr)
= \mathrm{round}(50.80) = 51.
$$</p>
</li>
</ul>
</li>
<li>
<p>因此，<strong>量化后第一层、第一输出通道权重</strong>：$127, 51$。</p>
</li>
</ul>
</li>
<li>
<p><strong>W1 第二个输出通道（第二行）权重</strong>：$-0.80, 0.10$</p>
<ul>
<li>
<p>最大绝对值：$\max(|-0.80|,,|0.10|) = 0.80$。</p>
</li>
<li>
<p>对称量化 scale：</p>
<p>$$
\text{scale}_{W1,1}
= \frac{0.80}{127} \approx 0.0062992.
$$</p>
</li>
<li>
<p><code>zero_point_{W1,1} = 0</code>。</p>
</li>
<li>
<p>数值映射：</p>
<ul>
<li>$W1[1,0] = -0.80$：
$\frac{-0.80}{0.0062992} = -127.04$，四舍五入 → $-127$。</li>
<li>$W1[1,1] = 0.10$：
$\frac{0.10}{0.0062992} = 15.87$，四舍五入 →  16。</li>
</ul>
</li>
<li>
<p>量化后：$-127, 16$。</p>
</li>
</ul>
</li>
</ul>
<h4 id="422-第二层-w2-逐通道量化">4.2.2 第二层 W2 逐通道量化</h4>
<ul>
<li>
<p>仅有一个输出通道，权重 $[,0.30,; -0.75,]$，最大绝对值 $\max(0.30,,0.75) = 0.75$。</p>
</li>
<li>
<p>对称量化 scale：</p>
<p>$$
\text{scale}<em>{W2,0} = \frac{0.75}{127} \approx 0.0059055,
\quad \text{zero_point}</em>{W2,0} = 0.
$$</p>
</li>
<li>
<p>数值映射：</p>
<ul>
<li>$W2[0] = 0.30$：
$;0.30/0.0059055 = 50.81$，四舍五入 → 51。</li>
<li>$W2[1] = -0.75$：
$-0.75/0.0059055 = -127.02$，四舍五入 →  -127。</li>
</ul>
</li>
<li>
<p>量化后：$51,; -127$。</p>
</li>
</ul>
<blockquote>
<p><strong>结果汇总</strong>：</p>
<ul>
<li>
<p>第一层 W1：</p>
<ul>
<li>通道 0 (第一行)：<code>scale = 0.0039370</code>，<code>zero_point = 0</code>，量化权重 $127, 51$。</li>
<li>通道 1 (第二行)：<code>scale = 0.0062992</code>，<code>zero_point = 0</code>，量化权重 $-127, 16$。</li>
</ul>
</li>
<li>
<p>第二层 W2：</p>
<ul>
<li>通道 0：<code>scale = 0.0059055</code>，<code>zero_point = 0</code>，量化权重 $51, -127$。</li>
</ul>
</li>
</ul></blockquote>
<hr>
<h3 id="43-量化后推理近似计算">4.3 量化后推理近似计算</h3>
<p>在量化模型推理时，通常会执行以下步骤（此例中我们保留 ReLU 激活为浮点，以聚焦权重量化部分）：</p>
<ol>
<li>
<p><strong>浮点输入</strong> <code>x = [1.2, -0.7]</code> 不量化（实际完整部署中也会量化激活，这里简化处理）。</p>
</li>
<li>
<p><strong>第一层浮点线性，用量化权重与反量化权重近似</strong>：</p>
<ul>
<li>
<p>我们实际上存储的是整型 <code>q_w</code> 及其对应的 <code>scale</code>，计算时先在“整数”空间进行乘加，然后再乘以 <code>scale</code> 得到近似浮点结果。</p>
</li>
<li>
<p><strong>整型乘加累积</strong>：</p>
<p>$$
s_1[0] = x[0]\times q_{W1,0}[0] + x[1]\times q_{W1,0}[1]
= 1.2 \times 127 + (-0.7)\times 51
= 152.4 - 35.7 = 116.7.
$$</p>
<p>注意：这里我们在权重为 <code>int8</code>、输入仍为浮点的情况下做了“混合”运算，为了演示思路即可；真实加速器会把输入也量化为整数再做纯整数运算。</p>
</li>
<li>
<p><strong>反量化</strong>：</p>
<p>$$
\hat{z}<em>1[0]
= s_1[0] ;\times; \text{scale}</em>{W1,0}
= 116.7 \times 0.0039370
\approx 0.4594.
$$</p>
</li>
<li>
<p>同理第二输出通道：</p>
<p>$$
s_1[1] = 1.2 \times (-127) + (-0.7)\times 16
= -152.4 - 11.2 = -163.6,<br>
\quad \hat{z}_1[1] = (-163.6)\times 0.0062992 \approx -1.030.
$$</p>
</li>
<li>
<p>加上偏置 <code>b1 = [0.1, -0.2]</code>（偏置也可量化或保留浮点），则：</p>
<p>$$
\hat{z}_1 = [,0.4594 + 0.1,; -1.030 - 0.2,]
= [,0.5594,; -1.230,].
$$</p>
</li>
<li>
<p>与浮点原始 <code>z1 = [0.56, -1.23]</code> 对比，量化引入误差很小（第一个分量误差 ≈ 0.0006，第二个基本无误差）。</p>
</li>
<li>
<p><strong>ReLU</strong>：
$\hat{a}_1 = [,0.5594,; 0,].$</p>
</li>
</ul>
</li>
<li>
<p><strong>第二层线性</strong> 同理：</p>
<ul>
<li>
<p>整型乘加：</p>
<p>$$
s_2 = \hat{a}<em>1[0] \times q</em>{W2,0}[0]
+ \hat{a}<em>1[1] \times q</em>{W2,0}[1]
= 0.5594 \times 51 + 0 \times (-127)
= 28.53.
$$</p>
</li>
<li>
<p>反量化：</p>
<p>$$
\hat{z}<em>2 = s_2 \times \text{scale}</em>{W2,0}
= 28.53 \times 0.0059055
\approx 0.1685.
$$</p>
</li>
<li>
<p>加上偏置 <code>b2 = 0.05</code>：</p>
<p>$$
\hat{y} = 0.1685 + 0.05 = 0.2185.
$$</p>
</li>
<li>
<p>与浮点原始 <code>y = 0.218</code> 对比，误差 ≈ 0.0005，几乎等同。</p>
</li>
</ul>
</li>
<li>
<p><strong>小结</strong>：</p>
<ul>
<li>由于我们只是对权重做 <code>int8</code> 量化，且权重与激活分布较窄（在量化范围内），因此量化后推理引入的误差非常小，模型精度基本不受影响。</li>
<li>若要把激活也量化为 <code>int8</code>，同样按照第一层「浮点 → 整数」映射（激活 scale &amp; zero_point），再做纯整数乘加，最后反量化到浮点。整体误差也会很小。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="五逐层逐类型量化要点与实践">五、逐层、逐类型量化要点与实践</h2>
<h3 id="51-权重-vs-激活-的量化顺序">5.1 权重 vs 激活 的量化顺序</h3>
<ul>
<li>
<p><strong>权重量化</strong>：在离线阶段统计并量化，权重是静态的，先计算权重的 <code>scale</code> 与 <code>zero_point</code>，全局或逐通道量化即可。</p>
</li>
<li>
<p><strong>激活量化</strong>：必须在推理时“在线统计”或“离线统计 + 动态补偿”：</p>
<ul>
<li><strong>静态量化 (Static Quantization)</strong>：先通过代表性数据集推断，统计每层激活在整个数据集上的最小/最大值，用它来计算固定的 <code>scale</code> 和 <code>zero_point</code>。推理时直接用该量化参数。</li>
<li><strong>动态量化 (Dynamic Quantization)</strong>：推理时时间窗口内动态统计激活最大/最小值，比如按 Batch 调整，或使用滑动窗口；但一般硬件不易支持动态量化，多用静态量化。</li>
<li><strong>直方图量化 (Histogram-based Quantization)</strong>：对激活分布做直方图统计，然后选择一个裁剪阈值（clipping），舍弃部分最极端值，减小量化误差。</li>
</ul>
</li>
</ul>
<h3 id="52-量化误差来源">5.2 量化误差来源</h3>
<ol>
<li><strong>舍入误差（Rounding Error）</strong>：每个浮点数映射到最近的整数，会有最多 $\tfrac{1}{2}$ 个整数步长的误差，对应浮点误差为 $\tfrac{\text{scale}}{2}$。</li>
<li><strong>裁剪误差（Clamping Error）</strong>：若激活或权重超过统计时选定的 $[x_{\min}, x_{\max}]$，会被裁剪到边界，导致较大误差。</li>
<li><strong>累积误差</strong>：多层网络中，量化误差会在层与层之间累加，尤其深层模型会放大误差。</li>
</ol>
<h3 id="53-精度-vs-压缩率的权衡">5.3 精度 vs 压缩率的权衡</h3>
<ul>
<li>
<p><strong>按位 (Bit-width) 越低</strong>：</p>
<ul>
<li>模型越小、运算越快、功耗越低；</li>
<li>量化误差越大、模型精度损失越明显，尤其当位宽低于 8 位（如 4 位、2 位）时，需要更复杂的量化规则或 QAT 来维持精度。</li>
</ul>
</li>
<li>
<p><strong>对称 vs 非对称</strong>：</p>
<ul>
<li>对称量化实现简单；</li>
<li>非对称量化能更好地覆盖偏斜分布，对激活量化尤为重要。</li>
</ul>
</li>
<li>
<p><strong>逐通道 vs 逐张量</strong>：</p>
<ul>
<li>逐通道量化会大幅降低权重量化误差；</li>
<li>逐张量量化实现简单、存储开销低，但误差较大。</li>
</ul>
</li>
</ul>
<hr>
<h2 id="六更多量化实用技巧">六、更多量化实用技巧</h2>
<h3 id="61-梯度校准-calibration">6.1 梯度校准 (Calibration)</h3>
<ul>
<li><strong>激活校准</strong>：使用几百到几千张代表性样本，通过推断得到各层激活的分布；</li>
<li><strong>选择合适采样数</strong>：样本数太少可能出现极值遗漏；样本数太多则统计开销大。一般 500–2000 张图片即可。</li>
<li><strong>直方图量化</strong>：通过统计激活值的直方图，找到一个 “最佳裁剪阈值” $–T, +T$，舍弃极端长尾，使量化 MSE 最小。</li>
</ul>
<h3 id="62-训练量化参数-learnable-quantization">6.2 训练量化参数 (Learnable Quantization)</h3>
<ul>
<li><strong>Learnable Scale / Zero-Point</strong>：把 <code>scale</code> 或 <code>zero_point</code> 设为可学习参数，在 QAT 过程中通过梯度更新，网络自动调整最优的量化参数。</li>
<li><strong>混合精度量化 (Mixed-Precision Quantization)</strong>：对不同层使用不同量化位宽。如对敏感层（第一层、最后一层或注意力层）使用 16 位，对其他层使用 8 位；或用 AutoML 方法自动搜索各层最优位宽。</li>
</ul>
<h3 id="63-量化感知剪枝-prune--quantize">6.3 量化感知剪枝 (Prune + Quantize)</h3>
<ul>
<li><strong>剪枝 (Pruning)</strong>：先做权重稀疏化剪枝（如 LOG 模式），减少模型参数数量；</li>
<li><strong>再量化 (Quantize After Prune)</strong>：对已经剪枝的稀疏模型做量化，一般能获得更大压缩率，同时保持较好精度。</li>
</ul>
<hr>
<h2 id="七综合示例完整的量化流程pytorch-风格伪代码">七、综合示例：完整的量化流程（PyTorch 风格伪代码）</h2>
<p>以下示例展示如何在 PyTorch 中使用<strong>后训练静态量化</strong>，附带简单注释与关键步骤。示例代码为伪代码，帮助理解流程。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torchvision
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.quantization <span style="color:#66d9ef">as</span> tq
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. 定义原始浮点模型</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SimpleNet</span>(torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(SimpleNet, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">16</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>relu1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>ReLU()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">32</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>relu2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>ReLU()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">32</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">8</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu1(self<span style="color:#f92672">.</span>conv1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu2(self<span style="color:#f92672">.</span>conv2(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2. 加载预训练模型或训练好的浮点模型</span>
</span></span><span style="display:flex;"><span>float_model <span style="color:#f92672">=</span> SimpleNet()
</span></span><span style="display:flex;"><span><span style="color:#75715e"># … 省略训练或加载 checkpoint 的代码 …</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. 指定要量化的层（观察层）</span>
</span></span><span style="display:flex;"><span>float_model<span style="color:#f92672">.</span>qconfig <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>get_default_qconfig(<span style="color:#e6db74">&#39;fbgemm&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 选择适合服务器 CPU 的量化后端，fbgemm 或 qnnpack</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4. 准备模型，插入 FakeQuant 节点（仅在 PyTorch 中做示例）</span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>prepare(float_model, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 5. 用代表性数据做前向推断，收集激活统计信息</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#    这里假设 dataloader 是 DataLoader 包装的代表性数据集</span>
</span></span><span style="display:flex;"><span>float_model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> images, _ <span style="color:#f92672">in</span> dataloader_calibration:
</span></span><span style="display:flex;"><span>        float_model(images)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 只需做几百到一千个 batch 即可完成统计</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 6. 将插桩后的模型转换为量化模型，所有权重与激活均变成整数</span>
</span></span><span style="display:flex;"><span>quantized_model <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>convert(float_model, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 7. 在量化模型上做推理，评估性能</span>
</span></span><span style="display:flex;"><span>quantized_model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 测试集推理，计算 Top-1 / Top-5 精度</span>
</span></span><span style="display:flex;"><span>    acc1, acc5 <span style="color:#f92672">=</span> evaluate(quantized_model, testloader)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Quantized Model Accuracy: Top-1 = </span><span style="color:#e6db74">{</span>acc1<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">%, Top-5 = </span><span style="color:#e6db74">{</span>acc5<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">%&#39;</span>)
</span></span></code></pre></div><blockquote>
<p><strong>说明</strong>：</p>
<ol>
<li><code>prepare()</code> 阶段会在指定层插入量化与反量化算子（FakeQuant）；</li>
<li>用 calibration 数据跑过一遍后，内部会统计每层激活的最小/最大值；</li>
<li><code>convert()</code> 阶段会把重量化、激活量化参数固化，替换为真正的量化算子和整数算子；</li>
<li>最终输出的 <code>quantized_model</code> 便是可部署的整数模型。</li>
</ol></blockquote>
<hr>
<h2 id="八总结与常见问答">八、总结与常见问答</h2>
<h3 id="81-量化能否在不精度损失的情况下完成">8.1 量化能否在不精度损失的情况下完成？</h3>
<ul>
<li>对于某些<strong>对称、分布窄</strong>的模型（如小型 CNN、NLP Transformer 少量层），<code>int8</code> 量化精度损失极小（&lt; 1% Top-1，甚至可以忽略不计）。</li>
<li>当模型层数很深、分布复杂时，量化误差累积会导致精度下降，这时推荐使用 <strong>量化感知训练 (QAT)</strong> 来补偿误差。</li>
</ul>
<h3 id="82-量化位宽可以低于-8-位吗">8.2 量化位宽可以低于 8 位吗？</h3>
<ul>
<li>
<p>4 位、2 位、甚至 1 位（二值化）都可实现，但难度更大：</p>
<ul>
<li>需要更精细的<strong>非线性映射</strong>或<strong>分段量化</strong>；</li>
<li>量化误差非常敏感，通常需要 QAT 才能保证足够精度；</li>
<li>硬件支持较少，市面上很少见能高效做 <code>int4</code> / <code>int2</code> 的推理加速器。</li>
</ul>
</li>
</ul>
<h3 id="83-量化后如何部署与硬件支持">8.3 量化后如何部署与硬件支持？</h3>
<ul>
<li>
<p>主流深度学习框架（PyTorch、TensorFlow Lite、TensorRT、ONNX Runtime 等）均支持 <code>int8</code> 静态/动态量化与 QAT。</p>
</li>
<li>
<p>不同硬件后端（CPU、GPU、NPU、DSP）对量化支持不同：</p>
<ul>
<li><strong>Server CPU (x86)</strong> ：常用 <code>fbgemm</code>（FB GEmM）或 <code>oneDNN</code> 后端，支持 <code>int8</code> 卷积与矩阵乘。</li>
<li><strong>移动端 CPU (ARM)</strong> ：常用 <code>qnnpack</code>，支持动态量化和静态对称量化。</li>
<li><strong>GPU (NVIDIA)</strong> ：TensorRT 支持 <code>int8</code> 精度推理，需要提供 calibration 数据。</li>
<li><strong>NPU / DSP / Edge Device</strong> ：各自有专用 SDK 与算子，需参考厂商文档。</li>
</ul>
</li>
</ul>
<h3 id="84-量化误差量化指标">8.4 量化误差量化指标</h3>
<ul>
<li>
<p><strong>重建误差 (Reconstruction Error)</strong>：测量权重或激活从浮点到整数再到浮点的均方误差  </p>
<p>$$
\mathrm{MSE} = \frac{1}{N}\sum_{i=1}^N (,x_i - \hat{x}_i,)^2.
$$</p>
</li>
<li>
<p><strong>模型整体精度</strong>：量化后对验证集/测试集做推理，计算 Top-1、Top-5 准确率、BLEU、mAP 等指标，评估性能下降程度。</p>
</li>
</ul>
<hr>
<h2 id="九附简单数值演练再看一次完整量化计算">九、附：简单数值演练（再看一次完整量化计算）</h2>
<p>下面再通过一组极简浮点数组（模拟“权重”），一步步做 <strong>非对称逐张量量化</strong>，以加深对前述公式的理解。</p>
<h3 id="91-给定浮点数组">9.1 给定浮点数组</h3>
<pre tabindex="0"><code>w = [ -1.2, 0.0, 0.5, 1.75, -0.3, 2.4 ]
</code></pre><p>浮点值分布：最小 <code>x_min = -1.2</code>，最大 <code>x_max = 2.4</code>。
我们要做非对称量化到 <code>uint8</code>（区间 $0, 255$）。</p>
<h3 id="92-计算-scale-与-zero_point">9.2 计算 scale 与 zero_point</h3>
<ol>
<li>
<p>计算 scale：</p>
<p>$$
\text{scale}
= \frac{x_{\max} - x_{\min}}{q_{\max} - q_{\min}}
= \frac{2.4 - (-1.2)}{255 - 0}
= \frac{3.6}{255} \approx 0.0141176.
$$</p>
</li>
<li>
<p>计算 zero_point：</p>
<p>$$
\text{zero_point}
= \mathrm{round}\Bigl(q_{\min} - \frac{x_{\min}}{\text{scale}}\Bigr)
= \mathrm{round}\Bigl(0 - \frac{-1.2}{0.0141176}\Bigr)
= \mathrm{round}(85.00) = 85.
$$</p>
<ul>
<li>此时 $\text{zero_point} = 85$，表示“浮点 0.0”在量化后映射为整数 85。</li>
<li>再检查 $[0, 255]$ 边界，显然 85 在其中，无需裁剪。</li>
</ul>
</li>
</ol>
<h3 id="93-一一量化">9.3 一一量化</h3>
<p>对每个元素 $w_i$，做：</p>
<p>$$
q_i = \mathrm{clamp}\Bigl(\mathrm{round}\bigl(\tfrac{w_i}{0.0141176}\bigr) + 85,; 0,,255\Bigr).
$$</p>
<ul>
<li>
<p><strong>第一项：</strong> $w[0] = -1.2$：
$\tfrac{-1.2}{0.0141176} = -85.00$，四舍五入 → $-85$，</p>
<p>$$
q_0 = -85 + 85 = 0.<br>
$$</p>
</li>
<li>
<p><strong>第二项：</strong> $w[1] = 0.0$：
$\tfrac{0.0}{0.0141176} = 0$，四舍五入 → 0，</p>
<p>$$
q_1 = 0 + 85 = 85.<br>
$$</p>
</li>
<li>
<p><strong>第三项：</strong> $w[2] = 0.5$：
$\tfrac{0.5}{0.0141176} = 35.44$，四舍五入 → 35，</p>
<p>$$
q_2 = 35 + 85 = 120.<br>
$$</p>
</li>
<li>
<p><strong>第四项：</strong> $w[3] = 1.75$：
$\tfrac{1.75}{0.0141176} = 123.97$，四舍五入 → 124，</p>
<p>$$
q_3 = 124 + 85 = 209.<br>
$$</p>
</li>
<li>
<p><strong>第五项：</strong> $w[4] = -0.3$：
$\tfrac{-0.3}{0.0141176} = -21.24$，四舍五入 → -21，</p>
<p>$$
q_4 = -21 + 85 = 64.<br>
$$</p>
</li>
<li>
<p><strong>第六项：</strong> $w[5] = 2.4$：
$\tfrac{2.4}{0.0141176} = 170.00$，四舍五入 → 170，</p>
<p>$$
q_5 = 170 + 85 = 255 ;(\text{裁剪到最大值 }255).<br>
$$</p>
</li>
</ul>
<p>量化后得到整数数组：</p>
<pre tabindex="0"><code>q = [  0, 85, 120, 209,  64, 255 ]
</code></pre><h3 id="94-反量化">9.4 反量化</h3>
<p>将 <code>q</code> 还原到浮点近似值：</p>
<p>$$
\hat{w}_i = 0.0141176 \times (,q_i - 85,).
$$</p>
<ul>
<li>$\hat{w}_0 = 0.0141176 \times (0 - 85) = -1.2.$</li>
<li>$\hat{w}_1 = 0.0141176 \times (85 - 85) = 0.0.$</li>
<li>$\hat{w}_2 = 0.0141176 \times (120 - 85) = 0.0141176 \times 35 = 0.4941 ≈ 0.5.$</li>
<li>$\hat{w}_3 = 0.0141176 \times (209 - 85) = 0.0141176 \times 124 = 1.749 ≈ 1.75.$</li>
<li>$\hat{w}_4 = 0.0141176 \times (64 - 85) = 0.0141176 \times (-21) = -0.296 ≈ -0.30.$</li>
<li>$\hat{w}_5 = 0.0141176 \times (255 - 85) = 0.0141176 \times 170 = 2.400 = 2.4.$</li>
</ul>
<p>误差：</p>
<ul>
<li>除了 <code>w[2]</code> 和 <code>w[4]</code> 处有约 ±0.0059 左右的小误差，其余几乎完全还原。</li>
</ul>
<hr>
<h2 id="十结语">十、结语</h2>
<p>通过以上内容，你应该对模型量化的<strong>基本原理</strong>、<strong>关键步骤</strong>、<strong>常见策略</strong>与<strong>具体数值示例</strong>有了全面而深入的了解。主要要点回顾如下：</p>
<ol>
<li>
<p><strong>量化核心</strong>：</p>
<ul>
<li>通过 <code>scale</code> 和 <code>zero_point</code>，把浮点数域映射到整数域；</li>
<li>量化会引入舍入与裁剪误差，需要通过策略控制。</li>
</ul>
</li>
<li>
<p><strong>量化类型</strong>：</p>
<ul>
<li>后训练量化 (PTQ)：快速、方便，但误差更大；</li>
<li>量化感知训练 (QAT)：在训练中模拟量化，精度损失最小，但训练成本高。</li>
</ul>
</li>
<li>
<p><strong>策略选用</strong>：</p>
<ul>
<li>对称 vs 非对称：根据分布是否偏斜选择；</li>
<li>逐通道 vs 逐张量：卷积/线性层权重用逐通道可显著降低误差；</li>
<li>位宽选择：<code>int8</code> 是目前最常用的方案，<code>int4</code> 、<code>int2</code> 需更复杂方法。</li>
</ul>
</li>
<li>
<p><strong>实战技巧</strong>：</p>
<ul>
<li>激活校准（静态或动态）；</li>
<li>直方图量化与裁剪阈值；</li>
<li>QAT 中可学习量化参数；</li>
<li>与剪枝、蒸馏等技术结合。</li>
</ul>
</li>
<li>
<p><strong>简单示例演示</strong>：</p>
<ul>
<li>对单层全连接网络的手工量化步骤；</li>
<li>对一维浮点数组做非对称量化到 <code>uint8</code>，并对比量化-反量化结果。</li>
</ul>
</li>
</ol>
<p>在实际项目中，你可以根据模型结构、硬件平台和精度要求，调整量化策略与具体细节。例如：某些模型首层与尾层对精度极度敏感，可以选择保留原始浮点或使用更高位宽；而中间层可以大胆做 <code>int8</code> 量化；对高度偏斜的激活，则需考虑对称 vs 非对称或裁剪策略。</p>
<p>在 INT8 量化过程中，你提到的两种范围：</p>
<ul>
<li><strong>[0, 255]：无符号 INT8（uint8）</strong></li>
<li><strong>[-128, 127]：有符号 INT8（int8）</strong></li>
</ul>
<p>这两种数值表示方式的选择，会直接影响量化公式、推理的数值表示方式和硬件兼容性。</p>
<hr>
<h2 id="-一什么是量化">🔍 一、什么是量化？</h2>
<p><strong>量化（Quantization）</strong> 是指将原始的高精度（通常是 float32）模型参数和中间激活值，用低精度的整数来近似表示（比如 int8）。这样可以：</p>
<ul>
<li>减少模型体积</li>
<li>提高推理速度</li>
<li>降低内存带宽需求</li>
</ul>
<hr>
<h2 id="-二核心公式对称量化--对零点非零的非对称量化">📘 二、核心公式（对称量化 / 对零点非零的非对称量化）</h2>
<p>设原始的浮点数是 $x$，量化后的整数是 $q$，量化使用的比例因子是 $s$，零点是 $z$，则：</p>
<ul>
<li>
<p><strong>非对称量化</strong>（常用于 uint8）：</p>
<p>$$
q = \text{round}\left(\frac{x}{s} + z\right)
$$</p>
<p>$$
x \approx s \cdot (q - z)
$$</p>
</li>
<li>
<p><strong>对称量化</strong>（常用于 int8）：</p>
<p>$$
q = \text{round}\left(\frac{x}{s}\right)
$$</p>
<p>$$
x \approx s \cdot q
$$</p>
</li>
</ul>
<hr>
<h2 id="-三uint8-vs-int8-的区别">🎯 三、uint8 vs int8 的区别</h2>
<table>
  <thead>
      <tr>
          <th>特性</th>
          <th>uint8 ([0, 255])</th>
          <th>int8 ([-128, 127])</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>是否有符号</td>
          <td>否（unsigned）</td>
          <td>是（signed）</td>
      </tr>
      <tr>
          <td>常用场景</td>
          <td>非对称量化（激活）</td>
          <td>对称量化（权重）</td>
      </tr>
      <tr>
          <td>是否包含 0</td>
          <td>包含</td>
          <td>包含</td>
      </tr>
      <tr>
          <td>中心点</td>
          <td>128</td>
          <td>0</td>
      </tr>
      <tr>
          <td>硬件支持</td>
          <td>较广（某些硬件只支持）</td>
          <td>越来越普遍</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="-四具体简单例子int8-vs-uint8">✅ 四、具体简单例子：int8 vs uint8</h2>
<p>设你有一组浮点值范围是：</p>
<pre tabindex="0"><code>x ∈ [-1.0, 1.0]
</code></pre><hr>
<h3 id="1使用-int8-128-127对称量化">1）使用 int8（[-128, 127]）对称量化</h3>
<h4 id="步骤">步骤：</h4>
<ol>
<li><strong>计算 scale（缩放比例）</strong>：</li>
</ol>
<p>$$
s = \frac{\max(|x_{max}|, |x_{min}|)}{127} = \frac{1.0}{127} \approx 0.007874
$$</p>
<ol start="2">
<li><strong>量化公式（无零点）</strong>：</li>
</ol>
<p>$$
q = \text{round}\left(\frac{x}{s}\right) = \text{round}(x \div 0.007874)
$$</p>
<ol start="3">
<li>举例：</li>
</ol>
<table>
  <thead>
      <tr>
          <th>浮点值 $x$</th>
          <th>量化后 $q$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>-1.0</td>
          <td>-127</td>
      </tr>
      <tr>
          <td>-0.5</td>
          <td>-64</td>
      </tr>
      <tr>
          <td>0.0</td>
          <td>0</td>
      </tr>
      <tr>
          <td>0.5</td>
          <td>64</td>
      </tr>
      <tr>
          <td>1.0</td>
          <td>127</td>
      </tr>
  </tbody>
</table>
<ol start="4">
<li><strong>反量化（近似还原）</strong>：</li>
</ol>
<p>$$
x&rsquo; = s \cdot q
$$</p>
<hr>
<h3 id="2使用-uint80-255非对称量化">2）使用 uint8（[0, 255]）非对称量化</h3>
<p>这时我们必须设置一个零点 $z$，因为数据的最小值是 -1.0，不包含 0。</p>
<h4 id="步骤-1">步骤：</h4>
<ol>
<li><strong>计算 scale</strong>：</li>
</ol>
<p>$$
s = \frac{x_{max} - x_{min}}{255} = \frac{1.0 - (-1.0)}{255} = \frac{2.0}{255} \approx 0.007843
$$</p>
<ol start="2">
<li><strong>计算 zero point</strong>：</li>
</ol>
<p>$$
z = \text{round}( -x_{min} / s ) = \text{round}(1.0 / 0.007843) = 128
$$</p>
<ol start="3">
<li><strong>量化公式</strong>：</li>
</ol>
<p>$$
q = \text{round}(x / s + z)
$$</p>
<ol start="4">
<li>举例：</li>
</ol>
<table>
  <thead>
      <tr>
          <th>浮点值 $x$</th>
          <th>$q$ = round(x / s + 128)</th>
          <th>反量化 $x&rsquo; = s \cdot (q - 128)$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>-1.0</td>
          <td>round(-1 / 0.007843 + 128) = 0</td>
          <td>-1.0</td>
      </tr>
      <tr>
          <td>-0.5</td>
          <td>round(-0.5 / 0.007843 + 128) = 64</td>
          <td>-0.5</td>
      </tr>
      <tr>
          <td>0.0</td>
          <td>128</td>
          <td>0.0</td>
      </tr>
      <tr>
          <td>0.5</td>
          <td>192</td>
          <td>0.5</td>
      </tr>
      <tr>
          <td>1.0</td>
          <td>255</td>
          <td>1.0</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="-总结选-uint8-还是-int8">🧠 总结：选 uint8 还是 int8？</h2>
<table>
  <thead>
      <tr>
          <th>类型</th>
          <th>优点</th>
          <th>缺点</th>
          <th>常见使用场景</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>uint8</td>
          <td>兼容性高（如 ARM）、无负数问题</td>
          <td>需要计算零点 z</td>
          <td>激活（ReLU 输出 &gt;= 0）</td>
      </tr>
      <tr>
          <td>int8</td>
          <td>更对称，计算简洁（z = 0）</td>
          <td>需要支持负数计算硬件</td>
          <td>权重或 TFLite 全量化</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="-附加补充计算误差">🧪 附加补充：计算误差</h2>
<p>量化会引入误差，误差由两方面决定：</p>
<ul>
<li><strong>scale 越小，精度越高</strong></li>
<li><strong>数据越偏离 0，误差越大</strong></li>
</ul>
<hr>
<p>如果你有兴趣，我可以继续给出一个 <strong>PyTorch/TensorFlow 的量化代码实例</strong> 或者介绍 <strong>感知量化（QAT）与后训练量化（PTQ）</strong> 的区别。是否继续？</p>
<p>在模型量化中，“<strong>零点偏移（Zero-point offset）</strong>”的作用是为了将 <strong>浮点数（实数）范围映射到整数范围时保留0的对齐性</strong>，确保 <strong>量化后的整数值能够准确地表示0</strong>。</p>
<hr>
<h2 id="-为什么需要零点偏移">🧠 为什么需要零点偏移？</h2>
<p>在量化中，我们将浮点数（比如从模型训练中得到的参数或中间输出）转换为整数（通常是 int8，即 [-128, 127]），以减小模型体积和加快推理速度。</p>
<p>但浮点数的值域通常不是对称的，比如 $0.0, 6.0$，而 int8 是对称的 $-128, 127$。我们需要找到一种方法，把浮点数的值域映射到整数值域，并且让“<strong>0.0 精准对齐某个整数</strong>”，否则会引入偏差。</p>
<p>这时候，**零点（zero point）**就派上用场了。</p>
<hr>
<h2 id="-量化公式">📐 量化公式</h2>
<p>设：</p>
<ul>
<li>$x$ 是浮点数值</li>
<li>$q$ 是量化后的整数值</li>
<li>$scale$ 是缩放因子（浮点数范围与整数范围之间的比例）</li>
<li>$zero_point$ 是零点偏移（整数，用来保证 0 的对齐）</li>
</ul>
<p>则量化和反量化公式为：</p>
<ul>
<li>
<p><strong>量化公式</strong>：</p>
<p>$$
q = \text{round}\left(\frac{x}{scale}\right) + zero_point
$$</p>
</li>
<li>
<p><strong>反量化公式</strong>（把量化值还原为浮点）：</p>
<p>$$
x = scale \cdot (q - zero_point)
$$</p>
</li>
</ul>
<hr>
<h2 id="-举个具体例子">✅ 举个具体例子</h2>
<p>假设我们有一个浮点数范围为 $0.0, 6.0$，我们要把它量化成 uint8（即整数范围为 $0, 255$）。</p>
<h3 id="第一步计算-scale-和-zero_point">第一步：计算 scale 和 zero_point</h3>
<ul>
<li>
<p>设定：</p>
<p>$$
x_{min} = 0.0,\quad x_{max} = 6.0
$$</p>
<p>$$
q_{min} = 0,\quad q_{max} = 255
$$</p>
</li>
<li>
<p>计算 scale：</p>
<p>$$
scale = \frac{x_{max} - x_{min}}{q_{max} - q_{min}} = \frac{6.0 - 0.0}{255 - 0} = \frac{6.0}{255} \approx 0.0235
$$</p>
</li>
<li>
<p>计算 zero_point：</p>
<p>$$
zero_point = \text{round}\left(q_{min} - \frac{x_{min}}{scale}\right) = \text{round}(0 - \frac{0.0}{0.0235}) = 0
$$</p>
</li>
</ul>
<p>说明：0.0 对应的整数值正好是 0。</p>
<hr>
<h3 id="第二步量化示例">第二步：量化示例</h3>
<p>用上面的参数：</p>
<ul>
<li>$scale = 0.0235$</li>
<li>$zero_point = 0$</li>
</ul>
<h4 id="浮点值-30">浮点值 3.0：</h4>
<p>$$
q = \text{round}\left(\frac{3.0}{0.0235}\right) + 0 = \text{round}(127.66) = 128
$$</p>
<h4 id="浮点值-00">浮点值 0.0：</h4>
<p>$$
q = \text{round}\left(\frac{0.0}{0.0235}\right) + 0 = 0
$$</p>
<hr>
<h3 id="第三步反量化回浮点">第三步：反量化回浮点</h3>
<p>现在假设我们有量化值 $q = 128$：</p>
<p>$$
x = 0.0235 \cdot (128 - 0) = 3.008 \approx 3.0
$$</p>
<hr>
<h2 id="-总结零点偏移的作用">💡 总结：零点偏移的作用</h2>
<table>
  <thead>
      <tr>
          <th>功能</th>
          <th>解释</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>对齐零点</td>
          <td>保证 <strong>浮点数的0.0能准确对应一个整数</strong>，防止误差扩大</td>
      </tr>
      <tr>
          <td>支持非对称范围</td>
          <td>浮点值域可能不是以0为中心，zero-point 用来“移位”调整</td>
      </tr>
      <tr>
          <td>减少精度损失</td>
          <td>不使用 zero-point 会让数值分布整体偏离，导致模型精度下降</td>
      </tr>
  </tbody>
</table>
<hr>
<p>如果你想看看 PyTorch 或 TensorFlow 的实际量化 API 示例，我也可以给你代码。是否需要？</p>
<p>下面我们通过一个更<strong>全面、具体</strong>的例子，<strong>逐步</strong>演示“零点偏移（zero-point）”在量化过程中的计算与作用。我们会从如下几个部分来讲解：</p>
<ol>
<li>问题背景回顾</li>
<li>选取浮点范围和整数范围</li>
<li>计算 scale（缩放因子）与 zero_point（零点偏移）</li>
<li>对一组示例浮点数进行量化（quantize）</li>
<li>再把量化值反量化（dequantize）回浮点，检验误差</li>
<li>总结零点偏移在这里的作用</li>
</ol>
<hr>
<h2 id="1-问题背景回顾">1. 问题背景回顾</h2>
<p>在神经网络模型量化时，我们常常要把<strong>浮点数</strong>（<code>float32</code>）的权重、激活值、偏置等转换成<strong>低精度整数</strong>（最常见的是 <code>int8</code>），这样可以大幅度减少模型体积、加速推断运算。在做这种从浮点到整数的映射时，需要解决两个核心问题：</p>
<ol>
<li><strong>浮点数值域通常是非对称的</strong>（例如 $-2.5, 3.5$），而我们要映射到的整数域一般是对称的（<code>int8</code> 是 $-128, +127$）。</li>
<li>我们希望“浮点中的 0.0”要<strong>精准对应到某个整数值</strong>，否则后续多层计算（卷积/矩阵乘）中会引入偏差，影响模型精度。</li>
</ol>
<p>这时，就要用到两个关键量：</p>
<ul>
<li><strong>scale（缩放因子）</strong>：用来把浮点值域“压缩”到整数值域。</li>
<li><strong>zero_point（零点偏移）</strong>：用来保证“浮点 0.0”对齐到某个整数，使得 0 的表示在量化后是精确的。</li>
</ul>
<hr>
<h2 id="2-选取浮点范围和整数范围">2. 选取浮点范围和整数范围</h2>
<h3 id="21-浮点范围-以示例为例">2.1 浮点范围 （以示例为例）</h3>
<p>假设我们有一个张量/权重/激活，其在训练后统计到的最小值和最大值分别是：</p>
<pre tabindex="0"><code>x_min = -2.5
x_max = +3.5
</code></pre><p>也就是说，该张量中的实际浮点值都落在 $-2.5, +3.5$ 之间。</p>
<h3 id="22-整数范围-以-int8-为例">2.2 整数范围 （以 int8 为例）</h3>
<p>我们希望把它量化为<strong>有符号 8 位整数</strong>（int8），此时整数范围是：</p>
<pre tabindex="0"><code>q_min = -128
q_max = +127
</code></pre><p>（注意 <code>int8</code> 的范围之和是 256 个数值，且对称：-128 ~ +127。）</p>
<hr>
<h2 id="3-计算-scale-与-zero_point">3. 计算 scale 与 zero_point</h2>
<h3 id="31-计算-scale">3.1 计算 scale</h3>
<p>首先，<strong>scale</strong> 定义为“浮点值域 的跨度 ÷ 整数值域 的跨度”：</p>
<p>$$
\text{scale} = \frac{x_{\max} - x_{\min}}{q_{\max} - q_{\min}} ;=; \frac{3.5 - (-2.5)}{127 - (-128)}
$$</p>
<ul>
<li>浮点跨度 = $3.5 - (-2.5) = 6.0$</li>
<li>整数跨度 = $127 - (-128) = 255$</li>
</ul>
<p>所以：</p>
<p>$$
\text{scale} = \frac{6.0}{255} \approx 0.0235294118;;(\text{约等于 }0.02353)
$$</p>
<blockquote>
<p><strong>含义</strong>：
每增加 1 个整数单位，对应浮点值增加约 0.02353。</p></blockquote>
<hr>
<h3 id="32-计算-zero_point">3.2 计算 zero_point</h3>
<p>有了 scale，我们还需要找到一个整数 <code>zero_point</code>，使得“浮点中的 0.0”能够尽量精准地对应到某个整数值。计算公式通常为：</p>
<p>$$
\text{zero_point} = \mathrm{round}\Bigl(q_{\min} - \tfrac{x_{\min}}{\text{scale}}\Bigr)
$$</p>
<ul>
<li>
<p>直观含义：先把浮点最小值 $x_{\min}$ 映射到整数范围的偏移，然后再让浮点 0.0 对应去这个偏移基础上的整数。</p>
</li>
<li>
<p>公式拆解：</p>
<ol>
<li>$\tfrac{x_{\min}}{\text{scale}}$ 是把浮点最小值“缩放”后得到的相对整数坐标。</li>
<li>$q_{\min} - \tfrac{x_{\min}}{\text{scale}}$ 表示：如果浮点最小值对应整数是 $q_{\min} = -128$，那么 0.0 会多 “偏移” 多少整数步长。</li>
<li>最后做四舍五入（round），确保 <code>zero_point</code> 为整数。</li>
</ol>
</li>
</ul>
<p><strong>代入我们具体的数值</strong>：</p>
<p>$$
\begin{aligned}
\frac{x_{\min}}{\text{scale}}
&amp;= \frac{-2.5}{0.0235294118}
\approx -106.25 \
q_{\min} - \frac{x_{\min}}{\text{scale}}
&amp;= -128 - (-106.25)
= -128 + 106.25
= -21.75 \
\text{round}(-21.75) &amp;= -22
\end{aligned}
$$</p>
<p>所以得到：</p>
<p>$$
\boxed{\text{zero_point} = -22}
$$</p>
<blockquote>
<p><strong>解释</strong>：</p>
<ul>
<li>当浮点值取到最小值 $-2.5$ 时，对应到整数坐标应该是 $q_{\min} = -128$。</li>
<li>这样，“浮点 0.0”会落在整数上 $-22$（即从 $-128$ 往上偏移 106.25 个整数单位，四舍五入后是 106；$-128 + 106 = -22$）。</li>
<li>换句话说，我们让整数 $-22$ 精确代表浮点 0.0。</li>
</ul></blockquote>
<p><strong>注意</strong>：</p>
<ul>
<li>计算完后，务必要把 <code>zero_point</code> 限制在 $[q_{\min},,q_{\max}]$ 范围内（本例中 $[-128,127]$），但 $-22$ 显然已经在这个区间里，无需裁剪。</li>
</ul>
<hr>
<h3 id="33-得到最终参数">3.3 得到最终参数</h3>
<p>综合上面两步，我们得到：</p>
<ul>
<li>$\text{scale} \approx 0.0235294118$</li>
<li>$\text{zero_point} = -22$</li>
</ul>
<p>后续的量化公式（量化 float → int）：</p>
<p>$$
q = \mathrm{round}\bigl(\tfrac{x}{\text{scale}}\bigr) + \text{zero_point}
$$</p>
<ul>
<li>得到的 $q$ 再裁剪到 $[q_{\min},,q_{\max}]$ = $[-128,,127]$ 区间。</li>
</ul>
<p>而反量化的公式（整数 → 近似浮点）：</p>
<p>$$
\hat{x} ;=; \text{scale} ,\times, \bigl(q - \text{zero_point}\bigr)
$$</p>
<ul>
<li>这里的 $\hat{x}$ 表示解算得到的“近似浮点值”。</li>
</ul>
<hr>
<h2 id="4-对一组示例浮点数进行量化">4. 对一组示例浮点数进行量化</h2>
<p>下面我们挑选一组典型的浮点数，让它们落在 $[x_{\min},,x_{\max}] = [-2.5,;3.5]$ 之间，并用上面的 scale、zero_point 进行量化。假设我们要量化的浮点值序列为：</p>
<pre tabindex="0"><code>[-2.5,  -1.0,   0.0,   1.0,   2.0,   3.5]
</code></pre><p>我们逐个带入量化公式：</p>
<p>$$
q ;=; \mathrm{round}\Bigl(\frac{x}{0.0235294118}\Bigr);+;(-22)
$$</p>
<p>并在最后做一个裁剪到 $[-128,,127]$。</p>
<hr>
<h3 id="41-计算示例-1x---25">4.1 计算示例 1：x = -2.5</h3>
<p>$$
\frac{-2.5}{0.0235294118} ;\approx; -106.250
\quad\Longrightarrow;
\mathrm{round}(-106.250) = -106
$$</p>
<p>$$
q = -106 + (-22) = -128
$$</p>
<ul>
<li>裁剪范围 $[-128,,127]$ 后，依然是 $-128$。</li>
</ul>
<p>所以：</p>
<p>$$
x = -2.5 ;\longmapsto; q = -128
$$</p>
<hr>
<h3 id="42-计算示例-2x---10">4.2 计算示例 2：x = -1.0</h3>
<p>$$
\frac{-1.0}{0.0235294118} ;\approx; -42.500
\quad\Longrightarrow;
\mathrm{round}(-42.500) = -42
$$</p>
<p>$$
q = -42 + (-22) = -64
$$</p>
<ul>
<li>裁剪后依然在 $[-128,,127]$，保持 $-64$。</li>
</ul>
<p>所以：</p>
<p>$$
x = -1.0 ;\longmapsto; q = -64
$$</p>
<hr>
<h3 id="43-计算示例-3x--00">4.3 计算示例 3：x = 0.0</h3>
<p>$$
\frac{0.0}{0.0235294118} = 0.0
\quad\Longrightarrow;
\mathrm{round}(0.0) = 0
$$</p>
<p>$$
q = 0 + (-22) = -22
$$</p>
<ul>
<li>裁剪后仍在区间内，不变。</li>
</ul>
<p>所以：</p>
<p>$$
x = 0.0 ;\longmapsto; q = -22
$$</p>
<blockquote>
<p><strong>注意</strong>：这就说明为什么要有 zero_point = -22，才能让浮点 0.0 对应整数 $-22$。</p></blockquote>
<hr>
<h3 id="44-计算示例-4x--10">4.4 计算示例 4：x = 1.0</h3>
<p>$$
\frac{1.0}{0.0235294118} ;\approx; 42.500
\quad\Longrightarrow;
\mathrm{round}(42.500) = 42
$$</p>
<p>$$
q = 42 + (-22) = 20
$$</p>
<ul>
<li>裁剪后仍在区间 $[-128,,127]$，保持 $;20$。</li>
</ul>
<p>所以：</p>
<p>$$
x = 1.0 ;\longmapsto; q = 20
$$</p>
<hr>
<h3 id="45-计算示例-5x--20">4.5 计算示例 5：x = 2.0</h3>
<p>$$
\frac{2.0}{0.0235294118} ;\approx; 85.000
\quad\Longrightarrow;
\mathrm{round}(85.000) = 85
$$</p>
<p>$$
q = 85 + (-22) = 63
$$</p>
<ul>
<li>裁剪后依然在区间内。</li>
</ul>
<p>所以：</p>
<p>$$
x = 2.0 ;\longmapsto; q = 63
$$</p>
<hr>
<h3 id="46-计算示例-6x--35">4.6 计算示例 6：x = 3.5</h3>
<p>$$
\frac{3.5}{0.0235294118} ;\approx; 148.750
\quad\Longrightarrow;
\mathrm{round}(148.750) = 149
$$</p>
<p>$$
q = 149 + (-22) = 127
$$</p>
<ul>
<li>注意：$;149 - 22 = 127$，已经刚好到达整数上界 127，无需裁剪。</li>
</ul>
<p>所以：</p>
<p>$$
x = 3.5 ;\longmapsto; q = 127
$$</p>
<hr>
<h3 id="47-小结浮点--整数-的映射">4.7 小结：浮点 → 整数 的映射</h3>
<p>把上面所有示例整理成表格，有助于理解：</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">浮点值 $x$</th>
          <th style="text-align: center">$\tfrac{x}{\text{scale}}$ （带小数）</th>
          <th style="text-align: center">四舍五入后</th>
          <th style="text-align: center">加上 zero_point = -22</th>
          <th style="text-align: center">最终裁剪 $[-128,127]$</th>
          <th style="text-align: center">整数 $q$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">-2.5</td>
          <td style="text-align: center">$-106.250$</td>
          <td style="text-align: center">$-106$</td>
          <td style="text-align: center">$-106) + (-22) = -128$</td>
          <td style="text-align: center">$-128$</td>
          <td style="text-align: center">$-128$</td>
      </tr>
      <tr>
          <td style="text-align: center">-1.0</td>
          <td style="text-align: center">$-42.500$</td>
          <td style="text-align: center">$-42$</td>
          <td style="text-align: center">$-42) + (-22) = -64$</td>
          <td style="text-align: center">$-64$</td>
          <td style="text-align: center">$-64$</td>
      </tr>
      <tr>
          <td style="text-align: center">0.0</td>
          <td style="text-align: center">$   0.000$</td>
          <td style="text-align: center">$0$</td>
          <td style="text-align: center">$(0) + (-22) = -22$</td>
          <td style="text-align: center">$-22$</td>
          <td style="text-align: center">$-22$</td>
      </tr>
      <tr>
          <td style="text-align: center">1.0</td>
          <td style="text-align: center">$  42.500$</td>
          <td style="text-align: center">$42$</td>
          <td style="text-align: center">$(42) + (-22) = 20$</td>
          <td style="text-align: center">$20$</td>
          <td style="text-align: center">$20$</td>
      </tr>
      <tr>
          <td style="text-align: center">2.0</td>
          <td style="text-align: center">$  85.000$</td>
          <td style="text-align: center">$85$</td>
          <td style="text-align: center">$(85) + (-22) = 63$</td>
          <td style="text-align: center">$63$</td>
          <td style="text-align: center">$63$</td>
      </tr>
      <tr>
          <td style="text-align: center">3.5</td>
          <td style="text-align: center">$ 148.750$</td>
          <td style="text-align: center">$149$</td>
          <td style="text-align: center">$(149) + (-22) = 127$</td>
          <td style="text-align: center">$127$</td>
          <td style="text-align: center">$127$</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="5-反量化把整数-q-回还成浮点-hatx">5. 反量化：把整数 $q$ 回还成浮点 $\hat{x}$</h2>
<p>量化之后，我们往往还要做“反量化（dequantize）”步骤，把整数值 $q$ 转换回对应的浮点近似值 $\hat{x}$，用于下一层运算或评估量化带来的误差。反量化公式是：</p>
<p>$$
\hat{x} ;=; \text{scale} ;\times;\bigl(q - \text{zero_point}\bigr)
$$</p>
<p>其中 $\text{scale} = 0.0235294118$，$\text{zero_point} = -22$。</p>
<p>下面我们对上表中对应的整数 $q$ 再反算回浮点，看误差。</p>
<hr>
<h3 id="51-反量化-示例-1q---128对应原始-x---25">5.1 反量化 示例 1：q = -128（对应原始 x = -2.5）</h3>
<p>$$
\hat{x}
= 0.0235294118 \times \bigl(-128 - (-22)\bigr)
= 0.0235294118 \times \bigl(-128 + 22\bigr)
= 0.0235294118 \times (-106)
\approx -2.494117647
$$</p>
<ul>
<li>原始浮点：$-2.5$</li>
<li>反量化后：$\hat{x} \approx -2.49412$</li>
<li>绝对误差：$\bigl|, -2.5 ;-; (-2.49412)\bigr| \approx 0.00588$</li>
</ul>
<hr>
<h3 id="52-反量化-示例-2q---64对应原始-x---10">5.2 反量化 示例 2：q = -64（对应原始 x = -1.0）</h3>
<p>$$
\hat{x}
= 0.0235294118 \times \bigl(-64 - (-22)\bigr)
= 0.0235294118 \times (-64 + 22)
= 0.0235294118 \times (-42)
\approx -0.988235294
$$</p>
<ul>
<li>原始浮点：$-1.0$</li>
<li>反量化后：$\hat{x} \approx -0.98824$</li>
<li>绝对误差：$\approx 0.01176$</li>
</ul>
<hr>
<h3 id="53-反量化-示例-3q---22对应原始-x--00">5.3 反量化 示例 3：q = -22（对应原始 x = 0.0）</h3>
<p>$$
\hat{x}
= 0.0235294118 \times \bigl(-22 - (-22)\bigr)
= 0.0235294118 \times 0
= 0.0
$$</p>
<ul>
<li>原始浮点：$0.0$</li>
<li>反量化后：$\hat{x} = 0.0$</li>
<li>绝对误差：$;0.0$</li>
</ul>
<blockquote>
<p>这正是“零点偏移”的价值所在：<strong>保证浮点中的 0.0 在量化后能够精确还原回 0.0</strong>，不会产生任何偏差。</p></blockquote>
<hr>
<h3 id="54-反量化-示例-4q--20对应原始-x--10">5.4 反量化 示例 4：q = 20（对应原始 x = 1.0）</h3>
<p>$$
\hat{x}
= 0.0235294118 \times \bigl(20 - (-22)\bigr)
= 0.0235294118 \times 42
\approx 0.988235294
$$</p>
<ul>
<li>原始浮点：$1.0$</li>
<li>反量化后：$\hat{x} \approx 0.98824$</li>
<li>绝对误差：$\approx 0.01176$</li>
</ul>
<hr>
<h3 id="55-反量化-示例-5q--63对应原始-x--20">5.5 反量化 示例 5：q = 63（对应原始 x = 2.0）</h3>
<p>$$
\hat{x}
= 0.0235294118 \times \bigl(63 - (-22)\bigr)
= 0.0235294118 \times 85
\approx 2.0
$$</p>
<ul>
<li>这里因为 $\tfrac{2.0}{0.0235294118} = 85$ 恰好是整数，所以四舍五入后误差很小。</li>
<li>原始浮点：$2.0$</li>
<li>反量化后：$\hat{x} \approx 2.0$</li>
<li>绝对误差：$\approx 0.00000$（基本无误差）</li>
</ul>
<hr>
<h3 id="56-反量化-示例-6q--127对应原始-x--35">5.6 反量化 示例 6：q = 127（对应原始 x = 3.5）</h3>
<p>$$
\hat{x}
= 0.0235294118 \times \bigl(127 - (-22)\bigr)
= 0.0235294118 \times 149
\approx 3.505882353
$$</p>
<ul>
<li>原始浮点：$3.5$</li>
<li>反量化后：$\hat{x} \approx 3.50588$</li>
<li>绝对误差：$\approx 0.00588$</li>
</ul>
<hr>
<h2 id="6-小结零点偏移的作用">6. 小结：零点偏移的作用</h2>
<p>通过以上具体数值计算，我们可以看到：</p>
<ol>
<li>
<p><strong>零点对齐了“浮点0.0 → 整数 q”</strong>：</p>
<ul>
<li>我们设定 <code>zero_point = -22</code>，使得在量化时，浮点值刚好为 0 时，计算 $\tfrac{0}{\text{scale}} = 0$，再加上 $-22$ 就得到整数 $-22$。</li>
<li>反量化时，把 $-22$ 带回 $\hat{x} = \text{scale} \times (,-22 - (,-22),) = 0$。</li>
<li>因此，<strong>浮点 0 在量化后仍然能被精确表示，大幅减小了当零点参与后续运算（矩阵乘、卷积、加法）时累积的误差。</strong></li>
</ul>
</li>
<li>
<p><strong>支持浮点域非对称分布</strong>：</p>
<ul>
<li>
<p>如果没有 zero_point，那么只能用单纯的 <code>q = round(x/scale)</code>，这会使得浮点 0 被映射到整数 0（没有偏移）。但是此时，整数 0 对应的浮点值是 $\hat{x} = 0 \times \text{scale} = 0$。</p>
</li>
<li>
<p>这样看似“浮点 0”也可以映射到整数 0，但如果浮点最小值 $x_{\min}$ 不是恰好等于 $-scale \times q_{\min}$，就会导致映射后范围不对称。举例：假设 $x\in[-2.5,3.5]$，如果不引入 zero_point，只用 <code>scale</code> 进行映射，那么：</p>
<ul>
<li>浮点 $-2.5$ 会浮到一个不整的整数，大概是 $-2.5/0.0235294 \approx -106.25$，四舍五入变 $-106$，但是 $-106$ 并不等于 $-128$；</li>
<li>同时浮点 3.5 会变成 $\approx +149$，但整数 $\max$ 只能是 +127，发生剪裁（clamp），导致所有正向大值都被压缩到 +127，出现较大失真。</li>
</ul>
</li>
<li>
<p>因此，当浮点范围不对称时，需要把整数坐标“整体往右或往左平移”一个偏移量，让浮点 0 对应到整数 $q$ 上的某一个合适位置。这就是 zero_point 的用意：<strong>“将浮点值域整体在整数值域上做一个平移，保证边界对齐、中心（0）对齐”。</strong></p>
</li>
</ul>
</li>
<li>
<p><strong>减少量化误差、提升模型精度</strong>：</p>
<ul>
<li>通过精确对齐“浮点 0 ↔ 整数 zero_point”，模型在量化后做卷积之类的累加操作时，<strong>零激活（zero activation）真正变成整数 zero_point，不会再引入附加的偏差</strong>。</li>
<li>由上面各个示例的反量化误差来看，除了边界值 $-2.5$ 与 $3.5$ 会有大约 $0.005$ 左右的误差外，其他值误差都很小，且<strong>当 x = 0.0 时，误差严格为 0</strong>。如果没有 zero_point，这个“准确对齐 0”就不可能实现。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="7-扩展为什么-zero_point-有时是正数有时是负数">7. 扩展：为什么 zero_point 有时是正数、有时是负数？</h2>
<ol>
<li>当浮点范围 $[x_{\min}, x_{\max}]$ 中 <code>x_min</code> 和 <code>x_max</code> <strong>都为正</strong>（例如 $[0, 6]$），为了让 0.0 对齐到某个整数，计算出来的 <code>zero_point</code> 往往是<strong>正数</strong>（比如例子里是 0→0，或者映射到 128）。</li>
<li>当浮点范围两端跨负正两侧（如本例 $[-2.5,3.5]$），<code>zero_point</code> 可以是负数，正好把 “0.0” 放到整数坐标系的负数一侧。</li>
<li>当浮点范围 <strong>全都为负</strong>（例如 $[-10, -2]$），那么 <code>zero_point</code> 也会算出一个“非常负”的整数，保证浮点值域从 $-10$ 对应到整数 $-128$，从 $-2$ 对应到整数 $-127$，此时浮点 0 并不在范围内，但仍要把“0 对齐到整数”(虽然实际上网络里可能永远不会出现正的 0)。</li>
</ol>
<p>总之，<strong>zero_point 的符号与大小，只取决于浮点域相对于整数域中“0 要放在哪里”</strong>。</p>
<hr>
<h2 id="8-结论">8. 结论</h2>
<ul>
<li><strong>零点偏移（zero-point）</strong> 的根本目的是 <strong>“让浮点 0.0 在量化后能精确对齐到某个整数值”</strong>，这样在后续计算中，零 (zero) 的存在不会带来额外误差。</li>
<li>在计算时，先用 $\displaystyle \text{scale} = \frac{x_{\max}-x_{\min}}{q_{\max}-q_{\min}}$ 得到缩放因子，再用 $\displaystyle \text{zero_point} = \mathrm{round}\bigl(q_{\min} - \frac{x_{\min}}{\text{scale}}\bigr)$ 得到整型零点。</li>
<li>通过本例 $[x_{\min},x_{\max}]=[-2.5,,3.5]$ 对映 $[q_{\min},q_{\max}]=[-128,,127]$，我们详细演示了 <code>scale≈0.0235294</code>、<code>zero_point=-22</code> 的计算过程，并一一对比了量化前后的浮点值及反量化误差，直观体现出 zero_point 如何让“0”保持无损，还能保证两端边界映射后尽量均匀分布在整数范围里。</li>
</ul>
<p>希望这个<strong>完整、一步步推导</strong>的示例，能让你对“零点偏移”的<strong>计算方式</strong>与<strong>实际作用</strong>有更深刻的理解。如果还有更深入的疑问（比如不同量化策略、对称 vs. 非对称量化、逐通道量化等），也可以继续交流！</p>
<div class="edit-meta"> <br></div><nav class="pagination"><a class="nav nav-prev" href="http://localhost:1313/transformer/" title="ML基础"><i class="fas fa-arrow-left" aria-hidden="true"></i>&nbsp;Prev - ML基础</a>
<a class="nav nav-next" href="http://localhost:1313/transformer/quantization/" title="模型量化基础代码版">Next - 模型量化基础代码版 <i class="fas fa-arrow-right" aria-hidden="true"></i></a>
</nav><footer> </footer>
</main>
<div class="sidebar">

<nav class="slide-menu">
<ul>
<li class=""><a href="http://localhost:1313/">about me</a></li>

<li class=" has-sub-menu"><a href="http://localhost:1313/learn_cs/">cs基础<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/learn_cs/git-crash-course/">git-crash-course</a></li>
<li class=""><a href="http://localhost:1313/learn_cs/react-electron/ts-foundation/">ts-foundation</a></li>
<li class=""><a href="http://localhost:1313/learn_cs/react-electron/ipc/">ipc基础</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="http://localhost:1313/algorithm/">算法题<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/algorithm/hash_set/">哈希表相关题目</a></li>
<li class=""><a href="http://localhost:1313/algorithm/double_point/">双指针</a></li>
<li class=""><a href="http://localhost:1313/algorithm/strings/">string类题目</a></li>
<li class=""><a href="http://localhost:1313/algorithm/dynamic-programmnig/">动态规划问题</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="http://localhost:1313/math_foundation/">ML中的数学<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/math_foundation/information/">信息量</a></li>
<li class=""><a href="http://localhost:1313/math_foundation/likelihood_entropy/">似然函数_交叉熵</a></li>
<li class=""><a href="http://localhost:1313/math_foundation/kl_dpo/">Kl散度与dpo算法</a></li>
</ul>
  
</li>

<li class="parent has-sub-menu"><a href="http://localhost:1313/transformer/">ML基础<span class="mark opened">-</span></a>
  
<ul class="sub-menu">
<li class="active"><a href="http://localhost:1313/transformer/quantization2/">模型量化基础</a></li>
<li class=""><a href="http://localhost:1313/transformer/quantization/">模型量化基础代码版</a></li>
<li class=""><a href="http://localhost:1313/transformer/do_sample_para/">Do_sample_para</a></li>
<li class=""><a href="http://localhost:1313/transformer/nn_begin/">Nn Begin</a></li>
<li class=""><a href="http://localhost:1313/transformer/entropy/">Entropy</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="http://localhost:1313/kubernetes/">kubernetes<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/kubernetes/installation/">Installation</a></li>
</ul>
  
</li>

<li class=""><a href="http://localhost:1313/learn_english/">英语学习</a>
  
</li>

<li class=" has-sub-menu"><a href="http://localhost:1313/golang/">golang<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/golang/byte-character/">字符编码基础知识详解</a></li>
<li class=""><a href="http://localhost:1313/golang/matrix-golang/">matrix-golang</a></li>
<li class=""><a href="http://localhost:1313/golang/sort/">sort包用法</a></li>
<li class=""><a href="http://localhost:1313/golang/foundation/">Go 语言基础知识</a></li>
<li class=""><a href="http://localhost:1313/golang/base/">Base</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="http://localhost:1313/linux_foundation/">Linux基础<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/linux_foundation/linux-commands/">50个常用Linux命令</a></li>
<li class=""><a href="http://localhost:1313/linux_foundation/cs_foundation/">计算机基础知识</a></li>
<li class=""><a href="http://localhost:1313/linux_foundation/linux-common/">Linux Common</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="http://localhost:1313/exercise/">workout<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/exercise/workout/">Workout</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="http://localhost:1313/others/">others<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="http://localhost:1313/others/create-hugo-gitpage/">使用 Hugo 和 GitHub Pages 创建个人网站</a></li>
</ul>
  
</li>
</ul>
</nav>

 
<div class="sidebar-footer"></div>
</div>

</div><a href="#" id="backtothetop-fixed" class="backtothetop"
 data-backtothetop-duration="600"
 data-backtothetop-easing="easeOutQuart"
 data-backtothetop-fixed-fadeIn="1000"
 data-backtothetop-fixed-fadeOut="1000"
 data-backtothetop-fixed-bottom="10"
 data-backtothetop-fixed-right="20">
<span class="fa-layers fa-fw">
<i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i>
</span></a>
</div>
</body>
</html>
