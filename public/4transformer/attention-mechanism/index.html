<!DOCTYPE html>
<html lang="zh-cn">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<title>注意力机制详解 - 高新 | AI平台开发工程师</title>
<meta name="description" content="Transformer的核心：注意力机制原理与实现">
<meta name="generator" content="Hugo 0.145.0">
<link href="https://mlcore-engine.github.io//index.xml" rel="alternate" type="application/rss+xml">
<link rel="canonical" href="https://mlcore-engine.github.io/4transformer/attention-mechanism/">
<link rel="stylesheet" href="https://mlcore-engine.github.io/css/theme.min.css">
<link rel="stylesheet" href="https://mlcore-engine.github.io/css/chroma.min.css">
<script defer src="https://mlcore-engine.github.io//js/fontawesome6/all.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js" integrity="sha256-H3cjtrm/ztDeuhCN9I4yh4iN2Ybx/y1RM7rMmAesA0k=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js" integrity="sha256-4XodgW4TwIJuDtf+v6vDJ39FVxI0veC/kSCCmnFp7ck=" crossorigin="anonymous"></script>
<script src="https://mlcore-engine.github.io/js/bundle.js"></script><style>
:root {--custom-font-color: #ffffff;--custom-background-color: #1a365d;}
</style>
<meta property="og:url" content="https://mlcore-engine.github.io/4transformer/attention-mechanism/">
  <meta property="og:site_name" content="高新 | AI平台开发工程师">
  <meta property="og:title" content="注意力机制详解">
  <meta property="og:description" content="Transformer的核心：注意力机制原理与实现">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="4transformer">
    <meta property="article:published_time" content="2024-04-03T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-04-03T00:00:00+00:00">
    <meta property="og:image" content="https://mlcore-engine.github.io/home/me.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://mlcore-engine.github.io/home/me.png">
  <meta name="twitter:title" content="注意力机制详解">
  <meta name="twitter:description" content="Transformer的核心：注意力机制原理与实现">

  <meta itemprop="name" content="注意力机制详解">
  <meta itemprop="description" content="Transformer的核心：注意力机制原理与实现">
  <meta itemprop="datePublished" content="2024-04-03T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-04-03T00:00:00+00:00">
  <meta itemprop="wordCount" content="1242">
  <meta itemprop="image" content="https://mlcore-engine.github.io/home/me.png"></head>
<body><div class="container"><header>
<h1>高新 | AI平台开发工程师</h1><a href="https://github.com/mlcore-engine/mlcore-engine" class="github"><i class="fab fa-github"></i></a>
</header>


<div class="content-container">
<main><h1>注意力机制详解</h1>
<h1 id="注意力机制详解">注意力机制详解</h1>
<p>注意力机制是Transformer架构的核心创新，使模型能够动态聚焦于输入序列中的相关部分。本文详细解析注意力机制的工作原理和具体实现。</p>
<h2 id="注意力的基本概念">注意力的基本概念</h2>
<p>注意力机制的基本思想源于人类认知：当我们阅读或听取信息时，会选择性地关注相关部分而忽略无关部分。在深度学习中，注意力机制允许模型根据当前上下文动态聚焦于输入的不同部分。</p>
<h3 id="早期注意力机制">早期注意力机制</h3>
<ul>
<li><strong>基于位置的注意力</strong>：在RNN中根据隐藏状态计算权重</li>
<li><strong>基于内容的注意力</strong>：考虑查询与键的相关性</li>
</ul>
<h2 id="self-attention详解">Self-Attention详解</h2>
<h3 id="数学定义">数学定义</h3>
<p>Self-Attention的计算可以表示为：</p>
<p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$</p>
<p>其中：</p>
<ul>
<li>$Q$（查询）：从输入序列计算得到的查询矩阵</li>
<li>$K$（键）：从输入序列计算得到的键矩阵</li>
<li>$V$（值）：从输入序列计算得到的值矩阵</li>
<li>$d_k$：键向量的维度，用于归一化</li>
</ul>
<h3 id="计算步骤">计算步骤</h3>
<ol>
<li>
<p><strong>线性投影</strong>：将输入向量通过线性变换转换为查询、键和值向量</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Q <span style="color:#f92672">=</span> X <span style="color:#f92672">*</span> W_Q
</span></span><span style="display:flex;"><span>K <span style="color:#f92672">=</span> X <span style="color:#f92672">*</span> W_K
</span></span><span style="display:flex;"><span>V <span style="color:#f92672">=</span> X <span style="color:#f92672">*</span> W_V
</span></span></code></pre></div></li>
<li>
<p><strong>计算注意力分数</strong>：查询和键的点积决定了每个位置的关注度</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>scores <span style="color:#f92672">=</span> matmul(Q, K<span style="color:#f92672">.</span>transpose(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)) <span style="color:#f92672">/</span> sqrt(d_k)
</span></span></code></pre></div></li>
<li>
<p><strong>应用掩码</strong>（可选）：在解码器中防止看到未来信息</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">if</span> mask <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>    scores <span style="color:#f92672">=</span> scores<span style="color:#f92672">.</span>masked_fill(mask <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1e9</span>)
</span></span></code></pre></div></li>
<li>
<p><strong>Softmax归一化</strong>：将分数转换为概率分布</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>attention_weights <span style="color:#f92672">=</span> softmax(scores, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div></li>
<li>
<p><strong>加权求和</strong>：根据权重聚合值向量</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>output <span style="color:#f92672">=</span> matmul(attention_weights, V)
</span></span></code></pre></div></li>
</ol>
<h2 id="多头注意力机制">多头注意力机制</h2>
<h3 id="概念">概念</h3>
<p>多头注意力通过并行运行多个注意力&quot;头&quot;来捕获不同子空间的信息，增强模型的表达能力。</p>
<h3 id="优势">优势</h3>
<ul>
<li>允许模型关注来自不同表示子空间的信息</li>
<li>从不同角度理解序列中的关系</li>
<li>增强模型的表达能力</li>
</ul>
<h3 id="实现">实现</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">multi_head_attention</span>(query, key, value, h):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 线性投影到h个头</span>
</span></span><span style="display:flex;"><span>    batch_size <span style="color:#f92672">=</span> query<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 投影并分割为h个头</span>
</span></span><span style="display:flex;"><span>    Q <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_Q(query)<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, h, d_k)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    K <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_K(key)<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, h, d_k)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    V <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_V(value)<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, h, d_k)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 计算注意力并连接</span>
</span></span><span style="display:flex;"><span>    attn_outputs <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(h):
</span></span><span style="display:flex;"><span>        attn_output <span style="color:#f92672">=</span> self_attention(Q[:, i], K[:, i], V[:, i])
</span></span><span style="display:flex;"><span>        attn_outputs<span style="color:#f92672">.</span>append(attn_output)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 连接并投影回原始维度</span>
</span></span><span style="display:flex;"><span>    concat <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat(attn_outputs, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_O(concat)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> output
</span></span></code></pre></div><h2 id="注意力变体">注意力变体</h2>
<h3 id="掩码自注意力">掩码自注意力</h3>
<ul>
<li>用于解码器的自回归生成</li>
<li>防止模型看到未来的信息</li>
<li>实现方式：在softmax前将未来位置的分数设为负无穷</li>
</ul>
<h3 id="交叉注意力">交叉注意力</h3>
<ul>
<li>用于编码器-解码器架构中</li>
<li>解码器关注编码器的输出</li>
<li>查询来自解码器，键和值来自编码器</li>
</ul>
<h2 id="注意力可视化">注意力可视化</h2>
<p>注意力权重可视化是理解模型行为的重要工具：</p>
<ul>
<li>横轴和纵轴表示序列中的位置</li>
<li>深色表示高注意力权重</li>
<li>可以揭示模型关注语法结构、语义关系等</li>
</ul>
<h2 id="注意力效率优化">注意力效率优化</h2>
<p>随着序列长度增加，标准注意力的计算复杂度为O(n²)，存在优化空间：</p>
<ul>
<li><strong>稀疏注意力</strong>：只计算部分位置对之间的注意力</li>
<li><strong>局部注意力</strong>：限制注意力窗口大小</li>
<li><strong>线性注意力</strong>：改变计算顺序降低复杂度</li>
<li><strong>FlashAttention</strong>：优化内存访问模式提高计算效率</li>
</ul>
<h2 id="实际应用示例">实际应用示例</h2>
<p>看一个简单的PyTorch实现：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SelfAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, embed_size, heads):
</span></span><span style="display:flex;"><span>        super(SelfAttention, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>embed_size <span style="color:#f92672">=</span> embed_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>heads <span style="color:#f92672">=</span> heads
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>head_dim <span style="color:#f92672">=</span> embed_size <span style="color:#f92672">//</span> heads
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>values <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(self<span style="color:#f92672">.</span>head_dim, self<span style="color:#f92672">.</span>head_dim, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>keys <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(self<span style="color:#f92672">.</span>head_dim, self<span style="color:#f92672">.</span>head_dim, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>queries <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(self<span style="color:#f92672">.</span>head_dim, self<span style="color:#f92672">.</span>head_dim, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc_out <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(heads <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>head_dim, embed_size)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, values, keys, query, mask):
</span></span><span style="display:flex;"><span>        N <span style="color:#f92672">=</span> query<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        value_len, key_len, query_len <span style="color:#f92672">=</span> values<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], keys<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], query<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 分割嵌入维度为多头</span>
</span></span><span style="display:flex;"><span>        values <span style="color:#f92672">=</span> values<span style="color:#f92672">.</span>reshape(N, value_len, self<span style="color:#f92672">.</span>heads, self<span style="color:#f92672">.</span>head_dim)
</span></span><span style="display:flex;"><span>        keys <span style="color:#f92672">=</span> keys<span style="color:#f92672">.</span>reshape(N, key_len, self<span style="color:#f92672">.</span>heads, self<span style="color:#f92672">.</span>head_dim)
</span></span><span style="display:flex;"><span>        queries <span style="color:#f92672">=</span> query<span style="color:#f92672">.</span>reshape(N, query_len, self<span style="color:#f92672">.</span>heads, self<span style="color:#f92672">.</span>head_dim)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 计算注意力</span>
</span></span><span style="display:flex;"><span>        energy <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;nqhd,nkhd-&gt;nhqk&#34;</span>, [queries, keys])
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> mask <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            energy <span style="color:#f92672">=</span> energy<span style="color:#f92672">.</span>masked_fill(mask <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, float(<span style="color:#e6db74">&#34;-1e20&#34;</span>))
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        attention <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(energy <span style="color:#f92672">/</span> (self<span style="color:#f92672">.</span>embed_size <span style="color:#f92672">**</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>)), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;nhql,nlhd-&gt;nqhd&#34;</span>, [attention, values])
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> out<span style="color:#f92672">.</span>reshape(N, query_len, self<span style="color:#f92672">.</span>heads <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>head_dim)
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc_out(out)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span></code></pre></div><h2 id="总结">总结</h2>
<p>注意力机制是现代NLP模型的基石，通过动态关注输入序列的不同部分，使模型能够处理长距离依赖和复杂语言结构。理解注意力机制的工作原理，对于深入理解和改进Transformer架构至关重要。</p>
<div class="edit-meta">
Last updated on 2024-04-03


<br>
Published on 2024-04-03
<br></div><nav class="pagination"><a class="nav nav-prev" href="https://mlcore-engine.github.io/4transformer/" title="Transformer架构解析"><i class="fas fa-arrow-left" aria-hidden="true"></i>&nbsp;Prev - Transformer架构解析</a>
<a class="nav nav-next" href="https://mlcore-engine.github.io/5kubernetes/" title="Kubernetes实践指南">Next - Kubernetes实践指南 <i class="fas fa-arrow-right" aria-hidden="true"></i></a>
</nav><footer><p class="powered">Powered by <a href="https://gohugo.io">Hugo</a>. Theme by <a href="https://themes.gohugo.io/hugo-theme-techdoc/">TechDoc</a>. Designed by <a href="https://github.com/thingsym/hugo-theme-techdoc">Thingsym</a>.</p>
</footer>
</main>
<div class="sidebar">

<nav class="slide-menu">
<ul>
<li class=""><a href="https://mlcore-engine.github.io/">Home</a></li>

<li class=" has-sub-menu"><a href="https://mlcore-engine.github.io/1learn-cs/">自学CS学习路线图<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="https://mlcore-engine.github.io/1learn-cs/python-basics/">Python基础入门</a></li>
<li class=""><a href="https://mlcore-engine.github.io/1learn-cs/algorithms/">算法与数据结构</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="https://mlcore-engine.github.io/2learn-english/">英语学习<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="https://mlcore-engine.github.io/2learn-english/reading-skills/">英语阅读技能提升</a></li>
<li class=""><a href="https://mlcore-engine.github.io/2learn-english/listening-skills/">英语听力技能提升</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="https://mlcore-engine.github.io/3math-foundation/">数学基础(矩阵的5种变换)<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="https://mlcore-engine.github.io/3math-foundation/linear-algebra/">线性代数基础</a></li>
</ul>
  
</li>

<li class="parent has-sub-menu"><a href="https://mlcore-engine.github.io/4transformer/">Transformer架构解析<span class="mark opened">-</span></a>
  
<ul class="sub-menu">
<li class="active"><a href="https://mlcore-engine.github.io/4transformer/attention-mechanism/">注意力机制详解</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="https://mlcore-engine.github.io/5kubernetes/">Kubernetes实践指南<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="https://mlcore-engine.github.io/5kubernetes/kubernetes-basics/">Kubernetes入门基础</a></li>
</ul>
  
</li>

<li class=" has-sub-menu"><a href="https://mlcore-engine.github.io/6linux-foundation/">Linux基础知识<span class="mark closed">+</span></a>
  
<ul class="sub-menu">
<li class=""><a href="https://mlcore-engine.github.io/6linux-foundation/linux-commands/">50个常用Linux命令</a></li>
</ul>
  
</li>
</ul>
</nav>



<div class="sidebar-footer"></div>
</div>

</div><a href="#" id="backtothetop-fixed" class="backtothetop"
 data-backtothetop-duration="600"
 data-backtothetop-easing="easeOutQuart"
 data-backtothetop-fixed-fadeIn="1000"
 data-backtothetop-fixed-fadeOut="1000"
 data-backtothetop-fixed-bottom="10"
 data-backtothetop-fixed-right="20">
<span class="fa-layers fa-fw">
<i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i>
</span></a>
</div>
</body>
</html>
