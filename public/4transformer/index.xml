<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformer架构解析 on 高新 | AI平台开发工程师</title>
    <link>https://mlcore-engine.github.io/4transformer/</link>
    <description>Recent content in Transformer架构解析 on 高新 | AI平台开发工程师</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 03 Apr 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://mlcore-engine.github.io/4transformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>注意力机制详解</title>
      <link>https://mlcore-engine.github.io/4transformer/attention-mechanism/</link>
      <pubDate>Wed, 03 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://mlcore-engine.github.io/4transformer/attention-mechanism/</guid>
      <description>&lt;h1 id=&#34;注意力机制详解&#34;&gt;注意力机制详解&lt;/h1&gt;&#xA;&lt;p&gt;注意力机制是Transformer架构的核心创新，使模型能够动态聚焦于输入序列中的相关部分。本文详细解析注意力机制的工作原理和具体实现。&lt;/p&gt;&#xA;&lt;h2 id=&#34;注意力的基本概念&#34;&gt;注意力的基本概念&lt;/h2&gt;&#xA;&lt;p&gt;注意力机制的基本思想源于人类认知：当我们阅读或听取信息时，会选择性地关注相关部分而忽略无关部分。在深度学习中，注意力机制允许模型根据当前上下文动态聚焦于输入的不同部分。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
