+++
date = '2025-04-16T17:11:57+08:00'
draft = false
title = 'Do_sample_para'
+++

##### 在大语言模型微调和采样过程中有几个重要的采样参数需要懂得，下面我将讲解三个特别重要的采样参数，这3个参数的设置会直接影响模型的表现。顺便再解释一下经常看到的一个重要概念logits。

- **top_k**
- **top_p**
- **temperature**

---

#### **背景：语言模型的概率分布**
假设语言模型在预测下一个词时，面对一个包含 5 个词的词汇表：["猫", "狗", "鸟", "鱼", "马"]。模型会为每个词分配一个概率，例如：

- 猫: 0.4  
- 狗: 0.3  
- 鸟: 0.15  
- 鱼: 0.1  
- 马: 0.05  

如果使用 **贪婪解码**（greedy decoding），模型会直接选择概率最高的词 "猫"。但这种方式生成的文本可能过于单一，因此我们引入 Top-K 和 Top-P 以及temperature 采样来增加多样性。

---

#### **Top-K 采样**
**Top-K 采样** 是从概率最高的前 K 个词中随机选择一个词。K 的大小决定了选择的范围和输出的多样性。

##### **例子：**
继续用上面的概率分布：
- 猫: 0.4  
- 狗: 0.3  
- 鸟: 0.15  
- 鱼: 0.1  
- 马: 0.05  

1. **K = 1**  
   只选择概率最高的一个词，即 "猫"。这等同于贪婪解码，输出完全基于最高概率，没有随机性。

2. **K = 2**  
   选择概率最高的前 2 个词：  
   - 猫 (0.4)  
   - 狗 (0.3)  
   模型会在这两个词中随机选择一个。比如，随机选择 "狗"，那么输出就是 "狗"。相比 K=1，多了一些随机性。

3. **K = 5**  
   选择所有 5 个词，相当于从整个词汇表中随机采样。这时输出的创造性最强，但可能会偏离高概率的词。

##### **特点：**
- K 值越高（如 K=5），输出越具创造性和多样性，因为可选的词更多。
- K 值越低（如 K=1），输出越受限制，更接近事实或高概率的词。

---

#### **Top-P 采样**
**Top-P 采样**（也叫 nucleus sampling）是根据累积概率选择词集。它会从概率最高的词开始累加，直到累积概率达到或超过某个阈值 P，然后从这个词集中随机选择一个词。

##### **例子：**
还是用相同的概率分布：
- 猫: 0.4  
- 狗: 0.3  
- 鸟: 0.15  
- 鱼: 0.1  
- 马: 0.05  

1. **P = 0.0**  
   理论上累积概率为 0 时不选择任何词，但实际上这等同于贪婪解码，只选 "猫"。

2. **P = 0.7**  
   从概率最高的词开始累加：  
   - 猫: 0.4  
   - 猫 + 狗: 0.4 + 0.3 = 0.7  
   累积概率达到 0.7，词集为 ["猫", "狗"]。模型在这两个词中随机选择一个，比如 "狗"。

3. **P = 0.85**  
   继续累加：  
   - 猫: 0.4  
   - 猫 + 狗: 0.4 + 0.3 = 0.7  
   - 猫 + 狗 + 鸟: 0.4 + 0.3 + 0.15 = 0.85  
   累积概率达到 0.85，词集为 ["猫", "狗", "鸟"]。模型从中随机选择一个，比如 "鸟"。

4. **P = 1.0**  
   所有词的累积概率为 1.0，词集包含 ["猫", "狗", "鸟", "鱼", "马"]。模型从整个词汇表中随机选择，输出的多样性最高。

##### **特点：**
- P 值越高（如 P=1.0），选择的词集越大，输出越多样化。
- P 值越低（如 P=0.0），选择的词集越小，输出越接近贪婪解码，更基于高概率词。

---

#### **Top-K 和 Top-P 的比较**
- **Top-K**：固定选择前 K 个词，范围大小不随概率分布变化。例如，K=2 时总是选 2 个词，即使概率分布很集中或很分散。
- **Top-P**：动态调整词集大小，根据概率分布选择累积概率达到 P 的词。如果概率分布集中，词集可能很小；如果分布平坦，词集会更大。

#### **举个极端例子：**
假设概率分布极度集中：  
- 猫: 0.9  
- 狗: 0.05  
- 鸟: 0.03  
- 鱼: 0.01  
- 马: 0.01  

- **Top-K (K=2)**：选 "猫" 和 "狗"，忽略概率差异。
- **Top-P (P=0.9)**：只选 "猫"，因为 0.9 已达到阈值，更贴近分布特性。

---

#### **实际应用**
- **创意写作**：用较大的 K（如 5）或 P（如 0.9），生成更具想象力的文本。
- **问答系统**：用较小的 K（如 1 或 2）或 P（如 0.5），生成更准确、基于事实的答案。

---

#### **总结**
- **Top-K 采样**：从概率最高的前 K 个词中随机选一个，K 越大越多样，K=1 等同贪婪解码。
- **Top-P 采样**：从累积概率达到 P 的词集中随机选一个，P 从 0（贪婪解码）到 1（全词汇表）。

#### **Temperature**

**Temperature** 是一个控制生成文本随机性和多样性的超参数，它直接调整模型输出的概率分布。

---

#### **背景：语言模型的概率分布**
假设语言模型预测下一个词，词汇表包含 5 个词：["猫", "狗", "鸟", "鱼", "马"]，原始概率分布如下：

- 猫: 0.4  
- 狗: 0.3  
- 鸟: 0.15  
- 鱼: 0.1  
- 马: 0.05  

这些概率是模型的原始输出（logits 经过 softmax 转换）。Temperature 参数通过缩放 logits 来改变概率分布的"形状"，从而影响生成的随机性和多样性。

---

#### **Temperature 的作用**
Temperature（记为 $ T $）是一个正数，作用于模型的 logits。公式如下：

1. 模型的 logits（未归一化的分数）为 $z_i$（对应每个词）。
2. Temperature 调整后的 logits 为 $z_i / T$。 
3. 调整后的 logits 再通过 softmax 转换为新的概率分布：

   $
   P(i) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
   $

- **T > 1**：使概率分布更平滑，增加低概率词的选中机会，生成更随机、更有创造性的输出。
- **T < 1**：使概率分布更尖锐，放大高概率词的优势，生成更确定、基于事实的输出。
- **T = 1**：保持原始概率分布不变。
- **T → 0**：趋向于贪婪解码，只选择概率最高的词。

---

#### **不同 Temperature 的效果**
为了简化，直接用原始概率分布（假设它们来自 T=1 的 softmax 输出），并通过 temperature 调整后的概率分布来说明效果。以下是不同 temperature 值对概率分布的影响：

##### **1. Temperature = 1（默认）**
- 概率分布保持不变：
  - 猫: 0.4  
  - 狗: 0.3  
  - 鸟: 0.15  
  - 鱼: 0.1  
  - 马: 0.05  
- 如果采样（而不是贪婪解码），模型会根据这些概率随机选择一个词。例如，40% 概率选 "猫"，30% 概率选 "狗"。输出有一定随机性，但仍偏向高概率词。

##### **2. Temperature = 0.5（低温度）**
降低 temperature 使概率分布更尖锐，高概率词的概率进一步增加，低概率词的概率被压缩。假设调整后概率分布变为：

- 猫: 0.65（概率显著增加）
- 狗: 0.25  
- 鸟: 0.07  
- 鱼: 0.02  
- 马: 0.01  

- **效果**：模型更倾向于选择 "猫" 或 "狗"，尤其是 "猫"（概率从 0.4 升到 0.65）。输出更确定、更可预测，接近贪婪解码（如果 T 趋向 0，则几乎只选 "猫"）。
- **应用场景**：适合需要高准确性、基于事实的任务，如问答系统。

##### **3. Temperature = 2（高温度）**
提高 temperature 使概率分布更平滑，低概率词的概率增加，高概率词的概率降低。假设调整后概率分布变为：

- 猫: 0.28  
- 狗: 0.24  
- 鸟: 0.20  
- 鱼: 0.16  
- 马: 0.12  

- **效果**：所有词的概率更接近，"马"（原本 0.05）现在有 0.12 的概率被选中。模型生成更随机、更具多样性的输出，可能生成意想不到的词如 "马"。
- **应用场景**：适合创意写作或头脑风暴，需要多样性和新奇的输出。

##### **4. Temperature = 0.1（极低温度）**
当 temperature 非常低时，概率分布极度尖锐，几乎只选择最高概率的词：

- 猫: 0.99  
- 狗: 0.01  
- 鸟: ~0  
- 鱼: ~0  
- 马: ~0  

- **效果**：模型几乎总是选择 "猫"，等同于贪婪解码。输出完全基于最高概率，没有随机性。
- **应用场景**：需要绝对确定性的场景，但可能过于单调。

##### **5. Temperature = 10（极高温度）**
当 temperature 非常高时（毫无意义），概率分布接近均匀分布：

- 猫: 0.2  
- 狗: 0.2  
- 鸟: 0.2  
- 鱼: 0.2  
- 马: 0.2  

- **效果**：每个词被选中的概率几乎相等，模型随机性极高，可能生成非常离谱或不相关的词，如 "马"。输出几乎完全随机，失去语义连贯性。
- **应用场景**：极少使用，除非需要完全随机的输出。

---

- **Temperature 调整概率分布的形状**：决定高概率词和低概率词的相对权重。
- **Top-K 或 Top-P 限制采样范围**：决定从哪些词中采样。

##### **结合例子：**
假设使用 Top-K=3 采样，原始分布为：
- 猫: 0.4, 狗: 0.3, 鸟: 0.15, 鱼: 0.1, 马: 0.05  

1. **T = 0.5, Top-K=3**：
   - 调整后分布：猫: 0.65, 狗: 0.25, 鸟: 0.07（鱼和马概率极低）。
   - Top-K=3 选择 ["猫", "狗", "鸟"]，但由于猫的概率极高，输出几乎总是 "猫" 或 "狗"。
   - 结果：输出非常可预测，接近事实。

2. **T = 2, Top-K=3**：
   - 调整后分布：猫: 0.28, 狗: 0.24, 鸟: 0.20（鱼和马概率较高，但未进入 Top-K）。
   - Top-K=3 选择 ["猫", "狗", "鸟"]，概率更均匀，"鸟" 被选中的机会显著增加。
   - 结果：输出更具多样性和创造性。

3. **T = 1, Top-P=0.85**：
   - 原始分布：猫 (0.4) + 狗 (0.3) + 鸟 (0.15) = 0.85。
   - Top-P=0.85 选择 ["猫", "狗", "鸟"]，按原始概率采样。
   - 结果：适度的随机性和多样性。

4. **T = 2, Top-P=0.85**：
   - 调整后分布更平滑：猫: 0.28, 狗: 0.24, 鸟: 0.20, 鱼: 0.16（累积到 0.88，接近 0.85）。
   - Top-P=0.85 可能只选 ["猫", "狗", "鸟"]，但 "鸟" 的机会更高。
   - 结果：比 T=1 时更随机，生成更具创意。

---

#### **实际应用**
- **低 Temperature（如 0.5）**：适合需要高准确性和可预测性的任务，如技术文档生成、问答系统。
- **高 Temperature（如 1.2）**：适合需要创意和多样性的任务，如故事生成、诗歌创作。
- **结合 Top-K/Top-P**：Temperature 控制概率分布的平滑度，Top-K/Top-P 限制采样范围，二者配合可以精细调整生成效果。

---

#### **总结**
- **Temperature** 控制概率分布的尖锐或平滑程度：
  - **T < 1**：分布更尖锐，偏向高概率词，输出更确定（接近贪婪解码）。
  - **T > 1**：分布更平滑，增加低概率词的机会，输出更随机和多样。
  - **T = 1**：保持原始分布。
- 在之前的例子中，T=0.5 使 "猫" 主导，T=2 让 "马" 也有机会被选中。
- 与 Top-K 和 Top-P 结合使用时，Temperature 决定概率分布的特性，Top-K/Top-P 决定采样范围，共同塑造生成文本的风格。

---

#### **解释一个上文提到的重要概念**
**Logits** 是语言模型在预测下一个词时，输出的原始、未归一化的分数（或“得分”）。它们通常是由模型的最后一层（例如全连接层）计算得来的，表示每个可能词的“倾向性”或“可能性”。Logits 本身没有特定的范围（可以是任意实数），需要通过归一化（如 softmax）转换为概率。

#### **背景**
假设语言模型预测下一个词，词汇表包含 5 个词：["猫", "狗", "鸟", "鱼", "马"]。模型的最后一层为每个词输出一个 logit 分数：

- 猫: 2.0  
- 狗: 1.5  
- 鸟: 0.5  
- 鱼: 0.0  
- 马: -1.0  

这些 logits 是模型对每个词的“偏好”评分，但它们不是概率（例如，2.0 和 -1.0 无法直接解释为概率）。为了将 logits 转换为概率，我们需要使用 **softmax** 函数。

---

#### **什么是 Softmax？**
**Softmax** 是一种归一化函数，将一组任意实数（logits）转换为概率分布，使得：
1. 每个概率值在 [0, 1] 范围内。
2. 所有概率之和等于 1。

Softmax 的公式为：

$
P(x_i) = \frac{\exp(z_i)}{\sum_{j=1}^N \exp(z_j)}
$

其中：
- $z_i$ 是第 $i$ 个词的 logit。
- $ \exp(z_i) $ 是 $ z_i $ 的指数函数（$ e^{z_i} $）。
- 分母是所有词的指数和，用于归一化。
- $ P(x_i) $ 是第 $ i $ 个词的概率。

Softmax 的作用是将 logits 转换为可解释的概率，同时保留 logits 之间的相对大小关系。

---

#### **从 Logits 到 Softmax**

##### **输入 Logits**
- 猫: 2.0  
- 狗: 1.5  
- 鸟: 0.5  
- 鱼: 0.0  
- 马: -1.0  

##### **步骤 1：计算指数$exp(z_i)$**
Softmax 使用指数函数 $ \exp(x) $（即 $ e^x $），其中 $ e \approx 2.71828 $ 来处理 logits，确保输出为正数：

- 猫: $ \exp(2.0) \approx 7.389 $  
- 狗: $ \exp(1.5) \approx 4.482 $  
- 鸟: $ \exp(0.5) \approx 1.649 $  
- 鱼: $ \exp(0.0) = 1.000 $  
- 马: $ \exp(-1.0) \approx 0.368 $  

这些值反映了 logits 的相对大小，指数函数放大了高 logits 的值，压缩了低 logits 的值。

##### **步骤 2：计算指数和**
将所有指数值相加，得到分母：

$
7.389 + 4.482 + 1.649 + 1.000 + 0.368 \approx 14.888
$

##### **步骤 3：计算概率（softmax）**
对每个词的指数值除以总和，得到概率：

- 猫: $ \frac{7.389}{14.888} \approx 0.496 $ （49.6%）  
- 狗: $ \frac{4.482}{14.888} \approx 0.301 $ （30.1%）  
- 鸟: $ \frac{1.649}{14.888} \approx 0.111 $ （11.1%）  
- 鱼: $ \frac{1.000}{14.888} \approx 0.067 $ （6.7%）  
- 马: $ \frac{0.368}{14.888} \approx 0.025 $ （2.5%）  

#### **输出概率分布**
最终的概率分布为：

- 猫: 0.496  
- 狗: 0.301  
- 鸟: 0.111  
- 鱼: 0.067  
- 马: 0.025  

检查：所有概率之和为 $ 0.496 + 0.301 + 0.111 + 0.067 + 0.025 = 1.000 $，满足概率分布的要求。

---

#### **Softmax 的特点**
1. **放大差异**：由于指数函数的非线性，较高的 logit（如 2.0）会得到远高于低 logit（如 -1.0）的概率（0.496 vs. 0.025）。
2. **归一化**：确保概率和为 1，适合用于分类任务（如预测下一个词）。
3. **保留相对顺序**：logit 的大小顺序在 softmax 后仍然保持（猫 > 狗 > 鸟 > 鱼 > 马）。

---

#### **验证 Temperature  **
temperature 参数会调整 logits。假设我们用 temperature: $ T = 0.5 $：

1. 调整 logits：$ z_i / T = z_i / 0.5 = 2 \cdot z_i $。
2. 新 logits：
   - 猫: $ 2.0 \cdot 2 = 4.0 $
   - 狗: $ 1.5 \cdot 2 = 3.0 $
   - 鸟: $ 0.5 \cdot 2 = 1.0 $
   - 鱼: $ 0.0 \cdot 2 = 0.0 $
   - 马: $ -1.0 \cdot 2 = -2.0 $

3. 计算指数：
   - 猫: $ \exp(4.0) \approx 54.598 $
   - 狗: $ \exp(3.0) \approx 20.085 $
   - 鸟: $ \exp(1.0) \approx 2.718 $
   - 鱼: $ \exp(0.0) = 1.000 $
   - 马: $ \exp(-2.0) \approx 0.135 $

4. 指数和：$ 54.598 + 20.085 + 2.718 + 1.000 + 0.135 \approx 78.536 $

5. 概率：
   - 猫: $ \frac{54.598}{78.536} \approx 0.695 $ （69.5%）
   - 狗: $ \frac{20.085}{78.536} \approx 0.256 $ （25.6%）
   - 鸟: $ \frac{2.718}{78.536} \approx 0.035 $ （3.5%）
   - 鱼: $ \frac{1.000}{78.536} \approx 0.013 $ （1.3%）
   - 马: $ \frac{0.135}{78.536} \approx 0.002 $ （0.2%）

**效果**：T=0.5 使分布更尖锐，高概率词（如 "猫"）的概率进一步增加，低概率词（如 "马"）的概率接近 0，输出更确定。

---

#### **结合 Top-K 和 Top-P**
- **Top-K=3**（T=1）：选择 ["猫", "狗", "鸟"]，概率为 [0.496, 0.301, 0.111]，从中随机采样。
- **Top-P=0.85**（T=1）：累积概率 0.496 + 0.301 + 0.111 = 0.908 > 0.85，选择 ["猫", "狗", "鸟"]。
- 如果用 T=0.5，Top-P=0.85 可能只选 ["猫", "狗"]（0.695 + 0.256 = 0.951 > 0.85），因为分布更集中。

---

#### **实际应用**
- **Logits**：模型的原始输出，表示对每个词的“倾向”。高 logit（如 2.0）表示更可能，低 logit（如 -1.0）表示不太可能。
- **Softmax**：将 logits 转换为概率，用于采样或选择下一个词。结合 temperature、Top-K 或 Top-P，可以控制输出的随机性和多样性。
- **场景**：
  - **问答**：低 temperature（如 0.5）+ 小 Top-K（如 2），基于高 logit 词生成准确答案。
  - **创意写作**：高 temperature（如 2）+ 大 Top-P（如 0.9），利用更平滑的分布生成多样化文本。

---
