+++
date = '2025-04-07T10:57:56+08:00'
draft = false
title = 'Entropy'
+++


### 对于Entropy的理解

$$H(X) = -\sum_{i=1}^{n} p(x_i) \log_2 p(x_i)$$

---
#### 1. **公式的直观解释**

先来看看公式里每个部分的含义：

- **$H(X)$**：表示随机变量 $X$ 的熵，也就是不确定性的度量。
- **$p(x_i)$**：第 $i$ 个可能结果的概率，介于 0 到 1 之间。
- **$\log_2 p(x_i)$**：对概率取以 2 为底的对数。因为 $p(x_i)$ 是小于 1 的数，所以 $\log_2 p(x_i)$ 是负值。
- **负号和求和**：对所有可能结果的 $p(x_i) \log_2 p(x_i)$ 求和，然后取负号，使熵变成正值。

**为什么用负号？**  
因为 $p(x_i)$ 小于 1 时，$\log_2 p(x_i)$ 是负数，$p(x_i) \log_2 p(x_i)$ 也是负数。加一个负号后，熵 $H(X)$ 变成正数，直观地反映不确定性的大小。

**熵的单位**  
由于用的是以 2 为底的对数，熵的单位是**比特（bit）**，表示描述随机事件所需的信息量。

**物理意义**  
简单来说，熵 $H(X)$ 告诉你：**平均需要多少比特的信息来描述随机变量 $X$ 的一个结果**。如果不确定性高，熵就大；如果结果几乎确定，熵就小。

---

#### 2. **具体例子：抛硬币**

为了让您更好地理解，我们用抛硬币的例子来计算熵。抛硬币有两种可能结果：正面（H）和反面（T）。我们会看两种情况：公平硬币和不公平硬币。

##### **情况 1：公平硬币**

假设硬币是公平的，正面和反面的概率相等：

- 正面（H）的概率 $p(H) = 0.5$
- 反面（T）的概率 $p(T) = 0.5$

现在代入公式计算熵：

$$H(X) = - \left[ p(H) \log_2 p(H) + p(T) \log_2 p(T) \right]$$

代入数值：

$$H(X) = - \left[ 0.5 \log_2 0.5 + 0.5 \log_2 0.5 \right]$$

计算每项：

- $\log_2 0.5 = \log_2 \left( \frac{1}{2} \right) = -1$ （因为 $2^{-1} = 0.5$）
- $0.5 \times (-1) = -0.5$

所以：

$$H(X) = - \left[ -0.5 + (-0.5) \right] = - [-1] = 1$$

**熵 $H(X) = 1$ 比特**。

**解释**：  
对于公平硬币，正面和反面的概率相同，你完全无法预测抛出的结果是什么，不确定性最大。这时熵是 1 比特，意味着你需要 1 比特的信息（比如 0 表示正面，1 表示反面）来描述结果。

##### **情况 2：不公平硬币**

现在假设硬币是不公平的，比如：

- 正面（H）的概率 $p(H) = 0.8$
- 反面（T）的概率 $p(T) = 0.2$

代入公式：

$$H(X) = - \left[ p(H) \log_2 p(H) + p(T) \log_2 p(T) \right]$$

计算每项：

- 对于正面（H）：
  - $p(H) = 0.8$
  - $\log_2 0.8 \approx -0.3219$ （可以用计算器算出）
  - $0.8 \times (-0.3219) \approx -0.2575$

- 对于反面（T）：
  - $p(T) = 0.2$
  - $\log_2 0.2 = \log_2 \left( \frac{1}{5} \right) \approx -2.3219$
  - $0.2 \times (-2.3219) \approx -0.4644$

求和：

$$-0.2575 + (-0.4644) = -0.7219$$

取负：

$$H(X) = - (-0.7219) \approx 0.7219$$

**熵 $H(X) \approx 0.7219$ 比特**。

**解释**：  
这次硬币偏向正面（0.8 的概率），结果更容易预测，不确定性变小了。所以熵从 1 比特下降到了 0.7219 比特，说明需要的信息量减少了。

##### **极端情况：完全确定的硬币**

再看一个极端情况，假设硬币总是正面：

- $p(H) = 1$
- $p(T) = 0$

代入公式：

$$H(X) = - \left[ 1 \times \log_2 1 + 0 \times \log_2 0 \right]$$

- $\log_2 1 = 0$ （因为 $2^0 = 1$）
- $0 \times \log_2 0$：虽然 $\log_2 0$ 是负无穷，但在数学约定中，$0 \times \text{无穷}$ 取 0。

所以：

$$H(X) = - [0 + 0] = 0$$

**熵 $H(X) = 0$ 比特**。

**解释**：  
结果完全确定，没有任何不确定性，所以熵为 0。你不需要任何信息就能知道结果是正面。

---

#### 3. **熵的直观理解**

通过这几个例子，我们可以总结：

- **熵高 = 不确定性高**  
  公平硬币（$p = 0.5, 0.5$）的熵是 1 比特，不确定性最大，因为两种结果完全等可能。
  
- **熵低 = 不确定性低**  
  不公平硬币（$p = 0.8, 0.2$）的熵是 0.7219 比特，结果更可预测；完全确定的硬币（$p = 1, 0$）熵为 0，没有不确定性。

**为什么用 $\log_2$？**  
以 2 为底的对数是因为信息论中常用比特（bits）作为单位，1 比特能表示两种状态（0 或 1），这和抛硬币的两种结果很契合。

**熵的意义**  
熵 $H(X)$ 可以看作是：**平均需要多少比特来编码或描述随机事件的结果**。比如公平硬币需要 1 比特，而不公平硬币需要不到 1 比特。

---

#### 4. **总结**

信息熵的公式 $H(X) = -\sum_{i=1}^{n} p(x_i) \log_2 p(x_i)$ 是一个数学工具，用来量化随机事件的不确定性。通过抛硬币的例子，我们看到：

- 公平硬币（$p = 0.5, 0.5$）：熵 = 1 比特，不确定性最大。
- 不公平硬币（$p = 0.8, 0.2$）：熵 ≈ 0.7219 比特，不确定性减少。
- 完全确定的硬币（$p = 1, 0$）：熵 = 0，没有不确定性。