---
date: '2025-04-15T14:31:46+08:00'
draft: true
title: 'Nn Begin'
---

记不住NN的一些基本概念， 下面用一个例子让你能够快速的理解这些概念并且记住。主要解释输入层、隐藏层（传输层/全连接层）、输出层和池化层等概念，并展示计算过程，当你记不起一些概念的时候回想这个例子，你都能轻松回忆起神经网络的工作原理。

---

#### 1. 输入层
- **作用**：接收外部输入数据。
- **特点**：输入层的神经元数量通常等于输入数据的特征数。每个神经元代表一个输入特征。

#### 2. 隐藏层（传输层/全连接层）
- **作用**：对输入数据进行处理和特征提取。
- **特点**：隐藏层可以有多层，每层包含多个神经元。每个神经元与前一层的所有神经元相连（全连接），通过权重计算和激活函数引入非线性。

#### 3. 输出层
- **作用**：生成最终的预测结果。
- **特点**：输出层的神经元数量取决于任务类型。例如，回归任务通常有1个输出神经元，分类任务的输出神经元数量等于类别数。

#### 4. 池化层
- **作用**：通常用于卷积神经网络（CNN）中，通过降维和特征提取减少计算量并防止过拟合。
- **特点**：池化层对输入特征图进行局部操作，例如取最大值（最大池化）或平均值（平均池化），从而减小尺寸。

---

### 简单的例子：判断点是否在单位圆内

一个简单的二分类问题来演示神经网络的计算过程：判断一个二维坐标点 (x, y) 是否在单位圆内（即 x^2 + y^2 <= 1）。

#### 问题描述
- **输入**：二维坐标 (x, y)。
- **输出**：
  - 如果 x^2 + y^2 <= 1（点在单位圆内），输出 1。
  - 否则，输出 0。

#### 神经网络结构
- **输入层**：2 个神经元，分别接收 x 和 y。
- **隐藏层**：1 个隐藏层，包含 3 个神经元，使用 ReLU 激活函数（ReLU 将负值变为 0，正值保持不变）。
- **输出层**：1 个神经元，使用 sigmoid 激活函数（将输出压缩到 0 到 1 之间）。
- **注**：这个例子不包含池化层，但稍后单独演示池化层的计算。

---

### 计算过程：前向传播

假设输入点是 (0.5, 0.5)，我们将一步步计算神经网络的输出。

#### 步骤 1：输入层
- 输入数据：x = [0.5, 0.5]
- 输入层的 2 个神经元分别接收 x = 0.5 和 y = 0.5。

#### 步骤 2：隐藏层
隐藏层有 3 个神经元，每个神经元与输入层的 2 个神经元全连接。我们需要用权重和偏置计算输出。

#### 权重和偏置
假设隐藏层的权重矩阵 W1 和偏置向量 b1 如下：

```
W1 =
|  1 |  1 |
|  1 | -1 |
| -1 |  1 |

b1 =
| 0 |
| 0 |
| 0 |
```
- W1 是一个 3x2 矩阵，表示 3 个隐藏神经元与 2 个输入神经元的连接权重。
- b1 是偏置，这里设为 0 以简化计算。

#### 线性组合
计算隐藏层的输入：

```
z1 = W1 * x + b1
```

代入数值：

```
w1 =
|  1 |  1 |
|  1 | -1 |
| -1 |  1 |

x =
| 0.5 |
| 0.5 |

b1 =
| 0 |
| 0 |
| 0 |
```

计算矩阵乘法：
- 第 1 个神经元：1 * 0.5 + 1 * 0.5 = 0.5 + 0.5 = 1
- 第 2 个神经元：1 * 0.5 + (-1) * 0.5 = 0.5 - 0.5 = 0
- 第 3 个神经元：(-1) * 0.5 + 1 * 0.5 = -0.5 + 0.5 = 0

结果：

```
z1 =
| 1 |
| 0 |
| 0 |
```

#### 激活函数
对 z1 应用 ReLU 激活函数（ReLU(x) = max(x, 0)）：

```
a1 = ReLU(z1) =
| max(1, 0) |
| max(0, 0) |
| max(0, 0) |
         =
| 1 |
| 0 |
| 0 |
```

隐藏层的输出是 a1 = [1, 0, 0]。

### 步骤 3：输出层
输出层有 1 个神经元，与隐藏层的 3 个神经元全连接。

#### 权重和偏置
假设输出层的权重矩阵 W2 和偏置 b2 如下：

```
W2 = [1, 1, 1]
b2 = 0
```
- W2 是一个 1x3 矩阵，表示输出神经元与 3 个隐藏神经元的连接权重。

#### 线性组合
计算输出层的输入：

```
z2 = W2 * a1 + b2
```

代入数值：

```
z2 = [1, 1, 1] * [1, 0, 0] + 0 = 1 * 1 + 1 * 0 + 1 * 0 = 1
```

#### 激活函数
对 z2 应用 sigmoid 激活函数：

```
a2 = 1 / (1 + exp(-z2))
```

代入 z2 = 1：

```
a2 = 1 / (1 + exp(-1)) ≈ 1 / (1 + 0.368) ≈ 0.731
```

#### 结果解释
- 输出 a2 ≈ 0.731。通常，二分类任务中，如果输出大于 0.5，则预测为 1，否则为 0。
- 这里 0.731 > 0.5，所以预测该点在单位圆内。
- 验证：0.5^2 + 0.5^2 = 0.25 + 0.25 = 0.5 <= 1，确实在单位圆内，预测正确。

---

### 池化层的演示

虽然上面的例子没有使用池化层，但池化层在卷积神经网络中非常重要。这里通过一个独立的小例子说明它的作用。

#### 示例特征图
假设有一个 4x4 的特征图：

```
[
  1,  3,  2,  4
  5,  6,  7,  8
  9, 10, 11, 12
 13, 14, 15, 16
]
```

#### 最大池化
应用 2x2 的最大池化，步长为 2：
- 将特征图分成 4 个 2x2 的区域，取每个区域的最大值：
  - 左上：max(1, 3, 5, 6) = 6
  - 右上：max(2, 4, 7, 8) = 8
  - 左下：max(9, 10, 13, 14) = 14
  - 右下：max(11, 12, 15, 16) = 16

- 池化结果：

```
[
  6,  8
 14, 16
]
```

#### 池化层的作用
- 将 4x4 的特征图缩小为 2x2，减少了计算量。
- 保留了主要特征（最大值），有助于提取重要信息并防止过拟合。

---

### 总结

通过这个例子，我们展示了神经网络的核心概念：
- **输入层**：接收数据（例如 (0.5, 0.5)）。
- **隐藏层（全连接层）**：通过权重和激活函数处理数据（例如从 [0.5, 0.5] 到 [1, 0, 0]）。
- **输出层**：生成预测结果（例如 0.731 表示在单位圆内）。
- **池化层**：降维并提取特征（例如 4x4 特征图变为 2x2）。